\section{Introduction}
\label{sec:intro}

This survey synthesizes recent progress in embodied AI with a focus on model design, data foundations, and training paradigms, and provides a structured comparison of algorithm-oriented and data-oriented studies in Section~\ref{sec:summary_tables}.

\subsection{The Frontier of Physical Intelligence}
The trajectory of Artificial Intelligence in the mid-2020s is defined by a stark dichotomy: the mastery of "Digital Intelligence" versus the nascent struggle for "Physical Intelligence." While Large Language Models (LLMs) have achieved near-human or superhuman proficiency in symbolic reasoning, coding, and creative generation---effectively solving the "Digital" domain---embodied agents remain fundamentally constrained by Moravec's Paradox. This paradox, formulated in the 1980s, posits that high-level reasoning requires relatively little computation, whereas low-level sensorimotor skills (perception, mobility, manipulation) require enormous computational resources and evolutionary prior. As of 2025, artificial agents can pass the Turing Test, score in the 99th percentile of the LSAT, and generate award-winning art, yet they struggle to fold a shirt, clear a cluttered table, or navigate a crowded sidewalk with the fluidity and robustness of a biological organism.

The central hypothesis driving the current research landscape is that \textbf{World Models} constitute the critical missing link in bridging this gap. Just as LLMs internalized the statistical structure of language through internet-scale text corpora, World Models aim to learn the "syntax of reality"---the underlying laws of physics, causal relationships, object permanence, collision dynamics, and material properties---from massive multimodal sensory streams \cite{li_comprehensive_2025,ding_understanding_2025,fung_embodied_2025,liu_aligning_2025}. A robust world model serves as an internal simulator, allowing an agent to predict the consequences of its actions without the risks and costs of real-world trial and error.

The imperative for World Models arises from the fundamental limitations of end-to-end model-free Reinforcement Learning (RL). While model-free methods have solved specific tasks like Go, Dota 2, or StarCraft, they suffer from extreme sample inefficiency, often requiring millions or billions of interaction cycles to converge. In the digital realm of video games, this simulation time is cheap. In the physical world, such data collection is prohibitively expensive, dangerous, and unscalable. Robots break, batteries die, and environments reset slowly. World Models address this by enabling \textbf{Model-Based Reinforcement Learning (MBRL)}, where the agent "dreams" potential futures, evaluates counterfactuals, and refines policies in a learned latent space before committing to expensive real-world actions. This capability transforms the agent from a reactive system to a predictive one, capable of planning over long horizons and generalizing to unseen environments \cite{gupta_essential_2024,zheng_jarvis_2025}.

\subsection{Historical Context: From Data Scarcity to the Data Engine Era}
The evolution of Embodied AI can be characterized by the transition from the "Data Scarcity" era to the "Data Engine" era. Historically, robotic learning was bottlenecked by the difficulty of acquiring high-quality interaction data. Unlike computer vision or NLP, where datasets like ImageNet or CommonCrawl provided billions of static examples, robotics lacked a comparable resource due to the physical risks and logistical complexities of teleoperation. Early approaches relied on small-scale, lab-controlled datasets or low-fidelity simulations, resulting in policies that failed to generalize beyond their training distribution (the "Reality Gap").

However, the period from 2024 to 2026 marked a paradigm shift driven by two converging trends: the rise of \textbf{Embodied Data Engines} and the scaling of \textbf{Generative Video Priors}. Initiatives such as the Open X-Embodiment collaboration \cite{collaboration_open_2025} and the DROID project \cite{khazatsky_droid_2025} demonstrated that diverse, cross-institutional robot data could be aggregated to train generalist policies. These projects fundamentally altered the data landscape, shifting from static repositories to dynamic "engines" where active learning loops continuously refine the model's understanding of rare events. The collection of real-world interaction data has scaled from thousands of trajectories to millions, enabled by lower-cost teleoperation hardware and fleet learning.

Simultaneously, the explosion of generative video models (e.g., Sora, Gen-3) revealed that internet-scale video data contains implicit physical knowledge. The field is now pivoting toward \textbf{Physicalization}---the process of grounding these generative priors in rigorous physical dynamics. Systems like $\pi_0$ \cite{black__0_2026}, $\pi_{0.5}$ \cite{intelligence__05_2025}, and $\pi^*_{0.6}$ \cite{intelligence__06_2025} exemplify this shift, treating video not merely as pixels to be predicted but as a rich source of diverse physical interactions that can be distilled into actionable control policies via techniques like Flow Matching and reinforcement learning \cite{pertsch_fast_2025}. This "Video-as-Policy" approach leverages the vast visual diversity of the web to handle open-world object recognition, while using robot interaction data to ground the dynamics in physical reality.

\subsection{Formalism: World Models as Actionable Dynamic Systems}
To rigorously define the scope of this survey, we must distinguish between \textit{Generative Video Models}, which prioritize visual fidelity for human consumption, and \textit{Embodied World Models}, which prioritize physical consistency and action-conditionability for control. Formally, an Embodied World Model is a Partially Observable Markov Decision Process (POMDP) characterized by a tuple $(\mathcal{S}, \mathcal{A}, \mathcal{O}, P, R, \gamma)$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, and $\mathcal{O}$ is the observation space.

The core function of the World Model is to approximate the transition dynamics of the environment. We define the \textbf{State Transition Model} as:
\begin{equation}
\label{eq:transition}
s_{t+1} \sim P_\theta(s_{t+1} | s_t, a_t, z_t)
\end{equation}
where $s_t$ denotes the compact latent state at time $t$, $a_t$ represents the agent's action (proprioception and control signals), and $z_t$ captures stochastic environmental uncertainty. Unlike pure video prediction, this transition must be \textit{action-conditioned}; the model must predict how the state changes \textit{because of} the agent's intervention, distinguishing causality from correlation \cite{gupta_essential_2024}. A model that predicts a cup moving without an applied force is a hallucination; a model that predicts a cup moving \textit{given} a pushing action is a world model.

Complementing the transition model is the \textbf{Observation Model} (or Decoder), which reconstructs high-dimensional sensory data (pixels, point clouds) from the latent state:
\begin{equation}
\label{eq:observation}
o_{t+1} \sim P_\phi(o_{t+1} | s_{t+1})
\end{equation}
While early approaches focused on pixel-space prediction, modern architectures increasingly operate in latent spaces (e.g., VQ-VAE codebooks or diffusion latents) to avoid the computational cost of rendering high-resolution images during planning. The objective is to minimize the divergence between the predicted future state distribution and the actual distribution observed from data, often formulated as minimizing a variational lower bound (ELBO) or a flow-matching objective \cite{black__0_2026,brehmer_edgi_2023}.

Furthermore, in goal-conditioned settings, we often introduce a reward model $r_t \sim P_\psi(r_t | s_t)$ or a value function $v_t = V(s_t)$ to guide the planning process towards desirable states \cite{li_embodied_2024}. The agent's policy $\pi(a_t|s_t)$ is then optimized to maximize cumulative reward within this "dreamed" environment:
\begin{equation}
\label{eq:policy}
\pi^* = \arg\max_\pi \mathbb{E}_{\tau \sim P_\theta, \pi} \left[ \sum_{t=0}^H \gamma^t r_t \right]
\end{equation}

\subsection{Paradigm Shifts: The Rise of Actionable Generative Models}
The convergence of generative modeling and robotics has catalyzed a new class of algorithms that blur the line between video generation and robot control. We identify three distinct phases in this evolution:

\subsubsection{Phase I: Passive Observation (2018--2023)}
Early World Models like the original "World Models" paper \cite{li_comprehensive_2025} and Dreamer variants focused on learning dynamics from low-dimensional inputs or simple simulated environments (e.g., Atari, DeepMind Control Suite). These models were primarily "spectators," predicting future frames based on past frames without a deep understanding of complex, contact-rich physical interactions. They excelled in game environments where physics is deterministic and discrete, but struggled with the visual complexity, contact discontinuities, and high-frequency dynamics of the real world.

\subsubsection{Phase II: Text-to-Video and the "Physics Hallucination" Problem (2024)}
The advent of large-scale diffusion models (Sora, Kling, Gen-3) enabled the generation of photorealistic video from text prompts. These models demonstrated an unprecedented grasp of visual semantics, lighting, and texture. However, they suffered from "Physics Hallucination"---objects would morph, disappear, or violate conservation laws (e.g., mass, momentum). A glass might merge into the table, or a hand might pass through a solid object. While visually impressive, they lacked the \textit{controllability} required for robotics. A robot cannot plan a grasp if the object teleports or changes shape in the predicted future. This highlighted the necessity of \textbf{Action-Conditioning} and \textbf{Physical Grounding} \cite{ding_understanding_2025}.

\subsubsection{Phase III: Actionable World Models and Flow Matching (2025--Present)}
The current state-of-the-art, represented by architectures like BridgeV2W \cite{chen_bridgev2w_2026} and $\pi_0$ \cite{black__0_2026}, explicitly integrates robot actions into the generative process. These models utilize \textbf{Flow Matching}---a continuous-time generalization of diffusion---to model the vector field of state transitions. By conditioning generation on precise proprioceptive history and future action tokens, these systems can predict the outcome of a robot's interaction with deformable objects (e.g., pressing a sponge, folding a cloth) with sufficient fidelity to serve as a policy. Furthermore, techniques like \textbf{Frequency-space Action Sequence Tokenization (FAST)} \cite{pertsch_fast_2025} have emerged to handle the high-frequency nature of robotic control (typically 50Hz--100Hz), compressing continuous action trajectories into discrete tokens compatible with transformer-based world models.

\subsection{The Cross-Embodiment Challenge}
A unique challenge in Embodied AI, distinct from NLP or Computer Vision, is the heterogeneity of hardware. "Physical Intelligence" must generalize across morphologies: from a 7-DoF Franka Emika arm to a bimanual Aloha setup, a quadrupedal Spot robot, or a humanoid figure. A World Model trained on a single robot is of limited utility. The 2026 frontier focuses on \textbf{Cross-Embodiment Generalization}---learning a universal physics representation that can be fine-tuned or prompted for specific kinematic chains. This requires handling diverse action spaces (joint angles vs. end-effector poses) and observation spaces (wrist cameras vs. head-mounted depth sensors). Models like $\pi_{0.5}$ \cite{intelligence__05_2025} approach this by co-training on heterogeneous datasets, learning to map diverse embodiment data into a shared latent action space. Moreover, multi-agent scenarios \cite{li_embodied_2025} introduce further complexity, requiring models to anticipate the intentions and dynamics of other actors in shared environments.

\subsection{Evaluation: Beyond Visual Fidelity}
Evaluating World Models for embodied agents presents a distinct set of challenges compared to evaluating generative video. Standard computer vision metrics like Fréchet Inception Distance (FID) or FVD (Fréchet Video Distance) measure visual realism but fail to capture physical correctness. A video of a robot hand passing through a mug might have a low FID (it looks realistic) but represents a catastrophic physics violation. Consequently, the field is moving toward \textbf{Task-Centric Evaluation}---measuring the success rate of a policy trained inside the world model when deployed in the real world (Sim-to-Real). New metrics focus on "Physical Consistency," "Action-Controllability," and "Counterfactual Accuracy" \cite{batra_rearrangement_2020,li_embodied_2024}.

\subsection{Contributions and Scope}
This survey provides a comprehensive analysis of the "World Model" paradigm in Embodied AI, synthesizing developments from robotics, computer vision, and generative modeling. Our contributions are four-fold:
\begin{enumerate}
    \item \textbf{Unified Taxonomy}: We propose a structured classification framework categorizing models by \textit{Data Provenance} (Passive Video vs. Active Interaction), \textit{Representation} (Pixel vs. Latent vs. Object-Centric), and \textit{Inference Mechanism} (Autoregressive vs. Diffusion vs. Flow), providing clarity in a crowded landscape (Section \ref{sec:taxonomy}).
    \item \textbf{The Data Engine Analysis}: We detail the architecture of modern Embodied Data Engines, analyzing how massive datasets like Open X-Embodiment are curated, stored, and retrieved to fuel world model training, addressing the "Physical Grounding Gap" \cite{collaboration_open_2025}.
    \item \textbf{Emerging Training Paradigms}: We rigorously review cutting-edge training methodologies, specifically contrasting traditional next-token prediction with \textbf{Flow Matching} and \textbf{Reinforcement Learning with Verifiable Rewards (RLVR)} for physical alignment \cite{black__0_2026}.
    \item \textbf{Infinite Simulation}: We explore the role of procedural generation and "Infinite Photorealistic Worlds" (e.g., Infinigen \cite{yang_physcene_2024}) in bridging the sim-to-real gap, allowing agents to learn in simulation and zero-shot transfer to reality.
\end{enumerate}

This survey serves as a technical roadmap for researchers aiming to build the "GPT for Robotics"---a general-purpose world model that grounds digital intelligence in the physical world. By systematically reviewing the convergence of large-scale generative models with embodied control, we illuminate the path toward agents that not only see the world but understand its underlying physics well enough to act upon it.
