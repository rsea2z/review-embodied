\section{Background and Mathematical Formulation}
\label{sec:background}

\subsection{Embodied Interaction as a Partially Observable Control Process}

We model embodied interaction as a \emph{partially observable Markov decision process} (POMDP):
\begin{equation}
\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{O}, p, \Omega, r, \gamma),
\label{eq:pomdp}
\end{equation}
where $s_t \in \mathcal{S}$ is the latent world state (geometry, object poses, joint configurations), $a_t \in \mathcal{A}$ is the control action (joint torques, end-effector deltas, waypoints, steering commands), and $o_t \in \mathcal{O}$ is the multimodal observation stream (RGB images, depth maps, force-torque readings, proprioception, language context). The world dynamics and observation model are given by:
\begin{equation}
s_{t+1} \sim p(s_{t+1}\mid s_t, a_t), \qquad
o_t \sim \Omega(o_t \mid s_t).
\end{equation}

The agent optimizes discounted cumulative reward:
\begin{equation}
J(\pi) = \mathbb{E}_{\pi,\,p}\!\left[\sum_{t=0}^{T}\gamma^t r(s_t, a_t)\right].
\label{eq:objective}
\end{equation}

The \emph{embodied} qualifier on this standard RL problem imposes several additional requirements beyond simulator RL:
\begin{itemize}
    \item \textbf{Real-time constraint}: the control policy $\pi$ must produce actions within a hard latency budget, typically 10--100~ms depending on control frequency.
    \item \textbf{Safety constraint}: some state-action pairs carry irreversibility (dropped items, self-collision, road accidents), imposing a constrained optimization structure $\mathbb{E}[\sum_t c_k(s_t, a_t)] \le d_k$ for each safety dimension $k$.
    \item \textbf{Partial observability}: $s_t$ is never directly accessed; only $o_t$ is available, making history-dependent policies $\pi(a_t \mid o_{\le t}, g)$ necessary for tasks requiring memory of past interactions.
    \item \textbf{Distribution shift}: the test environment distribution $p_{\text{test}}$ may differ substantially from training distribution $p_{\text{train}}$, requiring robust policy representations that generalize through physical geometry and semantic diversity.
\end{itemize}

\subsection{Belief State Compression and History Encoding}

Since $s_t$ is unobservable, the agent must maintain a \emph{belief state} $b_t = p(s_t \mid o_{\le t}, a_{<t})$. Exact Bayesian belief updating is intractable for high-dimensional state spaces. In practice, embodied systems approximate $b_t$ through four families of compact encoders:

\begin{enumerate}
    \item \textbf{Recurrent state encoders}: $h_t = f_\psi(h_{t-1}, o_t, a_{t-1})$, where $h_t$ is a hidden state compressing history. Widely used in model-based RL for robotics \citep{li_comprehensive_2025,fung_embodied_2025}.
    \item \textbf{Transformer attention windows}: attention over a fixed or growing context window $o_{t-W:t}$, providing temporal credit assignment without explicit recurrence. This is the dominant paradigm in current VLA architectures \citep{kim_openvla_2024,black__0_2026,intelligence__05_2025}.
    \item \textbf{3D spatial memory}: explicit voxel grids or point clouds that accumulate multi-view RGB-D observations into a persistent geometric representation \citep{bhat_3d_2025,zhen_3d-vla_2024,qu_spatialvla_2025,sun_geovla_2025,zhang_4d-vla_2025}. This design choice is especially valuable for manipulation tasks where object geometry and contact affordances critically determine feasibility.
    \item \textbf{Language-conditioned belief}: belief compression guided by the goal instruction $g$, so that task-irrelevant perceptual details are suppressed and goal-relevant features are amplified \citep{huang_voxposer_2023,li_cogvla_2025,huang_graphcot-vla_2025}.
\end{enumerate}

\paragraph{Memory mechanisms for long-horizon control.}
A critical limitation of fixed-window attention is that task-relevant information may exceed the context window, particularly in long-horizon manipulation where the agent must remember previously visited locations, completed subgoals, or object states that have changed. Recent approaches address this through explicit memory augmentation. MAP-VLA introduces demonstration-derived memory prompts as a plug-and-play module for frozen VLAs, using a memory library of historical demonstrations with learnable soft prompts that yield up to 25\% improvement on long-horizon real-robot tasks \citep{li_map-vla_2025}. ContextVLA compresses past observations into a single context token that amortizes multi-frame observation benefits while reducing training and inference costs \citep{jang_contextvla_2025}. TrackVLA extends memory to embodied visual tracking with a Target Identification Memory module and gated update strategy for persistent target representation \citep{liu_trackvla_2025}. Goal-VLA uses image-generative VLMs as object-centric world models to enable zero-shot manipulation by imagining goal states from current observations \citep{chen_goal-vla_2025}. Model-agnostic approaches to VLA improvement, including adversarial robustness through embedding disruption patches, highlight the need for memory and representation mechanisms that are resilient to distribution shift in the observation stream \citep{xu_model-agnostic_2025}.

\subsection{Pre-2024 Design Motifs That Shaped Current Formulations}

Several pre-2024 lines directly shaped current embodied modeling assumptions. Language-grounded planning works argued for explicit decomposition between high-level plan tokens and low-level motor execution, often with environment feedback and feasibility filters \citep{ahn_as_2022,huang_language_2022,huang_inner_2022,song_llm-planner_2023,wu_plan_2023,huang_grounded_2023}. In our notation, this motivates a latent plan variable $\xi_t$:
\begin{equation}
\pi(a_t \mid h_t, g) = \int \pi_{\text{low}}(a_t \mid h_t, \xi_t)\;\pi_{\text{high}}(\xi_t \mid h_t, g)\;d\xi_t,
\label{eq:hierarchical}
\end{equation}
where $h_t$ is the observation-action history encoding and $g$ is the natural language goal. This hierarchical decomposition appears in modern dual-system VLA architectures (e.g., GR00T N1's System 1/System 2 design \citep{nvidia_gr00t_2025}) and in chain-of-thought reasoning VLAs that generate explicit subgoal sequences before actions \citep{ye_vla-r1_2025,zhao_cot-vla_2025,zhang_reasoning-vla_2025,yin_deepthinkvla_2025}.

Generalist transformer-control systems showed that heterogeneous action modalities can be cast as autoregressive token prediction over tokenized observation-action sequences \citep{reed_generalist_2022,brohan_rt-1_2023,chebotar_q-transformer_2023,jiang_vima_2023}. This perspective strongly influenced VLA design choices on action tokenization, sequence conditioning, and VLM initialization. The insight that internet-scale pretraining provides strong semantic priors that transfer to physical manipulation was validated empirically by OpenVLA \citep{kim_openvla_2024} and later systematically studied in VLM4VLA \citep{zhang_vlm4vla_2026}, which found that VLM quality and VLA quality correlate but not monotonically---embodied adaptation objectives remain essential.

Action representations evolved from simple per-dimension binning to more expressive forms: Diffusion Policy introduced denoising-based continuous action generation that handles multimodal action distributions and high-dimensional action spaces naturally \citep{chi_diffusion_2024}, while FAST proposed DCT-based frequency-space tokenization to preserve dexterous high-frequency action structure \citep{pertsch_fast_2025}. More recently, VQ-VLA demonstrates that vector-quantized action tokenizers built on large-scale trajectory data can improve long-horizon real-world success rates by up to 30\% compared to naive discretization \citep{wang_vq-vla_2025}. Discrete diffusion approaches unify discrete and diffusion-based action generation, achieving 96.3\% on LIBERO through parallel decoding that breaks the autoregressive bottleneck \citep{liang_discrete_2025}. ACG introduces training-free test-time guidance that improves action coherence for flow-based VLA models by addressing jerks and pauses from imitation learning \citep{park_acg_2025}. Action tokenization is thus not merely a representation detail but a \textbf{systems lever} that influences training efficiency, dexterity, and deployment latency. The action tokenization reconstruction loss can be formalized as:
\begin{equation}
\mathcal{L}_{\text{act}} = \mathbb{E}\!\left[\sum_{t=1}^{T}\left\|a_t - \hat{a}_t\right\|^2 + \lambda_{\text{vq}}\left\|\mathrm{sg}[z_e(a_t)] - e\right\|^2 + \beta_{\text{vq}}\left\|z_e(a_t) - \mathrm{sg}[e]\right\|^2\right],
\label{eq:action-tokenization}
\end{equation}
where $z_e$ is the encoder, $e$ is the nearest codebook entry, and $\mathrm{sg}[\cdot]$ denotes the stop-gradient operator. The VQ commitment terms regularize the encoder-codebook alignment, and the balance between reconstruction fidelity and codebook utilization determines the downstream control quality.

\subsection{Latent World Models for Embodied Control}

A practical world model introduces a latent state $z_t$ to represent controllable scene dynamics compactly:
\begin{equation}
z_t \sim q_\phi(z_t \mid o_{\le t}, a_{<t}),\qquad
\hat{z}_{t+1} \sim p_\theta(\hat{z}_{t+1} \mid z_t, a_t),
\label{eq:latent-wm}
\end{equation}
where $q_\phi$ is an encoder (posterior over latent states), and $p_\theta$ is a dynamics predictor. Decoder and task-prediction heads project latent trajectories back to observations and task-relevant signals:
\begin{equation}
\hat{o}_{t+1} \sim p_\theta(o_{t+1} \mid \hat{z}_{t+1}),\qquad
\hat{y}_{t+1} = g_\psi(\hat{z}_{t+1}),
\end{equation}
where $\hat{y}_{t+1}$ may denote future object states, occupancy, predicted rewards, contact events, or safety-constraint violations, depending on the downstream control stack \citep{li_comprehensive_2025,fung_embodied_2025,team_gigabrain-0_2025,berg_semantic_2025}.

\textbf{The key design question} is not whether this decomposition exists, but \emph{where decision coupling is applied}. The options are:
\begin{itemize}
    \item End-to-end VLA policy head: the latent $z_t$ conditions an action head directly, combining perception, world modeling, and action generation in one network \citep{kim_openvla_2024,cen_worldvla_2025}.
    \item Model-predictive control over latent rollouts: the dynamics model $p_\theta$ is used to simulate futures, and a separate planner selects actions based on rollout reward \citep{wan_worldagen_2025,guo_vla-reasoner_2025}.
    \item World model as data engine: $p_\theta$ generates synthetic observations used to augment training data for a separately trained policy \citep{team_gigaworld-0_2025,team_gigabrain-0_2025,li_vla-rft_2025}.
    \item Offline-to-online adaptation: the world model is used for simulator-free policy improvement during deployment \citep{li_vla-rft_2025,lu_vla-rl_2025,chen_conrft_2025,intelligence__06_2025}.
\end{itemize}

\subsection{Unified Training Objective}

Most world-model+policy implementations optimize a joint objective that combines predictive, regularization, and task-facing terms:
\begin{equation}
\mathcal{L}
=
\underbrace{\mathcal{L}_{\text{obs}}}_{\text{observation prediction}}
+\beta\underbrace{\mathrm{KL}\!\left[q_\phi(z_t\mid\cdot)\;\|\;p_\theta(z_t\mid z_{t-1},a_{t-1})\right]}_{\text{dynamics consistency}}
+\lambda\underbrace{\mathcal{L}_{\text{task}}}_{\text{control/planning utility}},
\label{eq:wm-objective}
\end{equation}
where:
\begin{itemize}
    \item $\mathcal{L}_{\text{obs}}$ may be a reconstruction loss (pixel MSE/LPIPS), a contrastive loss over future states, or a next-frame prediction cross-entropy depending on whether the representation is pixel-level, tokenized, or semantic.
    \item The KL term regularizes the posterior $q_\phi$ to remain close to the dynamics prior $p_\theta$, preventing posterior collapse and improving rollout stability under long horizons.
    \item $\mathcal{L}_{\text{task}}$ includes action prediction loss (imitation learning), value estimates (offline RL), or contrastive task-conditioned objectives (instruction-conditioned control).
\end{itemize}

The hyperparameters $(\beta, \lambda)$ implement a tradeoff between predictive fidelity and task specificity. Systems that pretrain with high $\beta$ and low $\lambda$ learn richer dynamics representations transferable across tasks; systems that deploy with high $\lambda$ optimize directly for task success but may overfit the training distribution \citep{nvidia_cosmos_2025,team_gigaworld-0_2025,li_comprehensive_2025}.

\subsection{Decision Optimization with Learned Dynamics}

Given a learned dynamics model, open-loop planning over a horizon $H$ can be formulated as:
\begin{equation}
\mathbf{a}_{t:t+H-1}^*
= \arg\max_{\mathbf{a}_{t:t+H-1}}
\mathbb{E}_{p_\theta}\!\left[\sum_{k=0}^{H-1}\gamma^k \hat{r}_{t+k}\right],
\label{eq:planning}
\end{equation}
where $\hat{r}_{t+k} = r(g_\psi(\hat{z}_{t+k}), a_{t+k})$ is the predicted reward at step $t+k$. In practice, pure open-loop planning (Eq.~\ref{eq:planning}) is rarely sufficient. Embodied systems combine it with:
\begin{itemize}
    \item \textbf{Receding horizon control}: re-optimize at each step to correct compounding errors \citep{li_controlvla_2025,lu_vla-rl_2025}.
    \item \textbf{Monte Carlo Tree Search}: guided tree expansion in latent action space \citep{guo_vla-reasoner_2025,yin_deepthinkvla_2025}.
    \item \textbf{Intervention-guided online RL}: use human teleoperation corrections as additional reward signal during deployment \citep{intelligence__06_2025,chen_conrft_2025}.
    \item \textbf{Advantage-conditioned sampling}: RECAP conditions VLA token sampling on estimated advantage values for online RL without explicit value networks \citep{intelligence__06_2025}.
\end{itemize}

\subsection{The VLA Architecture Pattern}

Modern VLA models implement a three-component stack: a vision encoder $E_v$, a language model $E_l$, and an action head $\pi_a$. The standard forward pass is:
\begin{align}
v_t &= E_v(\text{image}_t, \text{depth}_t, \ldots),\\
l_t &= E_l(\text{instruction}_g, v_t),\\
a_t &\sim \pi_a(a_t \mid l_t, h_{t-W:t}),
\end{align}
where $h_{t-W:t}$ is the observation-action history over a context window of width $W$. Variations include:
\begin{itemize}
    \item \textbf{Autoregressive action head}: actions are discretized into tokens and predicted autoregressively, inheriting LLM's in-context learning capability \citep{kim_openvla_2024,li_towards_2024,li_towards_2025}.
    \item \textbf{Diffusion action head} (flow matching or DDPM): continuous actions are denoised conditioned on $l_t$, preserving action continuity \citep{black__0_2026,chi_diffusion_2024,wen_diffusionvla_2025}.
    \item \textbf{Hybrid head}: autoregressive token prediction augmented with diffusion denoising output, fusing reasoning and precision control \citep{liu_hybridvla_2025,zhong_flowvla_2025}.
    \item \textbf{Dual-system architecture}: slow reasoning module (language-level planning, ``System 2'') coupled with fast diffusion motor module (``System 1''), enabling real-time dexterous execution with high-level task understanding \citep{nvidia_gr00t_2025,won_dual-stream_2025}.
    \item \textbf{Multi-modal sensing action head}: extends $v_t$ to include tactile readings, force-torque signals, or audio contact events, enabling fine-grained contact-rich manipulation \citep{bi_vla-touch_2025,huang_tactile-vla_2025,yu_forcevla_2025,wei_audio-vla_2025}.
\end{itemize}

The action representation also varies: per-dimension binning (simple but coarse), DCT-based frequency tokenization \citep{pertsch_fast_2025}, continuous Gaussian \citep{chi_diffusion_2024}, or hybrid discrete-continuous mixtures \citep{liu_hybridvla_2025}. This choice directly affects dexterity, training stability, and inference latency.

\subsection{Action Chunking and Temporal Horizon}

A key systemic design choice is \emph{action chunking}: instead of producing a single action $a_t$, models predict a chunk $\mathbf{a}_{t:t+C-1}$ of $C$ actions simultaneously. Chunking reduces the auto-correlation between high-frequency actions, enables diffusion-based smoothing over the chunk, and amortizes the VLM inference cost over multiple control timesteps. The tradeoff is that longer chunks reduce closed-loop correction frequency:
\begin{equation}
\text{correction bandwidth} \propto \frac{1}{C \cdot \Delta t_{\text{infer}}},
\end{equation}
where $\Delta t_{\text{infer}}$ is model inference latency. Systems such as $\pi_0$ use $C \approx 50$ chunks at 50~Hz, giving a 1-second planning horizon; more reactive systems use $C = 4$--$8$ for contact-rich manipulation where rapid correction is essential \citep{black__0_2026,chen_conrft_2025,li_controlvla_2025}.

\subsection{Failure Modes in the 2024--2026 Regime}

Contemporary embodied systems exhibit three characteristic failure patterns that motivate the taxonomy developed in Section~\ref{sec:taxonomy}:

\textbf{Failure Mode 1: Long-horizon drift.} Let $\epsilon_t$ denote the per-step world model prediction error in latent space. Under sequential rollout, errors accumulate approximately as:
\begin{equation}
\mathcal{E}_{t+H} \leq \sum_{k=0}^{H-1} L^k \epsilon_{t+k},
\label{eq:drift}
\end{equation}
where $L$ is the Lipschitz constant of $p_\theta$ in latent space. For $L > 1$, errors grow exponentially toward the horizon, making long-horizon plans unreliable. Multi-stage household manipulation tasks, which may span dozens of component actions over minutes, are particularly vulnerable \citep{gupta_essential_2024,team_gigabrain-0_2025,wang_mechanistic_2026}. WorldBench diagnoses this through isolated concept-level physical evaluation \citep{upadhyay_worldbench_2026}, and recent work aims to reduce $L$ via physics-informed regularization and contact-aware dynamics \citep{han_percept-wam_2025}.

\textbf{Failure Mode 2: Representation mismatch.} Coordinate-space action control and pixel-space visual prediction operate in fundamentally different reference frames. A wrist joint angle increment does not have a straightforward pixel-space interpretation without explicit embodiment geometry (URDF, camera extrinsics, contact normals). This mismatch causes two problems: (a) the world model generates visually plausible futures that are geometrically inconsistent with the robot's actual kinematic constraints; and (b) the policy cannot exploit geometric structure in visual predictions to improve action estimation. BridgeV2W \citep{chen_bridgev2w_2026} addresses this through URDF-aligned embodiment masks rendered into the prediction pathway; FlowDreamer \citep{guo_flowdreamer_2026} uses optical flow as an intermediate representation that bridges pixel and action space. GeoVLA \citep{sun_geovla_2025} and 4D-VLA \citep{zhang_4d-vla_2025} incorporate depth and point cloud inputs to resolve this mismatch directly in the policy's observation space.

\textbf{Failure Mode 3: Evaluation gaps.} Standard success rate metrics aggregate task outcome into a binary signal, hiding causal and physical subtleties. A policy that achieves 80\% success by exploiting benchmark-specific visual cues may have 30\% success under natural scene variation or object substitution. This evaluation gap is especially dangerous because policy developers optimize for the visible metric, reinforcing it at the expense of genuine robustness \citep{valle_evaluating_2025,wu_pragmatic_2026,wu_what_2026}. The remedy requires multi-dimensional evaluations that separately quantify task competence, intervention frequency, recovery capability, physical consistency, and out-of-distribution generalization---the metric families formalized in Section~\ref{sec:data-metrics}.

These three failure modes motivate the three axes of the taxonomy in Section~\ref{sec:taxonomy}: Functionality coupling (addressing Mode 1), Spatial Representation (addressing Mode 2), and Evaluation Protocol (addressing Mode 3).

