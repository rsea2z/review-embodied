\section{Taxonomy and Problem Formulation}
\label{sec:taxonomy}

The rapid evolution of embodied AI and world models in the 2025--2026 landscape demands a structured classification framework that moves beyond traditional boundaries of model architecture. While early taxonomies focused primarily on the dichotomy between model-free and model-based reinforcement learning, the current era is defined by the convergence of large-scale generative models, massive heterogeneous datasets, and diverse training objectives. To systematize this complex landscape, we propose a comprehensive taxonomy centered on three primary axes: \textbf{(1) Data Provenance and Construction}, which delineates the source and nature of the training signal; \textbf{(2) Representation Modality}, which distinguishes how the world state is encoded and predicted; and \textbf{(3) Inference Paradigm}, which categorizes the temporal generation mechanism. This framework, illustrated in Table~\ref{tab:taxonomy_overview}, provides a unified lens to analyze systems ranging from foundational video generators like NVIDIA Cosmos to latent dynamics models like Meta's V-JEPA 2 and TARS Robotics' AWE2.0.

\subsection{Axis 1: Data Provenance and Construction}
The first axis categorizes world models based on the origin and active nature of their training data, reflecting a fundamental shift from data scarcity to data abundance through synthesis and internet-scale mining. \textbf{Passive Observation} models are trained primarily on large-scale, non-interactive video data sourced from the internet (e.g., YouTube, Ego4D). These systems, such as the foundation of NVIDIA Cosmos \cite{wang_genie_2025} and Genie \cite{bruce_genie_2024}, excel at learning general physical priors and visual dynamics but often lack explicit action conditioning. The key technical challenge in this category is the "physicalization" of video---filtering for physical consistency, causal logic, and temporal coherence to ensure the model learns a valid simulator of reality rather than a mere sequence of pixels \cite{ding_understanding_2025}. In contrast, \textbf{Embodied Interaction} models are grounded in active robot interaction data, capturing precise action-effect correlations from distributed teleoperation (DROID \cite{khazatsky_droid_2025}), human-centric wearables (TARS WIYH \cite{li_scalable_2025}), and large-scale aggregation initiatives (Open X-Embodiment \cite{collaboration_open_2025}). While these datasets provide high-fidelity proprioceptive and tactile signals, they are constrained by the high cost of physical collection. Finally, \textbf{Generative Synthesis} represents the frontier of "world model as data engine," where data is produced by high-fidelity simulators or the world models themselves. This category encompasses heterogeneous parallel simulation platforms like ManiSkill3 \cite{li_mimicdreamer_2025}, digital twins such as RoboGSim \cite{tai_realmirror_2025}, and approaches like GigaWorld \cite{team_gigaworld-0_2025} where synthetic experiences are generated to bootstrap policy learning.

\subsection{Axis 2: Representation Modality}
The second axis distinguishes models by their internal state representation, reflecting the inherent trade-off between visual fidelity and the abstract compactness required for robust control. \textbf{Pixel-Space Generative Models} predict future states directly in the high-dimensional pixel space, often treating video generation as the primary objective. Architectures like Vid2World \cite{zhang_dreamvla_2025} and Navigation World Models (NWM) \cite{li_urbanvla_2025} leverage diffusion backbones to produce interpretative rollouts, though they require significant computational resources for high-dimensional prediction. \textbf{Latent-Space Dynamics Models} operate in a compressed, abstract space, focusing on semantic and dynamic consistency rather than pixel-level perfection. This approach, typified by Joint Embedding Predictive Architectures (V-JEPA 2 \cite{tharwat_latent_2025}) and Latent Action Models (Genie), prioritizes sample efficiency and long-horizon planning stability by ignoring high-frequency visual noise that is irrelevant to task success. \textbf{Object-Centric and Structured Models} explicitly factorize the world state into objects and interaction primitives (e.g., FIOC-WM \cite{zhu_objectvla_2025}). These approaches leverage inductive biases to improve combinatorial reasoning and generalization in complex, multi-object environments, enabling more precise manipulation and causal reasoning.

\subsection{Axis 3: Inference and Training Paradigm}
The third axis categorizes the temporal generation mechanism and the optimization objectives used to align the world model with physical reality. \textbf{Autoregressive and Sequential} models generate future states step-by-step, conditional on the entire past history $x_{<t}$. Representative architectures like RSSM-based DreamerV3 \cite{li_comprehensive_2025} and Transformer-based Genie are inherently causal and suitable for real-time control, though they can suffer from error accumulation over long horizons. \textbf{Global Diffusion and Flow Matching} models represent a significant shift toward parallelized or iterative refinement mechanisms. Examples include $\pi_0$ \cite{black__0_2026}, which utilizes Flow Matching to achieve 50Hz continuous control, and Sora-inspired physics simulators. These methods typically achieve higher temporal coherence and are more robust to the multi-modal nature of human-like behavior. Finally, \textbf{Verifiable Reward Optimization (RLVR)} represents an emerging paradigm where models are optimized via reinforcement learning against verifiable physical or logical constraints, rather than just likelihood maximization. This post-training alignment, as seen in RLVR-World \cite{ye_vla-r1_2025}, ensures that the world model's predictions remain physically plausible and logically consistent during long-horizon rollouts.

\begin{table}[ht]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l p{4cm} p{7cm}}
\toprule
\textbf{Primary Axis} & \textbf{Category} & \textbf{Representative Works} \\ \midrule
\textbf{Data Provenance} & Passive Observation & Cosmos \cite{wang_genie_2025}, Genie \cite{bruce_genie_2024}, Ego4D \cite{lu_multimodal_2025}. \\ \midrule
 & Embodied Interaction & RT-X \cite{collaboration_open_2025}, DROID \cite{khazatsky_droid_2025}, TARS AWE2.0 \cite{li_scalable_2025}, HumanPlus. \\ \midrule
 & Generative Synthesis & ManiSkill3 \cite{li_mimicdreamer_2025}, RoboGSim \cite{tai_realmirror_2025}, GigaWorld \cite{team_gigaworld-0_2025}, DrEureka. \\ \midrule
\textbf{Representation} & Pixel-Space & Vid2World \cite{zhang_dreamvla_2025}, NWM \cite{li_urbanvla_2025}, DriveDreamer. \\ \midrule
 & Latent-Space & V-JEPA 2 \cite{tharwat_latent_2025}, DreamerV3 \cite{li_comprehensive_2025}, LWM. \\ \midrule
 & Object-Centric & FIOC-WM \cite{zhu_objectvla_2025}, ManiGaussian. \\ \midrule
\textbf{Inference} & Autoregressive & RSSM, Transformer-based (Genie \cite{bruce_genie_2024}). \\ \midrule
 & Global/Flow & $\pi_0$ \cite{black__0_2026}, Video Diffusion (Sora). \\ \midrule
 & RL-Optimized & RLVR-World \cite{ye_vla-r1_2025}, CoT-Reasoning. \\ \bottomrule
\end{tabular}
\caption{Taxonomy of World Models for Embodied AI (2025--2026 Landscape)} \label{tab:taxonomy_overview}
\end{table}

This taxonomy serves as the structural backbone for the subsequent sections, where we delve into the specific data engines (Section \ref{sec:datasets}), training methodologies (Section \ref{sec:training}), and simulation platforms (Section \ref{sec:simulation}) that embody these classifications.
