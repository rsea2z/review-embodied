\section{Comprehensive Literature Review}
\label{sec:literature_review}

This section provides a structured synthesis of representative studies in embodied AI. For clarity, Section~\ref{sec:summary_tables} further organizes the literature into algorithm-oriented and data-oriented papers.

The landscape of Embodied AI has undergone a seismic and rapid transformation over the past four years, evolving from disjointed, modular pipelines of perception and planning to unified, end-to-end Vision-Language-Action (VLA) architectures. This paradigm shift has been driven by the convergence of large-scale pre-training, multimodal integration, and a renewed, critical focus on physical grounding. This section provides an extensive and detailed synthesis of the literature, tracing the trajectory from the early, text-centric LLM-based planners of 2022 to the high-fidelity, world-model-augmented, and physically aware systems of 2026. We categorize this vast body of work into five emerging and distinct technical themes: spatial and physical grounding, cognitive reasoning and planning, world model integration, computational efficiency, and robust generalization.

\subsection{The Evolution of Embodied Intelligence (2022--2026)}
The progression of embodied agents can be characterized by a distinct shift from "semantic reasoning" in abstract spaces to "sensorimotor mastery" in the physical world. This evolution reflects a deepening understanding of what is required to translate internet-scale knowledge into actionable robotic skills.

\subsubsection{2022--2023: The Era of LLMs as Planners}
In the nascent stages of large model adoption in robotics, research primarily focused on leveraging the semantic reasoning capabilities of text-only Large Language Models (LLMs) to decompose abstract, high-level tasks into executable primitives. Huang et al. (2022) \cite{huang_language_2022} demonstrated that LLMs could perform zero-shot planning for embodied agents, provided the output space was strictly constrained to valid, pre-defined actions. This "Planner-Actor" paradigm was further formalized by Wu et al. (2023) \cite{wu_plan_2023} with the \textit{Plan, Eliminate, and Track (PET)} framework. PET utilized LLMs not just for generating steps, but for actively filtering irrelevant objects from the perception pipeline and tracking subtask completion, essentially acting as a high-level cognitive controller over a fixed low-level policy.

Similarly, \textit{LLM-Planner} \cite{song_llm-planner_2023} and \textit{Voyager} \cite{wang_voyager_2023} showcased the power of LLMs in open-ended, game-like environments such as Minecraft. \textit{Voyager}, in particular, introduced an automatic curriculum and an iterative prompting mechanism that allowed the agent to write and refine its own code skills, effectively "learning" by expanding a library of executable primitives. Nottingham et al. (2023) \cite{nottingham_embodied_2023} explored the concept of "embodied decision making" using language-guided world modeling, asking whether agents could "dream" of pixelated outcomes to inform their text-based plans. Zhang et al. (2024) \cite{zhang_building_2024} extended this to multi-agent cooperation with \textit{CoELA}, where LLMs facilitated communication and joint planning. However, these systems universally suffered from a "grounding gap"---the fundamental disconnect between high-level semantic plans (e.g., "pick up the apple") and the low-level physical execution (e.g., torque control, grasp synthesis). They relied heavily on hand-coded primitives or separate, often brittle, low-level controllers, limiting their applicability to complex, contact-rich real-world tasks \cite{nottingham_embodied_2023,zhang_building_2024}.

\subsubsection{2024: The Rise of Unified VLA Models}
The introduction of Vision-Language-Action (VLA) models marked a radical departure from modular pipelines toward end-to-end learning. By tokenizing robot actions and training them alongside text and images, models began to treat motor control as a sequence modeling problem. Zhang et al. (2025) \cite{zhong_survey_2025} provided a comprehensive survey of "Pure" VLA models. Cui et al. (2025) \cite{cui_end--end_2025} consolidated this end-to-end learning framework, while Xiang et al. (2025) \cite{xiang_vla_2025} and Li et al. (2025) \cite{li_vla_2025} demonstrated its efficacy in fine-grained control tasks. This unified approach allowed models to inherit the vast semantic knowledge of their VLM backbones while learning the "syntax" of physical action.

Sapkota et al. (2026) \cite{sapkota_vision-language-action_2026} further categorized these developments, noting the emergence of "generalist agents" capable of cross-task transfer. The \textit{VLA-Adapter} \cite{wang_vla-adapter_2025} represented a key architectural innovation, introducing efficient adaptation mechanisms to fine-tune massive pre-trained models for robotic tasks without catastrophic forgetting or prohibitive compute costs. Similarly, \textit{VITA-VLA} \cite{dong_vita-vla_2025} explored efficient "teaching" methods, distilling action knowledge into VLMs via expert action models. Kawaharazuka et al. (2025) \cite{li_survey_2025} reviewed this transition towards real-world applications, emphasizing that while VLAs solved the "interface" problem between language and action, they introduced new challenges in data efficiency and precision.

\subsubsection{2025--2026: Specialization and World Model Integration}
As the field matured into 2025 and 2026, the research focus shifted from "can it act?" to "how well, how fast, and how reliably can it act?" The limitations of pure imitation learning---specifically its struggle with distribution shifts and long-horizon compounding errors---became apparent. Li et al. (2025) \cite{li_comprehensive_2025} and Ding et al. (2025) \cite{ding_understanding_2025} formalized the role of \textit{World Models}---internal simulators that predict future states to guide decision-making. These surveys established a taxonomy for world models in embodied AI, distinguishing between decision-coupled and general-purpose simulators.

Systems like \textit{GigaBrain-0} \cite{team_gigabrain-0_2025} and \textit{$\pi_0$} \cite{black__0_2026} began to integrate advanced generative techniques like flow matching and diffusion processes to handle the multimodal, continuous nature of action distributions. The \textit{$\pi_0$} model, for instance, proposed a flow matching architecture on top of a VLM to inherit internet-scale semantics while outputting continuous actions, demonstrating capabilities in diverse tasks like laundry folding and box assembly. Simultaneously, the "Pragmatic" turn described by Wu et al. (2026) \cite{wu_pragmatic_2026} with \textit{LingBot-VLA} emphasized reliability, pushing for training on massive datasets (20,000+ hours) to achieve industrial-grade robustness. This era is defined by a move towards specialized architectures that address specific bottlenecks: \textit{NORA} \cite{hung_nora_2025} for efficiency, \textit{WholeBodyVLA} \cite{jiang_wholebodyvla_2025} for humanoid control, and \textit{ObjectVLA} \cite{zhu_objectvla_2025} for open-world object generalization.

\subsection{Theme I: Spatial and Physical Grounding}
A critical bottleneck for early VLAs was their reliance on 2D image inputs, which often discarded the precise 3D geometry and physical forces required for delicate or dynamic manipulation. Recent work has aggressively addressed this "dimensionality deficit" by incorporating depth, point clouds, and tactile feedback.

\subsubsection{From 2D to 3D Representations}
To overcome the inherent limitations of 2D vision, Turgunbaev et al. (2025) \cite{turgunbaev_perception_2025} and Din et al. (2025) \cite{din_vision_2025} argue that robust perception requires full 3D understanding. Sun et al. (2025) \cite{sun_geovla_2025} introduced \textit{GeoVLA}. This framework integrates 3D information by converting depth maps into point clouds and employing a customized Point Embedding Network. These geometric embeddings are fused with vision-language features, empowering the model with a richer spatial understanding that 2D-only models lack. Similarly, \textit{PointVLA} \cite{li_pointvla_2026} injects the 3D world directly into the VLA by processing point cloud data, allowing for more precise manipulation in cluttered or depth-complex environments.

Li et al. (2025) \cite{li_3ds-vla_2025} proposed \textit{3DS-VLA}, a 3D spatial-aware model designed for robust multi-task manipulation, further cementing the importance of explicit 3D encoders. Zhou et al. (2025) \cite{zhang_4d-vla_2025} extended this grounding to the temporal dimension with \textit{4D-VLA}. By utilizing sequential RGB-D inputs, 4D-VLA captures spatiotemporal dynamics and aligns the coordinate systems of the robot and the scene, addressing the "coordinate system chaos" that often plagues pre-training.
For applications where full 3D perception is expensive, \textit{OG-VLA} \cite{singh_og-vla_2025} offers an elegant alternative. It utilizes orthographic projections to canonicalize input observations, generating images that encode the next position and orientation of the end-effector in a view-invariant manner. \textit{SpatialVLA} \cite{qu_spatialvla_2025} and \textit{QDepth-VLA} \cite{li_qdepth-vla_2025} explore auxiliary supervision signals. \textit{SpatialVLA} introduces Ego3D Position Encoding and Adaptive Action Grids, while \textit{QDepth-VLA} augments the VLA with a quantized depth prediction task, forcing the model to learn structural representations of the scene implicitly. Patratskiy et al. (2025) \cite{patratskiy_spatial_2025} further emphasized this with "Spatial Traces." Berg et al. (2025) \cite{berg_semantic_2025} complement this physical grounding with deep semantic grounding, ensuring objects are understood not just by location but by function.

\subsubsection{Incorporating Force and Tactile Feedback}
Beyond geometry, successful manipulation---especially in contact-rich tasks---requires sensing and modulating force. \textit{ForceVLA} \cite{yu_forcevla_2025} treats 6-axis force-torque data not as a secondary signal, but as a "first-class" modality. It introduces \textit{FVLMoE}, a force-aware Mixture-of-Experts fusion module that dynamically integrates force feedback with visual-language embeddings, achieving significant success in tasks like plug insertion.
\textit{Tactile-VLA} \cite{huang_tactile-vla_2025} unlocks the physical knowledge embedded in VLMs. By connecting the VLM to a hybrid position-force controller and a tactile reasoning module, it enables the robot to adapt its strategy based on tactile feedback, achieving zero-shot generalization in contact-rich scenarios. \textit{VLA-Touch} \cite{bi_vla-touch_2025} adopts a dual-level approach. Dolgopolyi et al. (2025) \cite{dolgopolyi_bridging_2025} further advance this by bridging visual and tactile modalities into a unified representation space. A tactile-language model provides semantic feedback for high-level planning, while a diffusion-based controller refines actions based on tactile signals.

Zhang et al. (2026) introduced the \textit{CompliantVLA-adaptor} \cite{zhang_compliantvla-adaptor_2026}, which represents a step towards safety and compliance. This model interprets task contexts to adapt the stiffness and damping parameters of a variable impedance controller, ensuring safe interaction. Finally, \textit{TA-VLA} \cite{zhang_ta-vla_2025} systematically elucidates the design space for torque-aware architectures, finding that predicting torque as an auxiliary output---rather than just using it as input---encourages the model to build a physically grounded internal representation of interaction dynamics.

\subsection{Theme II: Cognitive Reasoning and Planning}
While foundational VLAs are adept at reactive control, they often lack the "System 2" reasoning capabilities required for multi-step problem solving, ambiguity resolution, and error recovery. The integration of Chain-of-Thought (CoT) reasoning into robotic policies has become a major research vector in 2025-2026.

\subsubsection{Chain-of-Thought in Robotics}
Zhong et al. (2026) proposed \textit{ACoT-VLA} \cite{zhong_acot-vla_2026}, which formulates reasoning as a structured sequence of "coarse action intents." The model features an Explicit Action Reasoner (EAR) that proposes reference trajectories and an Implicit Action Reasoner (IAR) that extracts latent action priors, co-forming a chain of thought that guides the final policy. This aligns with \textit{CoT-VLA} \cite{zhao_cot-vla_2025}, which employs visual chain-of-thought reasoning to better interpret visual cues. \textit{GraphCoT-VLA} \cite{huang_graphcot-vla_2025} takes a structured approach, constructing a real-time 3D Pose-Object graph. Xue et al. (2025) \cite{xue_leverb_2025} introduce \textit{LeVerb}, which explicitly leverages verbalization to ground complex planning steps.

Wu et al. (2026) \cite{wu_what_2026} introduced a novel runtime verification mechanism with \textit{Do What You Say}. This framework steers the VLA by verifying that the generated action sequences actually align with the model's own intermediate textual plan, effectively using the plan as a ground-truth constraint to filter hallucinated actions. \textit{ManualVLA} \cite{gu_manualvla_2025} explicitly trains models to generate "manuals"---intermediate steps consisting of images, position prompts, and text---before executing actions. \textit{VLA-R1} \cite{ye_vla-r1_2025} integrates Reinforcement Learning from Verifiable Rewards (RLVR) to reinforce this reasoning quality, ensuring that the chain-of-thought is not just plausible but executable. \textit{Reasoning-VLA} \cite{zhang_reasoning-vla_2025} applies this paradigm to autonomous driving, using learnable action queries initialized from ground-truth trajectories to generate continuous action paths.

\subsubsection{Self-Reflection and Counterfactuals}
Advanced reasoning also involves knowing what \textit{not} to do and learning from hypothetical failures. \textit{Counterfactual VLA} \cite{peng_counterfactual_2025} enables agents to perform "self-reflective" reasoning. It generates time-segmented meta-actions and then performs counterfactual reasoning to simulate potential outcomes, identifying unsafe behaviors and correcting the plan before execution. Hsieh et al. (2025) \cite{hsieh_what_2025} focused on the critical safety capability of "rejecting the impossible," teaching VLAs to identify and refuse unfeasible instructions.

\textit{VLA-Reasoner} \cite{guo_vla-reasoner_2025} empowers models with Monte Carlo Tree Search (MCTS) to explore future states at test time. By rolling out possible action trajectories and evaluating them with a world model, it scales compute during inference to improve decision quality. \textit{Latent-CoT} \cite{tan_latent_2025} pushes this reasoning into a latent space, interleaving action-proposal tokens with world-model tokens to reason about future outcomes without the computational overhead of decoding natural language. \textit{d-VLA} \cite{wen_dvla_2025} integrates a multimodal chain-of-thought directly into a diffusion objective, optimizing perception, reasoning, and action jointly.

Recent architectures have further formalized this cognitive cycle. Yin et al. (2025) \cite{yin_deepthinkvla_2025} enhanced the reasoning depth of VLAs with \textit{DeepThinkVLA}, utilizing a recursive module for iterative state evaluation in long-horizon tasks. In parallel, Zhong et al. (2025) \cite{zhong_flowvla_2025} introduced \textit{FlowVLA}, which grounds Chain-of-Thought reasoning directly into visual motion flows, effectively bridging the gap between abstract semantic planning and continuous control trajectories. Song et al. (2025) \cite{song_rationalvla_2025} complemented this with \textit{RationalVLA}, a dual-system architecture that generates explicit rationales for action selection, enabling robust error recovery and interpretability.

\subsection{Theme III: World Models as Predictive Simulators}
The integration of World Models---systems that predict future sensory data conditioned on actions---has fundamentally changed how agents learn and plan. They serve as internal simulators that allow agents to "experience" the future without acting in the real world.

\subsubsection{Video Generation for Control}
Generative video models have evolved from passive media creators to active control interfaces. \textit{DriveVLA-W0} \cite{li_drivevla-w0_2025} utilizes world modeling to predict future images in autonomous driving scenarios, using this dense supervision to amplify data scaling laws. \textit{WristWorld} \cite{qian_wristworld_2025} addresses the specific data scarcity of wrist-view cameras. It acts as a 4D world model that generates temporally coherent wrist-view videos solely from anchor views, bridging the gap between abundant third-person data and the fine-grained first-person data needed for manipulation.

\textit{BridgeV2W} \cite{chen_bridgev2w_2026} introduces "embodiment masks" to align video generation with robot kinematics. By rendering the robot's URDF into the video generation process, it ensures that the predicted futures are physically consistent with the robot's capabilities. \textit{Dream-VL} and \textit{Dream-VLA} \cite{ye_dream-vl_2026} leverage diffusion backbones to "dream" potential futures, achieving state-of-the-art performance on benchmarks like LIBERO. \textit{WorldVLA} \cite{cen_worldvla_2025} and \textit{RynnVLA-002} \cite{cen_rynnvla-002_2025} propose unified architectures where the world model and the policy are jointly trained, allowing the action model to aid visual generation and vice versa. \textit{GigaWorld-0} \cite{team_gigaworld-0_2025} scales this up, serving as a massive data engine that generates diverse, controllable, and physically plausible video data for training VLA models.

\subsubsection{Reinforcement Learning with World Models}
World models provide a safe, resettable sandbox for Reinforcement Learning (RL), mitigating the sample inefficiency and safety risks of real-world training. \textit{WMPO} \cite{zhu_wmpo_2025} introduces World-Model-based Policy Optimization, a framework for on-policy VLA RL that uses pixel-based predictions to align "imagined" trajectories with VLA features. \textit{VLA-RFT} \cite{li_vla-rft_2025} uses a data-driven world simulator to provide dense, trajectory-level rewards for reinforcement fine-tuning, achieving strong robustness with minimal samples.

\textit{World-Env} \cite{xiao_world-env_2025} constructs a virtual environment using a video-based world simulator and a VLM-guided reflector for reward generation, enabling effective post-training without physical interaction. \textit{IRL-VLA} \cite{jiang_irl-vla_2025} applies this to autonomous driving, building a reward world model via inverse reinforcement learning. \textit{SimpleVLA-RL} \cite{li_simplevla-rl_2025} and \textit{RLinf-VLA} \cite{zang_rlinf-vla_2025} provide scalable frameworks for this paradigm, implementing efficient resource allocation strategies to handle the heavy computational load of rendering, training, and inference in parallel. \textit{Latent Action Pretraining} \cite{tharwat_latent_2025} utilizes world modeling to learn latent action representations from unlabeled video data, enabling unsupervised skill acquisition.

\subsection{World Model Integration in VLA Architectures}
\label{subsec:world_model_vla}

The integration of world models into Vision-Language-Action (VLA) architectures has emerged as a transformative paradigm in 2025-2026, shifting the focus from purely reactive policies to predictive agents capable of simulating future outcomes. This integration addresses the critical "horizon" limitation of standard imitation learning by equipping agents with an internal simulator of physical dynamics.

\subsubsection{Unified World Model Architectures}
Recent works have proposed unified architectures that seamlessly blend perception, prediction, and action. Bi et al. (2025) introduced \textit{Motus} \cite{bi_motus_2025}, a unified latent action world model that employs a Mixture-of-Transformer (MoT) to integrate understanding, video generation, and action experts. By leveraging optical flow to learn latent actions, Motus achieves superior performance in both simulation and real-world tasks. Lillemark et al. (2026) advanced this by proposing \textit{Flow Equivariant World Models} \cite{lillemark_flow_2026}, which unify self-motion and object motion as Lie group flows. This equivariance provides stable latent world representations over long horizons, significantly outperforming diffusion-based baselines in partially observed environments. Magne et al. (2026) scaled this concept to gaming with \textit{NitroGen} \cite{magne_nitrogen_2026}, a foundation model trained on 40,000 hours of gameplay that exhibits strong cross-game generalization.

\subsubsection{Video Generation as World Simulation}
Generative video models are increasingly serving as the "visual cortex" of world models. Liao et al. (2025) presented the \textit{Genie Envisioner} \cite{liao_genie_2025}, a platform that maps latent representations to executable actions via flow matching, supported by a large-scale instruction-conditioned video diffusion model. Wan et al. (2025) introduced \textit{WorldAgen} \cite{wan_worldagen_2025}, which unifies state-action prediction with test-time world model training. In the domain of flow-based representations, Guo et al. (2026) proposed \textit{FlowDreamer} \cite{guo_flowdreamer_2026}, an RGB-D world model that captures motion dynamics through flow fields. Wang et al. (2023) \cite{wang_genie_2023} also contributed to this landscape with the \textit{Genie} interactive world model, emphasizing the generative aspects of environment simulation.

\subsubsection{Predictive Modeling and Unified Prediction}
Beyond generation, world models are being optimized for explicit prediction tasks. Han et al. (2025) developed \textit{Percept-WAM} \cite{han_percept-wam_2025}, a World-Awareness-Action Model that integrates 2D/3D perception into tokens, enabling robust spatial grounding in autonomous driving. Xu et al. (2025) introduced \textit{WAM-Diff} \cite{xu_wam-diff_2025}, a masked diffusion framework that iteratively refines discrete sequence predictions for trajectory generation. Zhang et al. (2025) proposed \textit{UP-VLA} \cite{zhang_up-vla_2025}, a unified model trained with both understanding and future prediction objectives, bridging the gap between high-level semantics and low-level physical dynamics.

The scope of world modeling has also expanded to encompass comprehensive knowledge integration. Zhang et al. (2025) \cite{zhang_dreamvla_2025} developed \textit{DreamVLA}, which integrates a vast repository of world knowledge into the generative process, allowing the agent to simulate diverse physical interactions and outcomes with high fidelity. This aligns with the capabilities of \textit{GigaBrain-0} \cite{team_gigabrain-0_2025}, which leverages massive-scale pre-training to power a world-model-centric VLA capable of generalist behavior.

\subsubsection{Mechanistic Integration and Steering}
Finally, understanding \textit{how} these world models influence VLA behavior is crucial. Haon et al. (2025) \cite{haon_mechanistic_2025} introduced a framework for mechanistic interpretability, identifying sparse semantic directions within VLA representations that causally link to action selection. This allows for direct steering of the model's behavior at inference time. Wang et al. (2026) \cite{wang_mechanistic_2026} provided a mechanistic taxonomy of video generation as world models, distinguishing between state construction and dynamics modeling to advance robust world simulators.

\subsection{Theme IV: Efficiency and Real-Time Inference}
As VLA models scale to billions of parameters, inference latency becomes a critical bottleneck. Closed-loop robotic control often requires low-latency updates, which is challenging for massive transformer models.

\subsubsection{Architectural Optimization and Pruning}
To address the computational cost, \textit{EfficientVLA} \cite{yang_efficientvla_2025} introduces a training-free acceleration framework that prunes redundant layers in the language module and optimizes visual token selection. \textit{HyperVLA} \cite{xiong_hypervla_2025} proposes a hypernetwork-based architecture that generates a small, task-specific policy at inference time, drastically reducing the number of active parameters while retaining the capacity of the large model. \textit{SpecPrune-VLA} \cite{wang_specprune-vla_2025} leverages the temporal redundancy of robot actions, employing action-aware speculative pruning to reduce visual tokens based on global history.

\textit{SQAP-VLA} \cite{fang_sqap-vla_2025} synergistically combines quantization and pruning, proposing quantization-aware pruning criteria to achieve nearly 2x speedups without performance loss. \textit{MoLe-VLA} \cite{zhang_mole-vla_2025} draws inspiration from the "Shallow Brain Hypothesis," using a Mixture-of-Layers strategy to dynamically skip layers based on the complexity of the current state. \textit{AC$^2$-VLA} \cite{yu_ac2-vla_2026} introduces Action-Context-aware Adaptive Computation. Focusing on optimization dynamics, Guo et al. (2025) \cite{guo_improving_2025} and Zhao et al. (2025) \cite{zhao_more_2025} propose methods for making VLA training and inference more effective through improved loss landscapes.

\subsubsection{Tokenization and Decoding Strategies}
Efficiently representing the action space is equally important. \textit{FAST} \cite{pertsch_fast_2025} introduces Frequency-space Action Sequence Tokenization, using the Discrete Cosine Transform (DCT) to compress high-frequency action sequences into fewer tokens. \textit{VQ-VLA} \cite{wang_vq-vla_2025} scales vector-quantized action tokenizers. Hancock et al. (2025) \cite{hancock_actions_2025} systematically analyze action representations, advocating for tokenization schemes that preserve kinematic fidelity. \textit{Spec-VLA} \cite{wang_spec-vla_2025} and \textit{CEED-VLA} \cite{song_ceed-vla_2025} adapt speculative decoding and early-exit strategies from the LLM domain to VLA policies, relaxing convergence conditions for easier steps to accelerate generation. \textit{PD-VLA} \cite{song_pd-vla_2025} integrates parallel decoding with action chunking, allowing the model to generate multiple action steps simultaneously. \textit{NinA} \cite{tarasov_nina_2025} replaces diffusion decoders with Normalizing Flows, enabling one-shot sampling that is significantly faster than iterative denoising.

\subsection{Efficient Vision-Language-Action Models}
\label{subsec:efficient_vla}

The deployment of Vision-Language-Action (VLA) models on embodied systems---ranging from mobile manipulators to humanoid robots---faces severe computational constraints. While foundation models offer strong generalization, large parameter scales can incur latencies that violate real-time control needs in dynamic manipulation. Consequently, a dedicated wave of research in 2025 has focused on ``VLA efficiency,'' developing specialized architectures and acceleration protocols that exploit spatiotemporal redundancy in robotic tasks.

\subsubsection{Efficient Architectures for Edge Deployment}
Deploying VLAs on the edge requires aggressive optimization of the architecture itself. Budzianowski et al. (2025) introduced \textit{EdgeVLA} \cite{budzianowski_edgevla_2025}, a framework tailored for resource-constrained inference. The paper reports substantial latency reduction through non-autoregressive design choices and selective quantization.
Complementing this, Wen et al. (2025) proposed \textit{TinyVLA} \cite{wen_tinyvla_2025}, showing that compact distilled models can preserve useful manipulation ability at lower compute cost. To scale capacity without proportional inference-cost growth, Du et al. (2025) developed \textit{HiMoE-VLA} \cite{du_himoe-vla_2025}. This hierarchical Mixture-of-Experts architecture activates only a subset of experts for each token, improving compute efficiency while retaining specialization.

Addressing the need for accessible robotics, Shukor et al. (2025) \cite{shukor_smolvla_2025} proposed \textit{SmolVLA}, demonstrating that highly optimized, smaller-scale architectures can achieve competitive performance through high-quality data curation, effectively democratizing VLA deployment. This push for efficiency is mirrored in architectural innovations like \textit{HyperVLA} \cite{xiong_hypervla_2025}, which utilizes hypernetworks to generate lightweight, task-specific policies on the fly, and \textit{MoLe-VLA} \cite{zhang_mole-vla_2025}, which implements a layer-skipping Mixture-of-Experts strategy to dynamically adjust computational expenditure based on task complexity.

\subsubsection{Temporal Acceleration and Token Caching}
Beyond static compression, researchers have leveraged the high temporal correlation inherent in robotic control loops. Liu et al. (2025) introduced \textit{TTF-VLA} \cite{liu_ttf-vla_2025}, which employs Temporal Token Fusion. Recognizing that consecutive frames in high-frequency control often contain limited semantic change, TTF-VLA merges redundant visual tokens across timesteps and processes scene ``deltas'' more efficiently.
Similarly, Xu et al. (2025) proposed \textit{VLA-Cache} \cite{xu_vla-cache_2025}, an adaptive token-caching mechanism analogous to KV-caching in LLMs but specialized for action sequences. Their reported results show meaningful latency reduction with limited task-performance degradation.

\subsubsection{Lightweight Adaptation and Discrete Representations}
Efficient training paradigms are as critical as inference acceleration. Goyal et al. (2025) established \textit{VLA-0} \cite{goyal_vla-0_2025}, a rigorous baseline that treats actions simply as text tokens without specialized heads. VLA-0 demonstrates that stripping away complex auxiliary losses often yields superior performance, serving as a minimalist standard for benchmarking efficiency claims.
For adapting pre-trained VLMs to robotic tasks, Li et al. (2025) introduced \textit{ControlVLA} \cite{li_controlvla_2025}, which employs a ControlNet-style architecture with zero-initialized projection layers. This supports few-shot adaptation with reduced forgetting of VLM priors. Li et al. (2025) also proposed \textit{HAMSTER} \cite{li_hamster_2025}, a hierarchical framework that separates high-level semantic planning (predicting coarse 2D paths) from low-level motor control. By offloading complex reasoning to a lower-frequency planner, HAMSTER keeps high-frequency motor control lightweight and responsive.
Optimizing the action representation itself, Liang et al. (2025) explored \textit{Discrete Diffusion VLA} \cite{liang_discrete_2025}, which formulates action generation as a discrete diffusion process to break the autoregressive bottleneck. Liang et al. (2025) also introduced \textit{PixelVLA} \cite{liang_pixelvla_2025}, which operates directly in pixel space or discrete latent codes to bypass complex kinematic state estimators.
Finally, efficient multi-modal integration has emerged as a key theme. Wei et al. (2025) presented \textit{Audio-VLA} \cite{wei_audio-vla_2025}, efficiently adding contact audio perception to improve interaction robustness, while Tan et al. (2025) developed \textit{Interactive VLA} (RIPT-VLA) \cite{tan_interactive_2025}, a post-training paradigm that optimizes policies using sparse binary rewards from user interactions, streamlining the adaptation loop.

Expanding the sensory horizon, Zhang et al. (2025) \cite{zhang_vtla_2025} introduced \textit{VTLA} (Vision-Tactile-Language-Action), a multimodal framework that deeply fuses tactile feedback with visual-linguistic streams to handle occlusion and precise manipulation. Wei et al. (2025) \cite{wei_audio-vla_2025} further augmented this sensory suite with \textit{Audio-VLA}, incorporating contact-rich audio signals to improve interaction robustness in environments where visual feedback is insufficient.

\subsection{Theme V: Generalization and Robustness}
The ultimate goal of VLA research is "Generalist" behavior---the ability to operate robustly across diverse environments, embodiments, and tasks without extensive retraining.

\subsubsection{Cross-Embodiment and Cross-Task Transfer}
\textit{X-VLA} \cite{zheng_x-vla_2025} tackles the challenge of diverse robot morphologies by using soft prompts to adapt a single transformer backbone to multiple embodiments. \textit{MergeVLA} \cite{fu_mergevla_2025} explores model merging techniques, attempting to combine VLA experts trained on disjoint skills into a single generalist agent by mitigating parameter interference. \textit{Galaxea} \cite{jiang_galaxea_2025} provides a massive open-world dataset and a dual-system framework (G0) to support this cross-embodiment learning. \textit{ObjectVLA} \cite{zhu_objectvla_2025} focuses on object-level generalization, leveraging vision-language data to manipulate unseen objects without requiring specific demonstrations. \textit{MimicDreamer} \cite{li_mimicdreamer_2025} bridges the human-robot domain gap by aligning human videos with robot kinematics, creating a scalable source of pre-training data. \textit{Scalable VLA Pretraining} \cite{li_scalable_2025} similarly leverages large-scale human hand activity videos. Yang et al. (2025) \cite{yang_beyond_2025} push beyond standard VLA architectures to handle extreme embodiment shifts, while Ye et al. (2025) \cite{ye_learning_2025} focus on efficient learning strategies for rapid adaptation. In the realm of open-world interaction, Zhou et al. (2025) \cite{zhou_chatvla-2_2025} presented \textit{ChatVLA-2}, which leverages its dialogue capabilities to facilitate open-world embodied reasoning, allowing the agent to generalize to new tasks and embodiments through interactive feedback loops. For long-horizon tasks, Fan et al. (2025) \cite{fan_long-vla_2025} introduced \textit{Long-VLA}, specifically designed to maintain coherent behavior across extended manipulation sequences. Complementing this, \textit{Interleave-VLA} \cite{fan_interleave-vla_2025} enhances instruction following by processing interleaved image-text prompts, bridging the gap between human demonstration formats and robotic execution.

\subsubsection{Robustness to Perturbations and Distribution Shift}
Despite high benchmark scores, VLAs can be brittle in deployment. \textit{Eva-VLA} \cite{liu_eva-vla_2025} and \textit{LIBERO-Plus} \cite{fei_libero-plus_2025} provide rigorous stress tests, revealing that models often fail under modest viewpoint changes, visual distractors, or instruction perturbations. \textit{VLA-Pilot} \cite{li_towards_2025} introduces an inference-time policy steering method to improve zero-shot deployment robustness without fine-tuning. \textit{ReconVLA} \cite{song_reconvla_2025} addresses the "dispersed attention" problem. Bendikas et al. (2025) \cite{bendikas_focusing_2025} similarly propose focusing mechanisms to stabilize attention. For deployment adaptation, Hancock et al. (2025) \cite{hancock_run-time_2025} introduce techniques for run-time calibration.

\textit{VLA$^2$} \cite{zhao_vla2_2025} leverages external retrieval modules to handle unseen concepts, boosting success rates on hard-generalization tasks. Pugacheva et al. (2025) \cite{pugacheva_bring_2025} systematically analyzed the impact of irrelevant context in language instructions ("Bring the apple, not the sofa"), highlighting the need for robust instruction filtering. \textit{Align-Then-Steer} \cite{zhang_align-then-steer_2025} introduces a framework to adapt VLAs to new domains by aligning action spaces via a unified latent space. \textit{FPC-VLA} \cite{yang_fpc-vla_2025} integrates a supervisor for failure prediction and correction, enhancing reliability in open-ended tasks.

\subsection{Vision-Language-Action Models for Autonomous Driving}
\label{subsec:driving_vla}

The paradigm of Vision-Language-Action (VLA) models has recently permeated the domain of autonomous driving, marking a departure from traditional modular stacks of perception, prediction, and planning. While VLA architectures for robotic manipulation focus on dexterity and object interaction, driving-specific VLAs must contend with high-speed dynamics, multi-agent interactions, and strict safety-critical latency requirements. The literature in 2025 and 2026 reflects a concerted effort to adapt large-scale pre-trained models to these constraints, leveraging the semantic understanding of VLMs to handle long-tail corner cases and ambiguous traffic rules.

\subsubsection{Benchmarks and Reasoning-Enhanced Architectures}
The shift towards end-to-end learning in driving has necessitated new evaluation methodologies. Hao et al. (2025) \cite{hao_driveaction_2025} introduced the \textit{DriveAction} benchmark, which serves as a standardized testbed for assessing VLA agents. Unlike traditional metrics focused solely on collision rates or displacement error, \textit{DriveAction} evaluates the semantic alignment of agent behaviors with language instructions and traffic laws, highlighting the unique value proposition of VLA models in interpreting complex scene contexts.

To address the "black box" nature of end-to-end driving, recent works have integrated explicit reasoning modules. Yuan et al. (2025) \cite{yuan_autodrive-r2_2025} proposed \textit{AutoDrive-R2}, a hierarchical VLA that employs a "reasoning-through-planning" mechanism. By explicitly generating textual explanations and strategic plans before outputting low-level control commands, \textit{AutoDrive-R2} achieves a form of interpretable "System 2" driving, allowing for better error analysis and trust calibration. Complementing this reasoning focus, Guo et al. (2025) \cite{guo_vdrive_2025} developed \textit{VDRive}, which combines VLA pre-training with reinforcement learning. \textit{VDRive} specifically targets the alignment of VLA outputs with safety constraints, demonstrating that RL fine-tuning can suppress the hallucinated or unsafe behaviors sometimes exhibited by generative foundation models, thus marrying the generalization of VLAs with the robustness of control theory.

\subsubsection{World Models as Driving Simulators}
The scale of data required for robust autonomous driving has driven the adoption of World Models as engines for simulation and prediction. Wang et al. (2025) \cite{wang_exploring_2025} explored the utility of generative world models in synthesizing rare and dangerous driving scenarios. Their work shows that agents trained on "dreamt" data covering adverse weather and adversarial traffic patterns generalize significantly better to real-world anomalies, effectively hallucinating safety-critical training data that is too dangerous to collect physically.

Jiang et al. (2025) \cite{jiang_better_2025} further advanced this domain by investigating the fidelity of world model representations. Their research, "Better Driving Understanding," posits that high-fidelity video prediction alone is insufficient for driving; instead, world models must capture the latent causal structure of the environment. By enforcing consistency in the physics and intent of traffic agents within the world model, they enable VLA agents to perform long-horizon planning with greater reliability. These contributions collectively signal a move towards "Generative Autonomy," where driving policies are distilled from foundational world models that continuously simulate and refine the agent's understanding of the road.

\subsection{VLA Architecture Variants and Specializations}
As the core principles of Vision-Language-Action models stabilize, research in late 2025 and early 2026 has diverged into specialized architectural variants designed to address specific limitations in spatial reasoning, cognitive depth, and domain adaptability. This section surveys these emerging variants, which move beyond the generalist paradigm to optimize for specific axes of performance.

\subsubsection{Spatial and 3D Grounding}
While standard VLAs operate primarily in 2D pixel space, accurate manipulation requires precise 3D understanding. Li et al. (2025) \cite{li_spatial_2025} introduced \textit{Spatial VLA}, which explicitly encodes geometric constraints into the token sequence, allowing for finer control in cluttered environments. Extending this to full volumetric understanding, Bhat et al. (2025) \cite{bhat_3d_2025} proposed diverse 3D VLA approaches that consume point cloud data directly, bypassing the information loss inherent in 2D projections. Feng et al. (2025) \cite{feng_spatial-aware_2025} further refined this with a spatial-aware VLA architecture that fuses depth information with semantic tokens to enhance relative positioning accuracy.

\subsubsection{Reasoning and Cognitive Architectures}
To bridge the gap between high-level planning and low-level control, recent works have embedded structured reasoning processes directly into the VLA backbone. Li et al. (2025) \cite{li_coa-vla_2025} developed \textit{Chain-of-Action VLA} (CoA-VLA), which generates intermediate action-reasoning traces before committing to physical motor commands. Similarly, Song et al. (2025) \cite{song_rationalvla_2025} introduced \textit{RationalVLA}, which explicitly models the rationale behind action selection to improve interpretability and error recovery. Yin et al. (2025) \cite{yin_deepthinkvla_2025} pushed this further with \textit{DeepThinkVLA}, integrating a deep reasoning module that performs iterative state evaluation for long-horizon tasks.

\subsubsection{Multi-Modal and Hybrid Approaches}
Hybrid architectures seek to combine the strengths of different control paradigms. Fang et al. (2025) \cite{fang_dualvla_2025} proposed \textit{DualVLA}, a dual-stream architecture that processes high-frequency motor control separately from low-frequency semantic planning. This concept is paralleled by Liu et al. (2025) \cite{liu_hybridvla_2025}, who explored a \textit{HybridVLA} framework that fuses discrete and continuous action representations. Jin et al. (2025) \cite{jin_dual-actor_2025} introduced a Dual-Actor VLA system, employing specialized actor heads for coarse and fine manipulation phases to decouple gross motion from precise interaction.

\subsubsection{Domain-Specific Adaptations}
Generalist models often struggle with the nuances of specific high-stakes domains, leading to the rise of task-specialized VLAs. Deng et al. (2025) \cite{deng_graspvla_2025} optimized \textit{GraspVLA} specifically for the physics of grasping diverse objects, achieving higher stability than general-purpose baselines. In healthcare, Li et al. (2025) \cite{li_robonurse-vla_2025} tailored \textit{RoboNurse-VLA} to handle the delicate interactions and safety constraints required for patient care. For aerial robotics, Lykov et al. (2025) \cite{lykov_cognitivedrone_2025} presented \textit{CognitiveDrone}, adapting VLA principles to the 6-DOF dynamics of drone flight. In the high-speed domain, Serpiva et al. (2025) \cite{serpiva_racevla_2025} developed \textit{RaceVLA}, optimizing latency and lookahead for autonomous racing. Li et al. (2025) \cite{li_jarvis-vla_2025} proposed \textit{JARVIS-VLA}, a specialized assistant architecture designed for complex, multi-stage household management tasks. For humanoid control, Ding et al. (2025) \cite{ding_humanoid-vla_2025} introduced \textit{Humanoid-VLA}, achieving universal full-body control with visual integration, while Jiang et al. (2025) \cite{jiang_wholebodyvla_2025} developed \textit{WholeBodyVLA} for coordinated loco-manipulation. Chen et al. (2026) \cite{chen_combatvla_2026} extended VLA to gaming with \textit{CombatVLA}, demonstrating efficient control in 3D action role-playing environments.

\subsubsection{Interactive and Adaptive Models}
The final frontier involves models that can communicate and adapt during deployment. Zhou et al. (2025) \cite{zhou_chatvla_2025} pioneered \textit{ChatVLA}, enabling users to adjust robot behavior in real-time through natural language dialogue, followed by the enhanced \textit{ChatVLA-2} \cite{zhou_chatvla-2_2025} which supports multi-turn context correction. Li et al. (2025) \cite{li_switchvla_2025} introduced \textit{SwitchVLA}, a dynamic architecture that can switch between specialist modules based on task requirements. The concept of omni-modal integration was explored by Hirose et al. (2025) \cite{hirose_omnivla_2025} and Guo et al. (2025) \cite{guo_omnivla_2025} with their respective \textit{OmniVLA} implementations, both aiming to unify audio, vision, and text into a cohesive sensorimotor policy. Jang et al. (2025) \cite{jang_contextvla_2025} focused on context-aware adaptation with \textit{ContextVLA}, while Koo et al. (2025) \cite{koo_retovla_2025} and Dey et al. (2025) \cite{dey_revla_2025} proposed \textit{RetoVLA} and \textit{ReVLA} respectively, emphasizing retrieval-augmented mechanisms to adapt to novel environments by referencing past experiences.
