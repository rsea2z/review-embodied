\section{Challenges and Future Directions}
\label{sec:challenges}

The rapid evolution of World Models for Embodied AI has fundamentally shifted the research landscape from proving algorithmic feasibility to addressing the rigorous demands of scalability, reliability, and safety in open-world deployment. While pioneering works demonstrated that generative models could simulate simplified game environments \textbf{\cite{li_comprehensive_2025}} or controlled robotic tasks, the transition to unstructured, real-world operation introduces a new class of challenges. As agents move from being "passive dataset consumers" to "active world modelers," the critical bottlenecks are no longer just representational capacity, but rather the fidelity of physical reasoning, the rigorous assurance of safety under uncertainty, and the ethical implications of persistent visual sensing \textbf{\cite{li_comprehensive_2025}}\textbf{\cite{ding_understanding_2025}}.

This section provides a strategic analysis of these hurdles, synthesizing technical limitations with broader societal impacts. We argue that the field is currently facing a "Physical Grounding Gap," where the semantic understanding of Foundation Models (FMs) vastly outpaces their kinematic and dynamic reliability. Bridging this gap requires not just larger models, but a fundamental rethinking of how we integrate perception, reasoning, and control.

\subsection{Physical Consistency and the ``Dream'' Problem}
\label{subsec:dream_problem}

The most pervasive technical challenge facing current generative world models is the tension between visual fidelity and physical consistency, a phenomenon often referred to as the \textbf{``Dream'' Problem}. State-of-the-art video generation models, primarily driven by diffusion architectures, demonstrate exceptional photorealism, capable of rendering complex textures, lighting, and semantic details with high fidelity. However, they frequently fail to uphold fundamental physical laws over extended time horizons, leading to simulated futures that are visually plausible but physically impossible \textbf{\cite{ye_dream-vl_2026}}\textbf{\cite{cen_rynnvla-002_2025}}.

\subsubsection{Hallucination of Physics}
Unlike Large Language Models (LLMs) where ``hallucination'' results in factual errors or semantic drift that may still be grammatically correct, hallucination in world models manifests as violations of conservation laws and geometric constraints. Common failure modes include:
\begin{itemize}
    \item \textbf{Object Permanence Failures:} Objects spontaneously disappearing when occluded or reappearing in geometrically impossible locations. This is particularly problematic for manipulation tasks where an agent must track objects even when they are hidden by a robotic arm \textbf{\cite{chen_bridgev2w_2026}}.
    \item \textbf{Rigid Body Interpenetration:} Solid objects passing through one another or the robot end-effector clipping through table surfaces, violating collision constraints. This ``ghosting'' effect renders the world model useless for planning contact-rich interactions.
    \item \textbf{Inconsistent Contact Dynamics:} Deformable objects (e.g., cloth, fluids) exhibiting ``floaty'' or non-Newtonian behavior that defies gravity and friction. Liquids may flow upwards, and rigid blocks may deform like jelly under pressure.
\end{itemize}

This inconsistency arises fundamentally because most current architectures optimize for pixel-level reconstruction likelihood rather than state-space physical validity \textbf{\cite{won_dual-stream_2025}}. The standard Diffusion Loss $\mathcal{L}_{\text{diff}}$ focuses on denoising score matching:
\begin{equation}
    \mathcal{L}_{\text{diff}} = \mathbb{E}_{x_0, \epsilon, t} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]
\end{equation}
This objective ensures that the generated image $x_0$ looks statistically similar to the training distribution, but it imposes no explicit penalty for violating Newton's laws. For example, a cup floating in mid-air is a perfectly valid "image" under the distribution of internet photos (which may contain magic tricks or special effects), but it is an invalid state for a robot planning a pick-and-place task.

\subsubsection{Neural Physics vs. Symbolic Constraints}
Overcoming this requires a paradigm shift toward \textbf{Physics-Informed Generative Learning}. Ray et al. \cite{ray_physical_2025} argue that achieving true embodied intelligence requires physical reasoning capabilities that transcend statistical correlations. They propose mechanisms to explicitly model causal physical properties, ensuring that generated predictions respect mass, momentum, and friction. Similarly, Zhang et al. \cite{zhang_vlac_nodate} introduce VLAC, a framework that integrates rigorous constraint satisfaction into the generative process. By enforcing "hard" constraints on the latent space dynamics, VLAC mitigates the "dream" problem, ensuring that the model's imagination remains grounded in feasible kinematic states. This represents a broader trend towards \textit{Neural-Symbolic World Modeling}, where models must learn to respect "hard" constraints (gravity, solidity) while maintaining the "soft" flexibility of neural representations.

\subsection{Safety Alignment and Constrained Control}
\label{subsec:safety}

As embodied agents are deployed in human-centric environments, ensuring safe interaction is paramount. Traditional safety constraints in Reinforcement Learning (RL) often fail to generalize to the open world. In robotics, safety is inherently \textit{state-dependent} and dynamic; an action that is safe in an isolated workspace (e.g., high-speed swinging) becomes hazardous in the presence of humans or fragile objects.

\subsubsection{Mathematical Formulation: CMDPs}
We formally frame this as a \textbf{Constrained Markov Decision Process (CMDP)}, where the objective is to maximize the expected return $J(\pi)$ subject to a safety cost limit $\delta$. The optimization problem is defined as:

\begin{equation}
    \max_{\pi} J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^T \gamma^t r(s_t, a_t) \right]
\end{equation}

\begin{equation}
    \text{subject to} \quad J_C(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^T \gamma^t c(s_t, a_t) \right] \le \delta
\end{equation}

Here, $c(s_t, a_t)$ represents a safety cost function (e.g., distance to obstacle, force magnitude, or human proximity). Solving this often involves Lagrangian relaxation, converting the constrained problem into an unconstrained dual problem:
\begin{equation}
    \min_{\lambda \ge 0} \max_{\pi} \left( J(\pi) - \lambda (J_C(\pi) - \delta) \right)
\end{equation}
where $\lambda$ is the Lagrange multiplier that penalizes safety violations. The multiplier is updated iteratively via gradient ascent:
\begin{equation}
    \lambda_{k+1} \leftarrow \max(0, \lambda_k + \alpha (J_C(\pi_k) - \delta))
\end{equation}
This adaptive penalty ensures that the agent prioritizes safety only when necessary, avoiding overly conservative behaviors that stifle task performance.

\subsubsection{Safety Critics and Verification}
In the context of World Models, we require a \textbf{Safety Critic} or \textbf{Safety World Model} capable of predicting the probability of future constraint violations $\mathcal{P}(C > \delta | s_t, a_{t:t+H})$ \textit{before} execution. Recent works like SafeVLA \cite{zhang_safevla_2025} and RECAP \textbf{\cite{intelligence__06_2025}} incorporate these critics into the sampling process. By learning a safety value function $Q_{safe}(s, a)$ alongside the task value function, these models can effectively ``veto'' dangerous trajectories in the latent planning space.

However, a critical challenge remains in \textbf{Distributional Shift}: a safety critic trained on standard data may fail to recognize edge-case hazards. Addressing this, Zhang et al. \cite{zhang_robustvla_2025} propose RobustVLA, which explicitly optimizes for worst-case performance under adversarial perturbations, ensuring that safety guarantees hold even when environmental conditions deviate from the training distribution. Complementing this, Liu et al. \cite{liu_eva-vla_2025} introduce Eva-VLA, a rigorous evaluation framework designed to stress-test VLA models under extreme variations in lighting, texture, and physics parameters. These approaches underscore the necessity of \textbf{Conservative Uncertainty Estimation}, ensuring that the model defaults to caution when ensuring physical safety in novel states. Furthermore, techniques like Reinforcement Learning from Verifiable Rewards (RLVR) are emerging to ground safety not just in human preference, but in verifiable physical metrics \textbf{\cite{ye_vla-r1_2025}}\textbf{\cite{zhang_compliantvla-adaptor_2026}}\textbf{\cite{hung_nora-15_2025}}.

\subsection{Interpretability and Mechanistic Transparency}
\label{subsec:interpretability}

As World Models grow in complexity, their internal decision-making processes become increasingly opaque, posing a significant barrier to trust and certification. Unlike traditional control theory, where stability margins are mathematically explicit, deep generative models often function as black boxes. Haon et al. \cite{haon_mechanistic_2025} pioneeringly apply \textbf{Mechanistic Interpretability} to embodied agents, attempting to decompose high-dimensional neural activations into understandable physical concepts such as "mass," "velocity," or "obstacle." By identifying the specific circuits responsible for physical reasoning, they aim to verify whether the model is genuinely learning physics or merely overfitting to visual patterns.

Building on this, Wang et al. \cite{wang_mechanistic_2026} introduce \textbf{Mechanistic Steering}, a technique that allows for active intervention in the model's latent space. By locating the direction vectors corresponding to specific physical properties (e.g., "friction coefficient"), they demonstrate the ability to "steer" the model's behavior, correcting physical misconceptions (such as underestimating an object's weight) before they manifest as unsafe actions. This line of research is critical for moving from "black-box" neural networks to "glass-box" systems where safety properties can be audited and guaranteed.

\subsection{Compute Constraints and Real-Time Inference}
\label{subsec:compute}

The computational cost of World Models presents a severe bottleneck for deployment. Real-time robotic control typically requires control loops running at 10Hz to 50Hz (20ms-100ms latency) to maintain stability during contact-rich tasks. However, high-fidelity autoregressive video prediction or diffusion denoising can take seconds per frame on consumer hardware, creating a massive "System 1 vs System 2" gap \textbf{\cite{yu_ac2-vla_2026}}.

\subsubsection{The Autoregressive Bottleneck}
The high dimensionality of visual observations means that standard "Next-Token Prediction" for video involves processing millions of pixels. Even with VQ-VAE compression, the sequence length for a few seconds of video can explode, making autoregressive generation prohibitively slow for closed-loop control. This latency introduces "blind" intervals where the robot acts on outdated information, leading to instability \textbf{\cite{xie_dynamicvla_2026}}.

\subsubsection{Accelerating Inference}
Several strategies are emerging to address this:
\begin{itemize}
    \item \textbf{Frequency-Domain Tokenization:} Approaches like FAST \textbf{\cite{pertsch_fast_2025}} move away from raw pixel space, encoding actions and dynamics in frequency domains (e.g., Discrete Cosine Transform - DCT) to compress the sequence length. By predicting the top-$k$ spectral coefficients rather than the full trajectory, FAST reduces the token count by orders of magnitude without losing high-frequency control information.
    \item \textbf{Speculative Decoding for Robotics:} Inspired by LLM acceleration, Spec-VLA \cite{wang_spec-vla_2025} utilizes a small, fast "draft" model to propose trajectories which are then verified in parallel by a larger, more accurate world model. The draft model $\mathcal{M}_{draft}$ runs at high frequency (e.g., 50Hz), while the verifier $\mathcal{M}_{verify}$ corrects deviations at a lower frequency (e.g., 5Hz), effectively amortizing the cost of the large model.
    \item \textbf{Adaptive Computation:} Frameworks like AC$^2$-VLA selectively activate model components based on context, allocating more compute to novel or complex situations while using lightweight paths for routine actions \textbf{\cite{yu_ac2-vla_2026}}.
    \item \textbf{Efficient Pruning:} Post-training recovery methods like GLUESTICK \textbf{\cite{jabbour_dont_2025}} restore functionality in pruned VLA models, enabling deployment on edge devices without sacrificing safety.
\end{itemize}

Despite these advances, running a multi-billion parameter VLA model like $\pi_0$ \textbf{\cite{black__0_2026}} onboard a mobile robot remains a systems engineering challenge, pushing the field towards \textbf{Cloud-Edge Co-Design}, where heavy world modeling occurs in the cloud while safety-critical reflex loops run locally.

\subsection{Data Quality and the Supervision Deficit}
\label{subsec:data_quality}

As the field moves toward "scaling laws" for embodied intelligence, data \textit{quality} has emerged as a critical bottleneck. The reliance on internet-scale video data (e.g., YouTube, Ego4D) offers vast diversity but introduces severe noise \textbf{\cite{lu_multimodal_2025}}. We face a "Supervision Deficit" where we have abundant pixels but scarce actions.

\subsubsection{The Action Label Deficiency}
Most internet videos lack proprioceptive or action labels. Inferring the precise force or torque applied by a human hand from pixels alone is an ill-posed inverse problem. A model trained on video might learn \textit{that} a cup moves, but not the subtle impedance control required to move it without spilling. This "Outcome vs. Action" gap limits the utility of passive video for learning contact-rich skills \cite{tharwat_latent_2025}.

\subsubsection{World Models as Active Data Generators}
To bridge this gap, World Models are increasingly used as \textbf{Active Data Engines}. Models like GigaBrain \cite{team_gigabrain-0_2025}, DriveVLA-W0 \textbf{\cite{li_drivevla-w0_2025}}, and World-Env \textbf{\cite{xiao_world-env_2025}} use the world model to generate synthetic training data, creating diverse, counterfactual scenarios ("what-if" reasoning) to bootstrap policy learning. This "Dreamer" paradigm allows agents to practice dangerous or rare skills (e.g., catching a falling vase) without real-world risk. By hallucinating physically consistent variations of tasks, these models effectively multiply the available training data, converting limited real-world demonstrations into massive synthetic datasets.

Recent datasets like Galaxea \textbf{\cite{jiang_galaxea_2025}} and techniques like Interleave-VLA \cite{fan_interleave-vla_2025} are also enhancing the semantic richness of training data by combining cross-embodiment demonstrations with interleaved image-text instructions, further mitigating the supervision deficit.

\subsection{Standardized Evaluation and Benchmarking}
\label{subsec:evaluation}

The evaluation of World Models for Embodied AI is fraught with inconsistency. Unlike Computer Vision, where static datasets (e.g., ImageNet) suffice, embodied agents require dynamic, interactive evaluation. Wang et al. \cite{wang_vlatest_2025} propose \textbf{VLATest}, a standardized benchmarking suite designed to assess long-horizon reasoning, physical consistency, and robustness across diverse manipulation tasks. VLATest moves beyond simple success rates, introducing fine-grained metrics for kinematic fidelity and error recovery.

However, Xie et al. \cite{xie_emerging_nodate} highlight \textbf{Emerging Challenges} in evaluation, arguing that current benchmarks are often too narrow to capture the open-ended nature of real-world interaction. They identify a need for benchmarks that can evaluate "physical common sense" and the ability to handle unforeseen physical phenomena (e.g., liquid dynamics, plastic deformation). This calls for a shift towards \textit{Procedural Metrology}, where evaluation environments are generated procedurally to test specific capabilities, rather than relying on fixed task sets.

\subsection{Ethical Implications of Egocentric Perception}
\label{subsec:ethics}

The deployment of agents equipped with persistent, ego-centric World Models raises significant privacy and ethical concerns. Unlike fixed security cameras, embodied agents are mobile, interactive, and often operate in private spaces (homes, offices).

\subsubsection{Privacy in the Loop}
A robot that builds a "World Model" of a user's home effectively creates a searchable, 3D semantic database of their private life. It knows where valuables are stored, daily routines, and social interactions. If this model is uploaded to the cloud for inference (as per Section \ref{subsec:compute}), it creates vectors for surveillance capitalism. Techniques for \textbf{Privacy-Preserving World Modeling}, such as federated learning or running semantic abstraction on-device before transmission, are essential to ensure user trust \textbf{\cite{liang_large_2025}}.

\subsubsection{Bystander Consent and Gaze}
Egocentric data collection inevitably captures non-consenting bystanders. While face blurring is a standard mitigation, it introduces a technical trade-off: gaze detection is crucial for human-robot interaction (HRI) and safety. Anonymizing faces destroys the very signal needed to predict human intent. Resolving this tension---perhaps through "feature-preserving anonymization" that retains gaze vectors but masks identity---is an open ethical-technical challenge.

\subsection{Sovereign Physical Intelligence}
\label{subsec:sovereign}

As World Models become the "operating system" of physical reality, their ownership becomes a matter of geopolitical strategy. We term this \textbf{Sovereign Physical Intelligence}. Just as nations are concerned with sovereign capability in LLMs, the ability to automate physical labor, logistics, and manufacturing via Foundation World Models is a critical economic asset.

Dependency on a few proprietary "closed" World Models for national infrastructure poses risks of lock-in and reduced resilience. If the model controlling a factory's logistics requires an API call to a foreign provider, that physical infrastructure is no longer sovereign. This drives a need for \textbf{Open-Weight Physical Models} (like the $\pi$ series \textbf{\cite{intelligence__05_2025}}\textbf{\cite{black__0_2026}}, OpenVLA, or VLA-Adapter \cite{wang_vla-adapter_2025}) that allow nations and industries to fine-tune and run controls entirely on-premise, ensuring security and stability of the physical supply chain.

\subsection{Future Directions: Towards Persistent Intelligence}
\label{subsec:future}

Looking toward 2026 and beyond, we anticipate several key trajectories that will define the next generation of Embodied AI.

\textbf{World Models as Active Data Engines:} We expect a shift from training \textit{on} data to training \textit{inside} data. High-fidelity World Models will serve as infinite generators of diverse, counterfactual scenarios ("what-if" reasoning) to bootstrap policy learning. This "Dreamer" paradigm allows agents to practice dangerous or rare skills without real-world risk.

\textbf{Unified Neuro-Symbolic Architectures:} Future architectures will likely dissolve the boundary between the "planner" (LLM) and the "controller" (Policy). Instead of chaining separate modules, we foresee Unified Neuro-Symbolic Transformers that process tokenized text, video, and action commands in a shared latent space. These models will possess both the semantic "common sense" of System 2 thinking ("Why should I clean this?") and the precise System 1 motor control ("How do I grasp this handle?") \textbf{\cite{pertsch_fast_2025}}\textbf{\cite{zheng_jarvis_2025}}.

\textbf{Lifelong Learning Strategies:} Finally, the static "train-then-deploy" paradigm must evolve into \textbf{Lifelong Learning}. Embodied agents will encounter novel objects and physics daily. They must be capable of \textbf{In-Context Physical Adaptation}---updating their internal physics model on-the-fly based on prediction errors (e.g., realizing a package is heavier than it looks) without catastrophic forgetting of previous skills. This moves us from specialized tools to true autonomous physical companions, capable of growing with their environment.
