\section{The Data Engine: Fueling World Models}
\label{sec:datasets}

The transition from specialized, task-specific robotic control to generalist Embodied AI has been propelled by a fundamental shift in data strategy: the move from small-scale, domain-specific datasets to massive, heterogeneous \textbf{Data Engines}. Just as Large Language Models (LLMs) rely on internet-scale text to learn the statistical structure of language, World Models for Embodied AI require a continuous, diverse stream of physical interaction data to ground their understanding of dynamics, physics, and causality. Recent comprehensive surveys \cite{zhong_survey_2025, jiang_survey_2025} \textbf{\cite{li_comprehensive_2025}}\textbf{\cite{ding_understanding_2025}}\textbf{\cite{liu_aligning_2025}}\textbf{\cite{liang_large_2025}} underscore this paradigm shift. The core premise is that a robust world model must not only predict future pixels but also understand the underlying causal mechanisms \textbf{\cite{gupta_essential_2024}} and physical laws governing the environment \textbf{\cite{fung_embodied_2025}}.

Historically, robotic datasets were constrained by the ``Moravec's Paradox'' of data collection: high-level semantic data (images, text) was abundant, but low-level physical interaction data (forces, torques, contact dynamics) was scarce. Early datasets like ImageNet provided semantic labels but no physical grounding. The robotics community responded with datasets like YCB-Video and GraspNet, which offered physical object models but lacked the diversity of real-world environments. The 2020s saw the rise of ``Action-Labelled Video'' (e.g., Something-Something), yet these still lacked the precise proprioceptive state information required for control. The breakthrough came with the Open X-Embodiment (OXE) initiative \textbf{\cite{collaboration_open_2025}}, which aggregated data across embodiments, effectively creating the ``ImageNet of Robotics.'' However, OXE was largely a retrospective aggregation. The new era of ``Data Engines'' is characterized by \textit{proactive}, \textit{adversarial}, and \textit{scalable} data generation loops.

This section systematically reviews the three critical fuel sources for these models: Passive Video Data (for learning physics and semantics), Embodied Interaction Data (for learning control and causal intervention), and Synthetic Simulation Data (for scaling and infinite diversity). We further analyze the emerging paradigm of ``Adversarial Data Collection'' and the trade-offs between teleoperation and wearable data acquisition, culminating in a discussion of the unified ``Data Engine'' loop that powers state-of-the-art models like $\pi_0$ \textbf{\cite{black__0_2026}} and NVIDIA Cosmos \textbf{\cite{wang_genie_2025}}.

\subsection{Passive Video Datasets: The Foundation of Physical Priors}
Passive video data serves as the bedrock for training foundational world models. Unlike robotic interaction data, which is expensive, slow, and dangerous to collect, passive video is abundant, diverse, and covers a vast range of real-world scenarios. Models pre-trained on this data inherit ``internet-scale semantic knowledge,'' enabling them to understand object properties, human interactions, and physical dynamics before they ever control a robot.

\subsubsection{Scaling Trends in Multimodal Robot Pre-training}
Recent work consistently reports scaling behavior with both model size and data scale, while also emphasizing that \emph{data quality} and interaction density are critical, not just raw volume \textbf{\cite{li_drivevla-w0_2025}}\textbf{\cite{wang_genie_2025}}. In practice, this has shifted embodied learning from a ``collect more video'' strategy to a ``collect more physically informative video'' strategy.

In this survey, we therefore treat scaling observations as directional evidence: larger and more diverse corpora generally help, but curation quality, temporal consistency, and action relevance strongly affect downstream control performance.

\subsubsection{Physicalization of Internet Data}
The core challenge with utilizing internet video for robotic learning is that it is often ``disembodied''---curated for human consumption (e.g., vlogs, tutorials) rather than for robotic control. The \textbf{Physicalization} of internet data involves algorithmic filtering and annotation to make raw video streams useful for training world models.

\paragraph{Cosmos Filtration Metrics}
NVIDIA's Cosmos pipeline implements a rigorous three-stage filtration process to ensure that only physically meaningful data enters the training stream, rejecting content that lacks clear causal dynamics:
\begin{enumerate}
    \item \textbf{Motion Saliency and Optical Flow}: The system computes the dense optical flow field $\vec{v}(x,y,t)$ for every frame using a lightweight flow network. A clip is retained only if the mean flow magnitude $|\vec{v}|$ exceeds a dynamic threshold $\delta$ in regions of interest. This step aggressively filters out "talking heads," static slide presentations, and screen recordings, ensuring the model focuses on object motion and deformation. The threshold $\delta$ is adaptive, scaling with the resolution and frame rate of the source video to maintain physical consistency.
    \item \textbf{Contact Probability Estimation}: A specialized interaction detector network, pre-trained on hand-object interaction datasets (e.g., 100DOH), predicts the probability $P(\text{contact})$ between agent effectors (hands, tools) and environmental objects. Only clips where $P > 0.85$ are retained. This focuses the model's capacity on the mechanics of manipulation---grasping, pushing, cutting, and assembly---rather than passive observation of scenery. This filter is crucial for learning the \textit{contact dynamics} that are often glossed over in visual representations.
    \item \textbf{Viewpoint Stability and Ego-Motion}: Using RANSAC-based background matching and simultaneous localization and mapping (SLAM) techniques, the pipeline estimates camera ego-motion. It filters out video with erratic, non-physical camera cuts or heavy video editing effects that violate the ``single-world'' temporal coherence assumption essential for learning consistent physics. Stable ego-motion allows the world model to disentangle camera movement from object movement, a prerequisite for accurate state estimation.
\end{enumerate}
Overall, the key message from this line of work is that filtration is not a minor preprocessing step; it is a core part of the training signal design for embodied world models.

\subsubsection{Bridging the View Gap: Egocentric Vision and Embodiment Masks}
The domain gap between third-person internet video (e.g., YouTube cooking tutorials) and the first-person view of a robot is a primary hurdle. Third-person video provides global context but obscures fine-grained manipulation details due to occlusion and distance. Egocentric video, captured from head-mounted cameras (e.g., GoPro, Aria glasses), bridges this gap. Datasets like \textbf{Ego4D} \textbf{\cite{lu_multimodal_2025}} (3,670 hours) and \textbf{EgoVid-5M} \cite{li_embodied_2025} provide massive repositories of egocentric human behavior. These datasets capture hand-object interactions from the perspective of the agent, providing a direct mapping to the visual inputs of a humanoid or manipulator. The tasks covered range from household chores (cooking, cleaning) to skilled labor (carpentry, gardening), offering a rich source of procedural knowledge.

Furthermore, \textbf{BridgeV2W} \textbf{\cite{chen_bridgev2w_2026}} demonstrates the power of bridging video generation models to embodied control using \textbf{Embodiment Masks}. By aligning coordinate-space actions with pixel-space videos from internet sources, BridgeV2W effectively ``hallucinates'' the robot's embodiment into the video. The process involves rendering a kinematic model of the robot (e.g., a URDF of a Franka arm) over the video frames, aligned with the estimated 3D pose of the human hand. This turns passive observation into ``pseudo-active'' data, allowing world models to learn proprioceptive feedback loops from video data that originally lacked any robot presence. This technique effectively multiplies the available embodied training data by orders of magnitude.

\subsection{Language-Guided and Open-Ended Environments}
Beyond physical manipulation, the "Data Engine" must also encompass high-level semantic reasoning and long-horizon planning. Open-ended environments provide a unique source of data for training agents to reason about abstract goals.

\subsubsection{The Minecraft Crucible: Voyager and MineDojo}
Minecraft has emerged as a fertile ground for training open-ended agents due to its infinite procedural generation and rich semantic hierarchy. \textbf{MineDojo} \cite{fan_minedojo_2022} aggregates internet-scale knowledge (YouTube videos, Wiki pages, Reddit threads) to train agents capable of thousands of tasks. It provides a massive database of time-aligned video and text, allowing agents to learn the correlation between language instructions (e.g., "build a nether portal") and complex sequences of actions. Building on this, \textbf{Voyager} \cite{wang_voyager_2023} utilizes GPT-4 to iteratively write and refine code for an embodied agent, creating a self-improving curriculum. This approach generates a distinct type of data: \textit{programmatic action traces} aligned with high-level reasoning. The ``pixelated sheep'' dream of embodied agents \textbf{\cite{nottingham_embodied_2023}} is realized here, where the world model learns not just physics, but the \textit{logic} of crafting and survival.

\subsubsection{Instruction Following and Dialogue}
Data for instruction following is critical for human-robot interaction. The \textbf{TEACh} dataset \textbf{\cite{gao_dialfred_2022}} and \textbf{DialFRED} \textbf{\cite{gao_dialfred_2022}} provide rich dialogues paired with embodied tasks, allowing agents to learn from feedback. These datasets capture the back-and-forth of clarification: "Pick up the blue cup." "Do you mean the one on the left?" "Yes." This interactive data is essential for training agents that can resolve ambiguity in the real world. \textbf{LLM-Planner} \cite{song_llm-planner_2023} and \textbf{Language Models as Zero-Shot Planners} \textbf{\cite{huang_language_2022}} demonstrate how to leverage the reasoning capabilities of LLMs to generate high-level plans that can be executed by low-level policies. Frameworks like \textbf{JARVIS} \textbf{\cite{zheng_jarvis_2025}} and \textbf{CoELA} \textbf{\cite{zhang_building_2024}} extend this to multi-agent cooperation, generating data where agents must communicate to solve problems. These datasets are essential for training the ``System 2'' reasoning components of world models. Further research into multi-agent embodied systems \textbf{\cite{li_embodied_2025}}\textbf{\cite{guo_embodied_2024}}\textbf{\cite{yang_embodied_2024}} and human-agent collaboration \cite{dasgupta_collaborating_2023} \textbf{\cite{wu_embodied_2023}} highlights the growing need for datasets that capture the social dynamics of interaction, not just physical manipulation.

\subsection{Embodied Interaction Data: Learning Causal Intervention}
While passive video teaches ``what happens,'' embodied interaction data teaches ``how to make it happen.'' This data consists of trajectories $\tau = (o_t, a_t, r_t, o_{t+1})$---observations, actions, rewards, and next observations---collected from robots interacting with the physical world. This high-cost, high-value data is the "gold standard" for training the policy head of Vision-Language-Action (VLA) models.

\subsubsection{The Open X-Embodiment (OXE) Initiative and Standardization}
The Open X-Embodiment (OXE) initiative \textbf{\cite{collaboration_open_2025}} represents a watershed moment in robotic learning, aggregating data from 22 different robot embodiments across 34 research labs to provide over 1 million trajectories of real-world interaction. Prior to OXE, robotic datasets were fragmented, with incompatible action spaces (e.g., joint position vs. end-effector velocity) and observation formats. OXE standardized data formats using the \textbf{Reinforcement Learning Datasets (RLDS)} schema, which defines a unified protocol for serializing multimodal trajectories. This standardization allowed for the training of generalist policies like RT-X and $\pi_0$ \textbf{\cite{black__0_2026}}. By learning a ``Universal Action Space'' across morphologies (Franka, UR5, WidowX), these models achieved a 2x improvement in zero-shot generalization compared to single-robot training. Recent extensions like \textit{Interleave-VLA} \cite{fan_interleave-vla_2025} have further enriched this data by re-annotating 210,000 episodes with interleaved image-text instructions, enabling fine-grained language control.

\subsubsection{Distributed Data Collection: DROID}
To go beyond aggregated historical data, the \textbf{Distributed Robot Interaction Dataset (DROID)} \textbf{\cite{khazatsky_droid_2025}} established a massive, consistent dataset for single-arm manipulation. Unlike OXE, which aggregated disparate legacy datasets, DROID was collected proactively using a standardized protocol across multiple institutions.
\paragraph{DROID Collection Protocol}
To ensure consistency, DROID emphasizes common hardware/protocol templates across sites:
\begin{itemize}
    \item \textbf{Embodiment consistency}: standardized arm-centric collection setup to reduce embodiment-induced variance.
    \item \textbf{Multi-view sensing}: synchronized camera views and calibration-aware recording pipelines.
    \item \textbf{In-the-wild diversity}: broad environment and task diversity to improve out-of-distribution robustness.
\end{itemize}
This standardization minimizes the "embodiment gap" during training, allowing models to focus purely on learning environmental dynamics rather than compensating for sensor calibration errors. DROID also emphasizes ``diversity'' in object instances, ensuring that the model learns robust visual representations that generalize to unseen objects.

\subsubsection{High-Intensity Humanoid Data: AgiBot and WholeBodyVLA}
For humanoid robots, the data requirements scale with the complexity of the morphology. AgiBot pioneered high-intensity data collection for bimanual humanoids. \textit{WholeBodyVLA} \textbf{\cite{jiang_wholebodyvla_2025}} was trained on 10,000 hours of data from the AgiBot X2 humanoid, focusing on whole-body control.
\paragraph{Hardware Shadowing and Retargeting}
AgiBot utilizes an ``Efficient Human Shadowing'' system where a human wearer's motion (captured via IMUs and vision) is retargeted to the humanoid's 40-DOF kinematics in real-time. This captures rich loco-manipulation knowledge---such as advancing, turning, and squatting while grasping---that is inherently missing from stationary arm datasets. The system uses a whole-body inverse kinematics (IK) solver to map the human's task-space actions to the robot's joint-space configuration, ensuring the generated motions are feasible and stable. Similarly, \textit{LingBot-VLA} \textbf{\cite{wu_pragmatic_2026}} expanded this to 20,000 hours across 9 dual-arm configurations, targeting the ``pragmatic'' coordination gap. The \textbf{Galaxea} dataset \textbf{\cite{jiang_galaxea_2025}} further extends this by providing open-world data for mobile manipulation, pairing precise subtask-level language annotations with consistent robotic embodiment to facilitate hierarchical planning.

\subsubsection{Specialized Data for Advanced VLA Architectures}
As VLA architectures evolve, so do their data requirements. \textbf{PointVLA} \textbf{\cite{li_pointvla_2026}} introduces 3D point cloud data into the VLA training loop, addressing the limitations of 2D vision in handling depth ambiguity. Point clouds provide explicit geometry, crucial for grasping irregular objects. \textbf{DynamicVLA} \textbf{\cite{xie_dynamicvla_2026}} focuses on dynamic object manipulation, collecting data on moving targets to train predictive control policies. This dataset includes scenarios like catching thrown objects or manipulating tools on a moving conveyor belt, challenging the temporal resolution of standard VLAs. \textbf{CompliantVLA} \textbf{\cite{zhang_compliantvla-adaptor_2026}} incorporates force/torque feedback data, enabling the learning of variable impedance control for contact-rich tasks. By recording the interaction forces during successful demonstrations (e.g., wiping a surface, inserting a peg), the model learns to modulate its stiffness, achieving safer and more robust manipulation. Furthermore, research into the architecture itself, such as \textbf{VLM4VLA} \cite{zhang_vlm4vla_2026} and \textbf{ACoT-VLA} \cite{zhong_acot-vla_2026}, highlights the need for data that supports "Action Chain-of-Thought" reasoning, where intermediate rationales are annotated alongside the final action.

\subsubsection{Quantifying Data Diversity and Coverage}
A critical unresolved challenge is defining a rigorous metric for "data diversity." Mere volume (terabytes) does not equate to information content. Recent works have proposed \textbf{Task-Space Entropy} as a diversity metric, estimating the coverage of the state-action space covered by a dataset. For instance, the \textbf{Galaxea} dataset utilizes a hierarchical taxonomy of 500+ atomic skills (e.g., ``grasp-mug,'' ``open-fridge'') to ensure uniform distribution across semantic categories. In contrast, uncurated datasets often suffer from a ``long-tail'' distribution, where common actions (walking, pick-and-place) are overrepresented while critical safety-critical behaviors (recovery from falls, emergency stops) are rare. \textbf{Eva-VLA} \textbf{\cite{liu_eva-vla_2025}} introduces a benchmark for evaluating robustness under physical variations, implicitly measuring the diversity of the training data's domain randomization. Future Data Engines must incorporate active learning loops that explicitly maximize this diversity metric, querying for data in under-represented regions of the state space.

\subsubsection{Data Efficiency and Advanced Representations}
As the parameter counts of World Models escalate, the efficiency of the "Data Engine" becomes a governing constraint. A comprehensive 2026 survey by Yu et al. \cite{yu_survey_2026} establishes a unified taxonomy for "Efficient Vision-Language-Action Models," dissecting the pipeline into efficient model design, training, and, crucially, \textit{efficient data collection}. Yu et al. highlight that current datasets often suffer from high redundancy and low "causal density." They advocate for active learning frameworks where the robot explicitly seeks novel interactions—a strategy that moves beyond the passive ingestion of internet scale data. This analysis suggests that the next leap in performance will come not just from more data, but from \textit{better} data that targets the sparse regions of the agent's state-space.

This shift mirrors the maturation of data strategies in other high-stakes scientific domains. The "Consortium Model" championed by the AACR Project GENIE \cite{pugh_aacr_2022} in oncology offers a compelling blueprint for Embodied AI. Pugh et al. \cite{pugh_aacr_2022} demonstrated that aggregating standardized, clinical-grade data from disparate institutions—while maintaining rigorous metadata standards—could unlock insights unavailable to any single entity. In robotics, this "AACR approach" validates the trajectory of projects like Open X-Embodiment. It emphasizes that the true value of a dataset lies in its schema consistency and the harmonization of heterogeneous sources (e.g., aligning action spaces across Franka and UR5 robots), rather than raw byte count. The lesson from \cite{pugh_aacr_2022} is that data utility scales with standardization, not just accumulation.

Complementing these organizational strategies are technical innovations in data representation and augmentation. To robustify World Models against viewpoint shifts and sensor noise, Gadre et al. \cite{gadre_continuous_2022} propose \textit{Continuous Scene Representations} (CSRs). Unlike traditional discrete augmentations (e.g., random crops or rotations) which introduce artifacts, CSRs encode the 3D scene as an implicit neural function. This allows the data pipeline to generate infinite, consistent views of a scene, effectively performing "geometry-aware" augmentation. For a world model, this means training on trajectories that are not just replayed, but re-rendered from novel perspectives, enforcing 3D consistency in the learned latent space. Gadre et al. \cite{gadre_continuous_2022} show that agents trained with CSRs exhibit superior generalization in navigation and manipulation tasks, as they learn the underlying continuous geometry of the world rather than overfitting to pixel-level patterns.

\subsection{Synthetic Simulation Data: The Infinite Afterburner}
As real-world collection hits scaling limits (cost, hardware wear, safety), synthetic data from high-fidelity simulators has become the ``Data Afterburner.'' Simulators allow for the generation of infinite, labeled data for corner cases that are dangerous or rare in the real world.

\subsubsection{ManiSkill3: GPU-Parallelized Physics}
ManiSkill3 \cite{li_mimicdreamer_2025} represents the state-of-the-art in parallelized physical simulation, utilizing the SAPIEN engine to achieve 30,000+ FPS on a single NVIDIA H100 GPU. Unlike traditional CPU-based simulators (Gazebo, MuJoCo), ManiSkill3 runs the entire physics pipeline on the GPU, eliminating the PCIe bottleneck. This architecture enables massive parallelization: thousands of environments can run simultaneously, generating billions of interaction steps per hour. Frameworks like \textit{RLinf-VLA} \textbf{\cite{zang_rlinf-vla_2025}} utilize these simulators to achieve 97.66\% success across 25 ManiSkill tasks, leveraging ``hybrid fine-grained pipeline allocation'' to speed up training by nearly 2x. This speed enables the training of policies via Reinforcement Learning (RL) with billions of interaction steps, a scale impossible in the physical world. ManiSkill3 also supports diverse rendering pipelines, including ray-tracing, to minimize the visual sim-to-real gap.

\subsubsection{RoboCasa and Generalizable Home Tasks}
\textbf{RoboCasa} \textbf{\cite{tai_realmirror_2025}} addresses the semantic poverty of earlier simulators. Built on the MuJoCo engine, it provides over 100 photorealistic kitchen environments and thousands of interactable objects sourced from high-quality asset stores. It focuses on ``Everyday Tasks'' (e.g., loading a dishwasher, making coffee, organizing cabinets), providing the semantic diversity needed to train VLA agents for domestic deployment. The benchmark revealed that photorealistic assets are critical for Sim-to-Real transfer; models trained on lower-fidelity assets (e.g., convex hulls, simple textures) suffer from severe ``vision-gap'' failures during real-world execution. RoboCasa mitigates this by using generative AI to texture assets, ensuring high visual fidelity. It also procedurally generates scene layouts, ensuring that the agent cannot simply memorize the map of a single kitchen.

\subsubsection{Urban and Large-Scale Environments}
Moving beyond the household, \textbf{Urban Generative Intelligence (UGI)} \textbf{\cite{xu_urban_2023}} provides a platform for embodied agents in city-scale environments, integrating urban knowledge graphs with simulation. This is complemented by \textbf{MultiPLY} \textbf{\cite{hong_multiply_2024}}, which focuses on multi-sensory object-centric data in 3D worlds, and \textbf{LEO} \textbf{\cite{huang_embodied_2024}}, a generalist agent trained in diverse 3D environments. \textbf{PhyScene} \textbf{\cite{yang_physcene_2024}} takes this further by synthesizing physically interactable 3D scenes, allowing agents to manipulate the structural elements of the environment itself. These large-scale simulators are crucial for training agents that must navigate and interact with the complex, dynamic topology of the real world \cite{duan_survey_2022} \textbf{\cite{li_comprehensive_2025}}. They provide the "macro" context (navigation, social norms) that complements the "micro" context (manipulation) of RoboCasa.

\subsubsection{Sim-to-Real Transfer Challenges}
Despite the advances in fidelity, the Sim-to-Real gap remains a formidable barrier. The \textbf{Physical Grounding Gap} \textbf{\cite{lu_multimodal_2025}} refers to the discrepancy between simulated contact dynamics (often simplified as rigid body impulses) and real-world compliance (deformation, friction, stiction). Recent approaches like \textbf{VLA-RFT} \textbf{\cite{li_vla-rft_2025}} and \textbf{World-Env} \textbf{\cite{xiao_world-env_2025}} attempt to bridge this by using real-world data to train a "Neural Simulator" or World Model, which then serves as the training environment for the policy. This hybrid approach---training in a neural simulation grounded in real data---offers a promising path to reliable transfer. By fine-tuning the simulator's physics parameters to match real-world observations (System Identification), or by learning a residual policy that corrects for simulation errors, researchers are steadily closing this gap.

\subsection{Adversarial Data Collection and Failure Mining}
A key innovation in recent data engines is the concept of \textbf{Adversarial Data Collection}, championed by AgiBot and the developers of $\pi^*_{0.6}$ \textbf{\cite{intelligence__06_2025}}. Instead of collecting only successful trajectories, this approach explicitly seeks out failure modes and ``edge cases'' to robustify the world model.

\paragraph{The RECAP Method}
The RECAP (RL with Experience and Corrections via Advantage-conditioned Policies) method \textbf{\cite{intelligence__06_2025}} formalizes this loop. It incorporates expert teleoperated interventions provided \textit{during} autonomous execution. When the policy's uncertainty metric $\mathcal{U}(s)$ exceeds a threshold, control is handed over to a human operator who corrects the behavior. These correction trajectories are heavily weighted during retraining. By focusing data collection on the decision boundaries where the model is most likely to fail, RECAP effectively turns deployment failures into high-value training signals. This methodology has been shown to double task throughput and halve failure rates in complex, long-horizon tasks like laundry folding and espresso making. It represents a shift from "static dataset" to "active learning," where the model itself directs the data collection process.

\paragraph{Test-Time Adaptation}
Complementing adversarial collection is \textbf{Test-Time Adaptation} (TTA). Systems like \textbf{TT-VLA} \textbf{\cite{liu_--fly_2026}} and \textbf{VLA-Reasoner} \textbf{\cite{guo_vla-reasoner_2025}} utilize online reinforcement learning or Monte Carlo Tree Search (MCTS) during deployment. By simulating potential future outcomes using the internal world model, these agents can reject unsafe actions before execution. This "thinking fast and slow" paradigm allows the agent to adapt to novel perturbations (e.g., a person bumping into the robot) without requiring full model retraining. It essentially treats the deployment phase as a continuous data collection and learning opportunity.

\begin{table}[ht]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l c l p{2cm} p{5cm} c}
\toprule
\textbf{Dataset} & \textbf{Year} & \textbf{Type} & \textbf{Scale} & \textbf{Key Features} & \textbf{Ref.} \\ \midrule
Open X-Embodiment & 2023 & Real & 1M+ traj. & Aggregation of 22+ embodiments. & \textbf{\cite{collaboration_open_2025}} \\ \midrule
DROID & 2024 & Real & 76k eps. & Consistent Franka hardware setup. & \textbf{\cite{khazatsky_droid_2025}} \\ \midrule
AgiBot-G1 & 2025 & Real & 20k+ hours & Bimanual humanoid coordination. & \textbf{\cite{jiang_wholebodyvla_2025}} \\ \midrule
LingBot & 2026 & Real & 20k hours & Dual-arm pragmatic manipulation. & \textbf{\cite{wu_pragmatic_2026}} \\ \midrule
Galaxea & 2025 & Real & Open-World & Mobile manipulation w/ subtasks. & \textbf{\cite{jiang_galaxea_2025}} \\ \midrule
Ego4D & 2022 & Video & 3,670 hours & Massive egocentric human behavior. & \textbf{\cite{lu_multimodal_2025}} \\ \midrule
Cosmos-Video & 2025 & Video & 20M hours & Physicalized internet footage. & \textbf{\cite{wang_genie_2025}} \\ \midrule
ManiSkill3 & 2025 & Sim & Infinite & GPU-parallelized physics steps. & \cite{li_mimicdreamer_2025} \\ \midrule
RoboCasa & 2024 & Sim & 100+ Envs & Photorealistic home manipulation. & \textbf{\cite{tai_realmirror_2025}} \\ \midrule
BridgeV2W & 2026 & Hybrid & Varies & Embodiment masks for video gen. & \textbf{\cite{chen_bridgev2w_2026}} \\ \midrule
TEACh & 2022 & Dialog & 3k dialogues & Instruction following w/ clarification. & \textbf{\cite{gao_dialfred_2022}} \\ \midrule
MineDojo & 2022 & Open & Internet & Massive Minecraft tasks. & \cite{fan_minedojo_2022} \\ \bottomrule
\end{tabular}
\caption{Key Datasets and Benchmarks for Embodied AI World Models (2022--2026). The "Type" column distinguishes between Real robot data, Passive Video, Simulation, and Hybrid/Dialogue formats.} \label{tab:datasets_engine}
\end{table}

\subsection{Data Governance, Privacy, and Safety}
As Data Engines ingest petabytes of real-world video, governance becomes paramount. \textbf{Ego4D} implements strict privacy protocols, blurring faces and personally identifiable information (PII) to comply with GDPR and CCPA. However, for world models to understand human emotion and intent, some facial cues are necessary, creating a trade-off between privacy and performance. In the domain of humanoid robotics, ``Safety Filters'' are critical. Datasets like AgiBot-G1 must be scrubbed of unsafe or harmful behaviors to prevent the policy from learning dangerous actions (e.g., swinging tools near humans). The concept of ``Constitutional AI'' is being adapted for robotics to ensure that the data engine itself adheres to safety norms, filtering out trajectories that violate predefined constraints.

\subsection{Summary: The Data Engine Flywheel}
The ``Data Engine'' of 2026 operates as a virtuous cycle: massive video data ($\pi_0$ \textbf{\cite{black__0_2026}}) builds physical intuition, high-quality robot interaction teaches specific skills (DROID \textbf{\cite{khazatsky_droid_2025}}), online corrections ($\pi^*_{0.6}$ \textbf{\cite{intelligence__06_2025}}) close the performance gap, and synthetic expansion (ManiSkill3 \cite{li_mimicdreamer_2025}) multiplies these scenarios into millions of variations. This ``flywheel'' ensures that generalist agents can transition from episodic task-solvers to truly autonomous physical companions. The integration of these diverse data streams---video, real, and sim---into a unified training curriculum remains the defining challenge and opportunity for the field. Future work must address the ``Physical Grounding Gap'' \textbf{\cite{lu_multimodal_2025}} and develop more robust metrics for ``Data Diversity'' to ensure that this engine continues to scale effectively. The ultimate goal is a self-sustaining ecosystem where agents learn from every interaction, whether physical, simulated, or observed, continuously refining their world models in pursuit of Artificial General Intelligence.
