\begin{landscape}
\section{Comparative Summary Tables}
\label{sec:summary_tables}

\subsection*{A. Algorithm-Oriented Papers}
\centering
\footnotesize
\renewcommand{\arraystretch}{1.15}
\setlength{\tabcolsep}{3pt}
\begin{xltabular}{\linewidth}{l p{0.27\linewidth} X l}
\caption{Algorithm-oriented literature summary.} \label{tab:alg_papers} \\
\toprule
\textbf{Year} & \textbf{Model/Method} & \textbf{Technical Contribution} & \textbf{Reference} \\
\midrule
\endfirsthead
\toprule
\textbf{Year} & \textbf{Model/Method} & \textbf{Technical Contribution} & \textbf{Reference} \\
\midrule
\endhead
\bottomrule
\endfoot
2022 & Do As I Can, Not As I Say & We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible & \cite{ahn_as_2022} \\
2022 & Continuous scene representations for embodied ai & Not explicitly specified. & \cite{gadre_continuous_2022} \\
2022 & Dialfred & Not explicitly specified. & \cite{gao_dialfred_2022} \\
2022 & Inner Monologue & We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios & \cite{huang_inner_2022} \\
2022 & Language models as zero-shot planners & Not explicitly specified. & \cite{huang_language_2022} \\
2022 & GenIE & Not explicitly specified. & \cite{josifoski_genie_2022} \\
2022 & AACR Project GENIE & Not explicitly specified. & \cite{pugh_aacr_2022} \\
2022 & A Generalist Agent & Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. & \cite{reed_generalist_2022} \\
2023 & RoboCat & Inspired by recent advances in foundation models for vision and language, we propose a multi-embodiment, multi-task generalist agent for robotic manipulation. & \cite{bousmalis_robocat_2023} \\
2023 & Edgi & Not explicitly specified. & \cite{brehmer_edgi_2023} \\
2023 & Collaborating with language models for embodied reasoning & We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this & \cite{dasgupta_collaborating_2023} \\
2023 & Can an embodied agent find your “cat-shaped mug”\pi llm-based zero-shot & Not explicitly specified. & \cite{dorbala_can_2023} \\
2023 & PaLM-E & We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and  & \cite{driess_palm-e_2023} \\
2023 & RT-Trajectory & We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to ef & \cite{gu_rt-trajectory_2023} \\
2023 & Grounded Decoding & Recent progress in large language models (LLMs) has demonstrated the ability to learn and leverage Internet-scale knowledge through pre-training with autoregressive model & \cite{huang_grounded_2023} \\
2023 & VoxPoser & We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipul & \cite{huang_voxposer_2023} \\
2023 & Do embodied agents dream of pixelated sheep & Not explicitly specified. & \cite{nottingham_embodied_2023} \\
2023 & Genie in the Model & In this paper, we propose to push forward the frontiers of the DNN performance-resource trade-off by introducing human intelligence as a new design dimension. & \cite{wang_genie_2023} \\
2023 & Voyager & We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel d & \cite{wang_voyager_2023} \\
2023 & Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware & Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks\pi We present a low-cost system that performs end-to-end imitation learning d & \cite{zhao_learning_2023} \\
2024 & Genie & Not explicitly specified. & \cite{bruce_genie_2024} \\
2024 & GR-2 & We present GR-2, a state-of-the-art generalist robot agent for versatile and generalizable robot manipulation. & \cite{cheang_gr-2_2024} \\
2024 & Embodied LLM Agents Learn to Cooperate in Organized Teams & Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. & \cite{guo_embodied_2024} \\
2024 & The Essential Role of Causality in Foundation World Models for & Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodi & \cite{gupta_essential_2024} \\
2024 & Multiply & Not explicitly specified. & \cite{hong_multiply_2024} \\
2024 & Sara-rt & Not explicitly specified. & \cite{leal_sara-rt_2024} \\
2024 & Out of Many, One & In this work we introduce Genie 2, extending Genie to capture a larger and more diverse protein structure space through architectural innovations and massive data augment & \cite{lin_out_2024} \\
2024 & Embodied multi-modal agent trained by an llm from a parallel & Not explicitly specified. & \cite{yang_embodied_2024} \\
2024 & Physcene & Not explicitly specified. & \cite{yang_physcene_2024} \\
2024 & Achieving Faster and More Accurate Operation of Deep Predictive Learning & Achieving both high speed and precision in robot operations is a significant challenge for social implementation. & \cite{yoshikawa_achieving_2024} \\
2024 & Building Cooperative Embodied Agents Modularly with Large Language Models & In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective task & \cite{zhang_building_2024} \\
2025 & EVOLVE-VLA & We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrati & \cite{bai_evolve-vla_2025} \\
2025 & Focusing on What Matters & Building on the insights of object-centric representation learning, our method introduces an inductive bias towards scene objects and the agent's own visual information. & \cite{bendikas_focusing_2025} \\
2025 & Semantic World Models & Planning with world models offers a powerful paradigm for robotic control. & \cite{berg_semantic_2025} \\
2025 & Motus & In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. & \cite{bi_motus_2025} \\
2025 & WorldVLA & We present WorldVLA, an autoregressive action world model that unifies action and image understanding and generation. & \cite{cen_worldvla_2025} \\
2025 & ConRFT & In this paper, we propose a reinforced fine-tuning approach for VLA models, named ConRFT, which consists of offline and online fine-tuning with a unified consistency-base & \cite{chen_conrft_2025} \\
2025 & InternVLA-M1 & We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelli & \cite{chen_internvla-m1_2025} \\
2025 & villa-X & In this paper, we introduce villa-X, a novel Vision-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipul & \cite{chen_villa-x_2025} \\
2025 & VLA+VLBA to ngVLA Transition Option Concepts & The next-generation Very Large Array (ngVLA) is intended to be the premier centimeter-wavelength facility for astronomy and astrophysics, building on the substantial scie & \cite{corsi_vlavlba_2025} \\
2025 & Revla & Not explicitly specified. & \cite{dey_revla_2025} \\
2025 & Understanding World or Predicting Future\pi A Comprehensive Survey of World & The concept of world models has garnered significant attention due to advancements in multimodal large language models such as GPT-4 and video generation models such as S & \cite{ding_understanding_2025} \\
2025 & Bridging Perception, Language, and Action & Not explicitly specified. & \cite{dolgopolyi_bridging_2025} \\
2025 & VITA-VLA & In this work, we propose a simple yet effective distillation-based framework that equips VLMs with action-execution capability by transferring knowledge from pretrained s & \cite{dong_vita-vla_2025} \\
2025 & Knowledge Insulating Vision-Language-Action Models & Vision-language-action (VLA) models provide a powerful approach to training control policies for physical systems, such as robots, by combining end-to-end learning with t & \cite{driess_knowledge_2025} \\
2025 & SQAP-VLA & We overcome the incompatibility by co-designing the quantization and token pruning pipeline, where we propose new quantization-aware token pruning criteria that work on a & \cite{fang_sqap-vla_2025} \\
2025 & Embodied AI & Not explicitly specified. & \cite{feng_embodied_2025} \\
2025 & Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos & To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabl & \cite{feng_spatial-aware_2025} \\
2025 & MergeVLA & To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. & \cite{fu_mergevla_2025} \\
2025 & Embodied AI Agents & We propose that the development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environm & \cite{fung_embodied_2025} \\
2025 & VLA-OS & To systematically investigate the impacts of different planning paradigms and representations isolating from network architectures and training data, in this paper, we in & \cite{gao_vla-os_2025} \\
2025 & Efficient Vision-Language-Action Models for Embodied Manipulation & Vision-Language-Action (VLA) models extend vision-language models to embodied control by mapping natural-language instructions and visual observations to robot actions. & \cite{guan_efficient_2025} \\
2025 & OmniVLA & We present OmniVLA, an omni-modality VLA model that integrates novel sensing modalities for physically-grounded spatial intelligence beyond RGB perception. & \cite{guo_omnivla_2025} \\
2025 & Run-time observation interventions make vision-language-action models more visually robust & Not explicitly specified. & \cite{hancock_run-time_2025} \\
2025 & Mechanistic interpretability for steering vision-language-action models & Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal re & \cite{haon_mechanistic_2025} \\
2025 & Do what\pi Teaching vision-language-action models to reject the impossible & Not explicitly specified. & \cite{hsieh_what_2025} \\
2025 & Joint Optimization of Fine-grained Representation and Workflow Orchestration in Metaverse & Not explicitly specified. & \cite{hu_joint_2025} \\
2025 & GraphCoT-VLA & Vision-language-action models have emerged as a crucial paradigm in robotic manipulation. & \cite{huang_graphcot-vla_2025} \\
2025 & Tactile-VLA & This paper introduces Tactile-VLA, a novel framework that deeply fuses vision, language, action, and tactile sensing. & \cite{huang_tactile-vla_2025} \\
2025 & NORA & To address the limitations of existing VLA models, we propose NORA, a 3B-parameter model designed to reduce computational overhead while maintaining strong task performan & \cite{hung_nora_2025} \\
2025 & $\pi_{0.5}$ & In order for robots to be useful, they must perform practically relevant tasks in the real world, outside of the lab. & \cite{intelligence__05_2025} \\
2025 & Don't Run with Scissors & We introduce GLUESTICK, a post-pruning recovery method that restores much of the original model's functionality while retaining sparsity benefits. & \cite{jabbour_dont_2025} \\
2025 & ContextVLA & In this paper, we introduce ContextVLA, a policy model that robustly improves robotic task performance by effectively leveraging multi-frame observations. & \cite{jang_contextvla_2025} \\
2025 & Don't Blind Your VLA & The growing success of Vision-Language-Action (VLA) models stems from the promise that pretrained Vision-Language Models (VLMs) can endow agents with transferable world k & \cite{kachaev_dont_2025} \\
2025 & RetoVLA & Recent Vision-Language-Action (VLA) models demonstrate remarkable generalization in robotics but are restricted by their substantial size and computational cost, limiting & \cite{koo_retovla_2025} \\
2025 & Embodied Understanding of Driving Scenarios & Not explicitly specified. & \cite{leonardis_embodied_2025} \\
2025 & 3ds-vla & Not explicitly specified. & \cite{li_3ds-vla_2025} \\
2025 & Coa-vla & Not explicitly specified. & \cite{li_coa-vla_2025} \\
2025 & ControlVLA & To achieve this, we propose ControlVLA, a novel framework that bridges pre-trained VLA models with object-centric representations via a ControlNet-style architecture for  & \cite{li_controlvla_2025} \\
2025 & Embodied Multi-Agent Systems & Not explicitly specified. & \cite{li_embodied_2025} \\
2025 & MimicDreamer & To bridge this gap, we propose MimicDreamer, a framework that turns fast, low-cost human demonstrations into robot-usable supervision by jointly aligning vision, viewpoin & \cite{li_mimicdreamer_2025} \\
2025 & Robonurse-vla & Not explicitly specified. & \cite{li_robonurse-vla_2025} \\
2025 & SwitchVLA & We propose SwitchVLA, a unified, execution-aware framework that enables smooth and reactive task switching without external planners or additional switch-specific data. & \cite{li_switchvla_2025} \\
2025 & UrbanVLA & To this end, we propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework designed for scalable urban navigation. & \cite{li_urbanvla_2025} \\
2025 & VLA Models Are More Generalizable Than You Think & To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. & \cite{li_vla_2025} \\
2025 & Large Model Empowered Embodied AI & For embodied learning, we introduce mainstream learning methodologies, elaborating on how large models enhance imitation learning and reinforcement learning in-depth. & \cite{liang_large_2025} \\
2025 & Evo-0 & We evaluate our method on a set of spatially challenging tasks in both simulation and the real world. & \cite{lin_evo-0_2025} \\
2025 & VOTE & To address these issues, we develop a training framework to finetune VLA models for generating significantly fewer action tokens with high parallelism, effectively reduci & \cite{lin_vote_2025} \\
2025 & HybridVLA & To address these limitations, we introduce HybridVLA, a unified framework that absorbs the continuous nature of diffusion-based actions and the contextual reasoning of au & \cite{liu_hybridvla_2025} \\
2025 & HybridVLA & Not explicitly specified. & \cite{liu_hybridvla_2025-1} \\
2025 & MLA & To this end, we introduce a multisensory language-action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory obje & \cite{liu_mla_2025} \\
2025 & TTF-VLA & We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference qual & \cite{liu_ttf-vla_2025} \\
2025 & VLA-Mark & Not explicitly specified. & \cite{liu_vla-mark_2025} \\
2025 & VLA-RL & We present VLA-RL, an algorithmic and systematic framework that leverages online reinforcement learning (RL) to improve pretrained auto-regressive VLAs in downstream task & \cite{lu_vla-rl_2025} \\
2025 & Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search & We present Vision-Language-Action Planning \\& Search (VLAPS) -- a novel framework and accompanying algorithms that embed model-based search into the inference procedure o & \cite{neary_improving_2025} \\
2025 & GraSP-VLA & In this paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that uses a Continuous Scene Graph representation to generate a symbolic representation of  & \cite{neau_grasp-vla_2025} \\
2025 & Cosmos World Foundation Model Platform for Physical AI & In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. & \cite{nvidia_cosmos_2025} \\
2025 & ACG & In this paper, we present Action Coherence Guidance (ACG) for VLA models, a training-free test-time guidance algorithm that improves action coherence and thereby yields p & \cite{park_acg_2025} \\
2025 & Spatial Traces & Not explicitly specified. & \cite{patratskiy_spatial_2025} \\
2025 & Bring the Apple, Not the Sofa & In this work, we present a novel systematic study of the robustness of state-of-the-art VLA models under linguistic perturbations. & \cite{pugacheva_bring_2025} \\
2025 & SpatialVLA & Specifically, we introduce Ego3D Position Encoding to inject 3D information into the input observations of the visual-language-action model, and propose Adaptive Action G & \cite{qu_spatialvla_2025} \\
2025 & Exploration-Driven Generative Interactive Environments & Not explicitly specified. & \cite{savov_exploration-driven_2025} \\
2025 & Leave No Observation Behind & We introduce Asynchronous Action Chunk Correction (A2C2), which is a lightweight real-time chunk correction head that runs every control step and adds a time-aware correc & \cite{sendai_leave_2025} \\
2025 & VLA-R & In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception  & \cite{seong_vla-r_2025} \\
2025 & CEED-VLA & To address it, we introduce consistency distillation training to predict multiple correct action tokens in each iteration, thereby achieving acceleration. & \cite{song_ceed-vla_2025} \\
2025 & PD-VLA & To tackle this problem, we propose PD-VLA, the first parallel decoding framework for VLA models integrated with action chunking. & \cite{song_pd-vla_2025} \\
2025 & CollabVLA & In this work, we present CollabVLA, a self-reflective vision-language-action framework that transforms a standard visuomotor policy into a collaborative assistant. & \cite{sun_collabvla_2025} \\
2025 & From Perception to Action with Integrated VLA Systems & Not explicitly specified. & \cite{turgunbaev_perception_2025} \\
2025 & Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots & In this paper, we propose eight uncertainty metrics and five quality metrics specifically designed for VLA models for robotic manipulation tasks. & \cite{valle_evaluating_2025} \\
2025 & WorldAgen & Not explicitly specified. & \cite{wan_worldagen_2025} \\
2025 & Exploring the adversarial vulnerabilities of vision-language-action models in robotics & Not explicitly specified. & \cite{wang_exploring_2025} \\
2025 & Genie & Not explicitly specified. & \cite{wang_genie_2025} \\
2025 & Spec-vla & Not explicitly specified. & \cite{wang_spec-vla_2025} \\
2025 & SpecPrune-VLA & We introduce SpecPrune-VLA, a training-free method with two-level pruning and heuristic control: (1) Static pruning at action level: uses global history and local context & \cite{wang_specprune-vla_2025} \\
2025 & VLATest & To address this gap, we present VLATest, a fuzzing framework designed to generate robotic manipulation scenes for testing VLA models. & \cite{wang_vlatest_2025} \\
2025 & Audio-VLA & Additionally, this paper introduces the Task Completion Rate (TCR) metric to systematically evaluate dynamic operational processes. & \cite{wei_audio-vla_2025} \\
2025 & DexVLA & This paper introduces DexVLA, a novel framework designed to enhance the efficiency and generalization capabilities of VLAs for complex, long-horizon tasks across diverse  & \cite{wen_dexvla_2025} \\
2025 & DiffusionVLA & Not explicitly specified. & \cite{wen_diffusionvla_2025} \\
2025 & LLaDA-VLA & In this work, we present LLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon pretrained d-VLMs for robotic manipulation. & \cite{wen_llada-vla_2025} \\
2025 & Tinyvla & Not explicitly specified. & \cite{wen_tinyvla_2025} \\
2025 & Momanipvla & Not explicitly specified. & \cite{wu_momanipvla_2025} \\
2025 & VLA Model-Expert Collaboration for Bi-directional Manipulation Learning & The emergence of vision-language-action (VLA) models has given rise to foundation models for robot manipulation. & \cite{xiang_vla_2025} \\
2025 & HyperVLA & In this paper, we propose HyperVLA to address this problem. & \cite{xiong_hypervla_2025} \\
2025 & STARE-VLA & Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, i & \cite{xu_stare-vla_2025} \\
2025 & VLA-Cache & This paper introduces VLA-Cache, a training-free inference acceleration method that reduces computational overhead by adaptively caching and reusing static visual tokens  & \cite{xu_vla-cache_2025} \\
2025 & WAM-Diff & End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and c & \cite{xu_wam-diff_2025} \\
2025 & FPC-VLA & To address these challenges, we propose FPC-VLA, a dual-model framework that integrates VLA with a supervisor for failure prediction and correction. & \cite{yang_fpc-vla_2025} \\
2025 & Balancing Signal and Variance & Vision-Language-Action (VLA) models based on flow matching have shown excellent performance in general-purpose robotic manipulation tasks. & \cite{zhang_balancing_2025} \\
2025 & InSpire & To tackle this challenge, we propose Intrinsic Spatial Reasoning (InSpire), a simple yet effective approach that mitigates the adverse effects of spurious correlations by & \cite{zhang_inspire_2025} \\
2025 & MoLe-VLA & We introduce a Spatial-Temporal Aware Router (STAR) for MoLe to selectively activate only parts of the layers based on the robot's current state, mimicking the brain's di & \cite{zhang_mole-vla_2025} \\
2025 & RobustVLA & In this work, we introduce RobustVLA, a lightweight online RL post-training method designed to explicitly enhance the resilience of VLA models. & \cite{zhang_robustvla_2025} \\
2025 & A Step Toward World Models & Autonomous agents are increasingly expected to operate in complex, dynamic, and uncertain environments, performing tasks such as manipulation, navigation, and decision-ma & \cite{zhang_step_2025} \\
2025 & Cot-vla & Not explicitly specified. & \cite{zhao_cot-vla_2025} \\
2025 & MoRE & This paper introduces a novel vision-language-action (VLA) model, mixture of robotic experts (MoRE), for quadruped robots that aim to introduce reinforcement learning (RL & \cite{zhao_more_2025} \\
2025 & A Survey on Vision-Language-Action Models & The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such int & \cite{zhong_survey_2025} \\
2025 & ChatVLA-2 & In this work, we introduce ChatVLA-2, a novel mixture-of-expert VLA model coupled with a specialized two-stage training pipeline designed to preserve the VLM's original s & \cite{zhou_chatvla-2_2025} \\
2025 & WMPO & We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. & \cite{zhu_wmpo_2025} \\
2026 & Flowdreamer & Not explicitly specified. & \cite{guo_flowdreamer_2026} \\
2026 & PI-VLA & Not explicitly specified. & \cite{jian_pi-vla_2026} \\
2026 & Pointvla & Not explicitly specified. & \cite{li_pointvla_2026} \\
2026 & Reflection-Based Task Adaptation for Self-Improving VLA & We introduce Reflective Self-Adaptation, a framework for rapid, autonomous task adaptation without human intervention. & \cite{li_reflection-based_2026} \\
2026 & Aligning Agentic World Models via Knowledgeable Experience Learning & To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. & \cite{ren_aligning_2026} \\
2026 & Vision-Language-Action (VLA) Models & Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. & \cite{sapkota_vision-language-action_2026} \\
2026 & Learning Action-Conditioned World Models for Cataract Surgery from Unlabeled Videos & Not explicitly specified. & \cite{shah_learning_2026} \\
2026 & An Efficient and Multi-Modal Navigation System with One-Step World Model & To address this bottleneck, we propose a lightweight navigation world model that adopts a one-step generation paradigm and a 3D U-Net backbone equipped with efficient spa & \cite{shen_efficient_2026} \\
2026 & Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models & Humans construct internal world models and reason by manipulating the concepts within these models. & \cite{wu_visual_2026} \\
2026 & Do What You Say & Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspire & \cite{wu_what_2026} \\
2026 & ACoT-VLA & We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the  & \cite{zhong_acot-vla_2026} \\
2026 & Digital Twin AI & Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the int & \cite{zhou_digital_2026} \\
-- & Goal-VLA & Not explicitly specified. & \cite{chen_goal-vla_nodate} \\
-- & Emerging Paradigms in Deep Learning & Not explicitly specified. & \cite{xie_emerging_nodate} \\
-- & VLAC & Not explicitly specified. & \cite{zhang_vlac_nodate} \\
\end{xltabular}

\subsection*{B. Data-Oriented Papers}
\centering
\footnotesize
\renewcommand{\arraystretch}{1.15}
\setlength{\tabcolsep}{3pt}
\begin{xltabular}{\linewidth}{l p{0.22\linewidth} p{0.16\linewidth} p{0.14\linewidth} p{0.14\linewidth} p{0.14\linewidth} l}
\caption{Data-oriented literature summary.} \label{tab:data_papers} \\
\toprule
\textbf{Year} & \textbf{Dataset/Framework} & \textbf{Data Modality} & \textbf{Task Focus} & \textbf{Source Type} & \textbf{Scale} & \textbf{Reference} \\
\midrule
\endfirsthead
\toprule
\textbf{Year} & \textbf{Dataset/Framework} & \textbf{Data Modality} & \textbf{Task Focus} & \textbf{Source Type} & \textbf{Scale} & \textbf{Reference} \\
\midrule
\endhead
\bottomrule
\endfoot
2020 & Rearrangement & Vision + Language & General embodied tasks & Simulated/Generated & Not explicitly specified & \cite{batra_rearrangement_2020} \\
2022 & A survey of embodied ai & Not explicitly specified & General embodied tasks & Simulated/Generated & Not explicitly specified & \cite{duan_survey_2022} \\
2022 & MineDojo & Vision + Language & Navigation/interaction & Simulated/Generated & Not explicitly specified & \cite{fan_minedojo_2022} \\
2023 & RT-1 & Vision + Language & General embodied tasks & Real-world & Not explicitly specified & \cite{brohan_rt-1_2023} \\
2023 & Q-Transformer & Vision + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Real-world & Not explicitly specified & \cite{chebotar_q-transformer_2023} \\
2023 & VIMA & Vision + Language + Action Trajectory & Robotic manipulation & Simulated/Generated & Not explicitly specified & \cite{jiang_vima_2023} \\
2023 & Code as Policies & Vision + Language + Action Trajectory & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{liang_code_2023} \\
2023 & Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models & Vision + Language + Action Trajectory & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{sarch_open-ended_2023} \\
2023 & LLM-Planner & Language & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{song_llm-planner_2023} \\
2023 & Bridgedata v2 & Not explicitly specified & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{walke_bridgedata_2023} \\
2023 & Embodied Task Planning with Large Language Models & Vision + Language + Action Trajectory & General embodied tasks & Simulated/Generated & Not explicitly specified & \cite{wu_embodied_2023} \\
2023 & Plan, Eliminate, and Track -- Language Models are Good Teachers & Language + Action Trajectory & Navigation/interaction & Not explicitly specified & Not explicitly specified & \cite{wu_plan_2023} \\
2023 & Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{wu_unleashing_2023} \\
2023 & Urban Generative Intelligence (UGI) & Language + Action Trajectory & Autonomous driving & Simulated/Generated & Not explicitly specified & \cite{xu_urban_2023} \\
2024 & OptiMUS & Language & General embodied tasks & Simulated/Generated & Not explicitly specified & \cite{ahmaditeshnizi_optimus_2024} \\
2024 & Diffusion Policy & Action Trajectory & Robotic manipulation & Not explicitly specified & Not explicitly specified & \cite{chi_diffusion_2024} \\
2024 & CoPa & Vision + Language & Robotic manipulation & Real-world & Not explicitly specified & \cite{huang_copa_2024} \\
2024 & An Embodied Generalist Agent in 3D World & Vision + Depth/3D + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{huang_embodied_2024} \\
2024 & Learning Generative Interactive Environments By Trained Agent Exploration & Action Trajectory + Force/Tactile/Audio & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{kazemi_learning_2024} \\
2024 & OpenVLA & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & 29 tasks & \cite{kim_openvla_2024} \\
2024 & Behavior Generation with Latent Actions & Vision + Language + Action Trajectory & Autonomous driving & Not explicitly specified & Not explicitly specified & \cite{lee_behavior_2024} \\
2024 & Embodied agent interface & Not explicitly specified & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{li_embodied_2024} \\
2024 & Towards Generalist Robot Policies & Vision + Language + Action Trajectory & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{li_towards_2024} \\
2024 & Vision-Language Foundation Models as Effective Robot Imitators & Vision + Language & Robotic manipulation & Not explicitly specified & Not explicitly specified & \cite{li_vision-language_2024} \\
2024 & Open x-embodiment & Not explicitly specified & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{oneill_open_2024} \\
2024 & Bringing the RT-1-X Foundation Model to a SCARA robot & Action Trajectory & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{salzer_bringing_2024} \\
2024 & Octo & Vision + Language + Action Trajectory & Robotic manipulation & Not explicitly specified & 800k trajectories & \cite{team_octo_2024} \\
2024 & Genie & Action Trajectory & General embodied tasks & Simulated/Generated & Not explicitly specified & \cite{yehudai_genie_2024} \\
2024 & Learning Manipulation by Predicting Interaction & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{zeng_learning_2024} \\
2024 & 3D-VLA & Vision + Depth/3D + Language + Action Trajectory & General embodied tasks & Real-world & Not explicitly specified & \cite{zhen_3d-vla_2024} \\
2025 & cVLA & Vision + Depth/3D + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{argus_cvla_2025} \\
2025 & 3D CAVLA & Vision + Depth/3D + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{bhat_3d_2025} \\
2025 & VLA-Touch & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{bi_vla-touch_2025} \\
2025 & EdgeVLA & Vision + Language + Action Trajectory & Robotic manipulation & Not explicitly specified & Not explicitly specified & \cite{budzianowski_edgevla_2025} \\
2025 & RynnVLA-002 & Vision + Language + Action Trajectory & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{cen_rynnvla-002_2025} \\
2025 & Planning with Reasoning using Vision Language World Model & Vision + Language + Action Trajectory & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{chen_planning_2025} \\
2025 & Unified Diffusion VLA & Vision + Depth/3D + Language + Action Trajectory & General embodied tasks & Real-world & Not explicitly specified & \cite{chen_unified_2025} \\
2025 & Impromptu VLA & Vision + Language + Action Trajectory & Autonomous driving & Not explicitly specified & Not explicitly specified & \cite{chi_impromptu_2025} \\
2025 & Open X-Embodiment & Vision + Language & Robotic manipulation & Not explicitly specified & 160266 tasks & \cite{collaboration_open_2025} \\
2025 & End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Real-world & Not explicitly specified & \cite{cui_end--end_2025} \\
2025 & GraspVLA & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{deng_graspvla_2025} \\
2025 & Vision Language Action Models in Robotic Manipulation & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{din_vision_2025} \\
2025 & Humanoid-VLA & Vision + Language + Action Trajectory & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{ding_humanoid-vla_2025} \\
2025 & HiMoE-VLA & Vision + Language + Action Trajectory & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{du_himoe-vla_2025} \\
2025 & Interleave-VLA & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & 210k episodes & \cite{fan_interleave-vla_2025} \\
2025 & Long-VLA & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{fan_long-vla_2025} \\
2025 & DualVLA & Vision + Language + Action Trajectory & Robotic manipulation & Not explicitly specified & Not explicitly specified & \cite{fang_dualvla_2025} \\
2025 & LIBERO-Plus & Vision + Depth/3D + Language + Action Trajectory & Robotic manipulation & Not explicitly specified & Not explicitly specified & \cite{fei_libero-plus_2025} \\
2025 & VLA-0 & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{goyal_vla-0_2025} \\
2025 & Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{grover_enhancing_2025} \\
2025 & ManualVLA & Vision + Depth/3D + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{gu_manualvla_2025} \\
2025 & Improving Vision-Language-Action Model with Online Reinforcement Learning & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Real-world & Not explicitly specified & \cite{guo_improving_2025} \\
2025 & VDRive & Vision + Language + Action Trajectory + Force/Tactile/Audio & Autonomous driving & Simulated/Generated & Not explicitly specified & \cite{guo_vdrive_2025} \\
2025 & VLA-Reasoner & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{guo_vla-reasoner_2025} \\
2025 & Percept-WAM & Vision + Depth/3D + Language + Action Trajectory & Autonomous driving & Not explicitly specified & Not explicitly specified & \cite{han_percept-wam_2025} \\
2025 & Actions as Language & Vision + Language + Action Trajectory & Navigation/interaction & Real-world & Not explicitly specified & \cite{hancock_actions_2025} \\
2025 & IA-VLA & Vision + Language + Action Trajectory & Robotic manipulation & Not explicitly specified & Not explicitly specified & \cite{hannus_ia-vla_2025} \\
2025 & DriveAction & Vision + Language + Action Trajectory & Autonomous driving & Hybrid (real + simulated) & Not explicitly specified & \cite{hao_driveaction_2025} \\
2025 & OmniVLA & Vision + Language + Action Trajectory & Navigation/interaction & Real-world & Not explicitly specified & \cite{hirose_omnivla_2025} \\
2025 & Sample-Efficient Robot Skill Learning for Construction Tasks & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Real-world & Not explicitly specified & \cite{hu_sample-efficient_2025} \\
2025 & AdaPower & Language & Robotic manipulation & Simulated/Generated & Not explicitly specified & \cite{huang_adapower_2025} \\
2025 & NORA-1.5 & Vision + Language + Action Trajectory & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{hung_nora-15_2025} \\
2025 & $\pi^*_{0.6}$ & Vision + Language + Action Trajectory + Force/Tactile/Audio & General embodied tasks & Real-world & Not explicitly specified & \cite{intelligence__06_2025} \\
2025 & The Better You Learn, The Smarter You Prune & Vision + Language + Action Trajectory & General embodied tasks & Real-world & Not explicitly specified & \cite{jiang_better_2025} \\
2025 & Galaxea Open-World Dataset and G0 Dual-System VLA Model & Vision + Language + Action Trajectory & Robotic manipulation & Not explicitly specified & Not explicitly specified & \cite{jiang_galaxea_2025} \\
2025 & IRL-VLA & Vision + Language + Action Trajectory + Force/Tactile/Audio & Autonomous driving & Simulated/Generated & Not explicitly specified & \cite{jiang_irl-vla_2025} \\
2025 & A Survey on Vision-Language-Action Models for Autonomous Driving & Vision + Language + Action Trajectory & Autonomous driving & Not explicitly specified & Not explicitly specified & \cite{jiang_survey_2025} \\
2025 & WholeBodyVLA & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{jiang_wholebodyvla_2025} \\
2025 & Dual-Actor Fine-Tuning of VLA Models & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Real-world & Not explicitly specified & \cite{jin_dual-actor_2025} \\
2025 & Refined Policy Distillation & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{julg_refined_2025} \\
2025 & DROID & Action Trajectory & Robotic manipulation & Real-world & 350 hours; 84 tasks & \cite{khazatsky_droid_2025} \\
2025 & Fine-Tuning Vision-Language-Action Models & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{kim_fine-tuning_2025} \\
2025 & CogVLA & Vision + Language + Action Trajectory & General embodied tasks & Real-world & Not explicitly specified & \cite{li_cogvla_2025} \\
2025 & A Comprehensive Survey on World Models for Embodied AI & Vision + Action Trajectory & Autonomous driving & Simulated/Generated & Not explicitly specified & \cite{li_comprehensive_2025} \\
2025 & DriveVLA-W0 & Vision + Language + Action Trajectory & Autonomous driving & Not explicitly specified & Not explicitly specified & \cite{li_drivevla-w0_2025} \\
2025 & HAMSTER & Vision + Depth/3D + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{li_hamster_2025} \\
2025 & JARVIS-VLA & Vision + Language + Action Trajectory & Navigation/interaction & Not explicitly specified & Not explicitly specified & \cite{li_jarvis-vla_2025} \\
2025 & MAP-VLA & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{li_map-vla_2025} \\
2025 & QDepth-VLA & Vision + Depth/3D + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{li_qdepth-vla_2025} \\
2025 & Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human & Vision + Depth/3D + Language + Action Trajectory & Robotic manipulation & Real-world & 1M episodes & \cite{li_scalable_2025} \\
2025 & SimpleVLA-RL & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Real-world & Not explicitly specified & \cite{li_simplevla-rl_2025} \\
2025 & Spatial Forcing & Vision + Depth/3D + Language + Action Trajectory + Force/Tactile/Audio & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{li_spatial_2025} \\
2025 & Survey of Vision-Language-Action Models for Embodied Manipulation & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{li_survey_2025} \\
2025 & Towards Deploying VLA without Fine-Tuning & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{li_towards_2025} \\
2025 & VLA-RFT & Vision + Language + Action Trajectory + Force/Tactile/Audio & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{li_vla-rft_2025} \\
2025 & Discrete Diffusion VLA & Vision + Language + Action Trajectory & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{liang_discrete_2025} \\
2025 & PixelVLA & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{liang_pixelvla_2025} \\
2025 & Genie Envisioner & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{liao_genie_2025} \\
2025 & Evo-1 & Vision + Language + Action Trajectory & General embodied tasks & Real-world & Not explicitly specified & \cite{lin_evo-1_2025} \\
2025 & HiF-VLA & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{lin_hif-vla_2025} \\
2025 & Aligning Cyber Space with Physical World & Action Trajectory & General embodied tasks & Simulated/Generated & Not explicitly specified & \cite{liu_aligning_2025} \\
2025 & Eva-VLA & Vision + Depth/3D + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{liu_eva-vla_2025} \\
2025 & EvoVLA & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{liu_evovla_2025} \\
2025 & TrackVLA++ & Vision + Language + Action Trajectory & General embodied tasks & Real-world & Not explicitly specified & \cite{liu_trackvla_2025} \\
2025 & Multimodal Data Storage and Retrieval for Embodied AI & Not explicitly specified & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{lu_multimodal_2025} \\
2025 & CognitiveDrone & Vision + Language + Action Trajectory & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{lykov_cognitivedrone_2025} \\
2025 & GR00T N1 & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{nvidia_gr00t_2025} \\
2025 & Counterfactual VLA & Vision + Language + Action Trajectory & Autonomous driving & Not explicitly specified & Not explicitly specified & \cite{peng_counterfactual_2025} \\
2025 & FAST & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & 10k hours & \cite{pertsch_fast_2025} \\
2025 & WristWorld & Vision + Depth/3D + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{qian_wristworld_2025} \\
2025 & Physical AI & Vision + Action Trajectory & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{ray_physical_2025} \\
2025 & RaceVLA & Vision + Language + Action Trajectory & Navigation/interaction & Not explicitly specified & Not explicitly specified & \cite{serpiva_racevla_2025} \\
2025 & Large VLM-based Vision-Language-Action Models for Robotic Manipulation & Vision + Depth/3D + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Not explicitly specified & Not explicitly specified & \cite{shao_large_2025} \\
2025 & SmolVLA & Vision + Language + Action Trajectory & General embodied tasks & Real-world & Not explicitly specified & \cite{shukor_smolvla_2025} \\
2025 & OG-VLA & Vision + Depth/3D + Language + Action Trajectory & Robotic manipulation & Real-world & 5 demonstrations & \cite{singh_og-vla_2025} \\
2025 & RationalVLA & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & 14,000 samples; 000 samples & \cite{song_rationalvla_2025} \\
2025 & ReconVLA & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & 100k trajectories & \cite{song_reconvla_2025} \\
2025 & GeoVLA & Vision + Depth/3D + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{sun_geovla_2025} \\
2025 & ExpReS-VLA & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & 12 demonstrations & \cite{syed_expres-vla_2025} \\
2025 & RealMirror & Vision + Depth/3D + Language + Action Trajectory & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{tai_realmirror_2025} \\
2025 & Interactive Post-Training for Vision-Language-Action Models & Vision + Language + Action Trajectory + Force/Tactile/Audio & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{tan_interactive_2025} \\
2025 & Latent Chain-of-Thought World Modeling for End-to-End Driving & Vision + Language + Action Trajectory + Force/Tactile/Audio & Autonomous driving & Not explicitly specified & Not explicitly specified & \cite{tan_latent_2025} \\
2025 & NinA & Vision + Language + Action Trajectory & General embodied tasks & Real-world & Not explicitly specified & \cite{tarasov_nina_2025} \\
2025 & GigaBrain-0 & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{team_gigabrain-0_2025} \\
2025 & GigaWorld-0 & Vision + Depth/3D + Language + Action Trajectory & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{team_gigaworld-0_2025} \\
2025 & Latent Action Pretraining Through World Modeling & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{tharwat_latent_2025} \\
2025 & Unified Vision-Language-Action Model & Vision + Language + Action Trajectory & Autonomous driving & Hybrid (real + simulated) & Not explicitly specified & \cite{wang_unified_2025} \\
2025 & VLA-Adapter & Vision + Language + Action Trajectory & General embodied tasks & Real-world & 8 hours & \cite{wang_vla-adapter_2025} \\
2025 & VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning & Vision + Language + Action Trajectory + Force/Tactile/Audio & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{wang_vla_2025} \\
2025 & VQ-VLA & Vision + Language + Action Trajectory & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{wang_vq-vla_2025} \\
2025 & dVLA & Vision + Language + Action Trajectory & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{wen_dvla_2025} \\
2025 & Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model & Vision + Language + Action Trajectory & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{won_dual-stream_2025} \\
2025 & World-Env & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{xiao_world-env_2025} \\
2025 & Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models & Vision + Language + Action Trajectory & General embodied tasks & Simulated/Generated & Not explicitly specified & \cite{xu_model-agnostic_2025} \\
2025 & LeVERB & Vision + Language + Action Trajectory + Force/Tactile/Audio & Navigation/interaction & Simulated/Generated & 150 tasks & \cite{xue_leverb_2025} \\
2025 & When Alignment Fails & Vision + Language + Action Trajectory & Robotic manipulation & Not explicitly specified & Not explicitly specified & \cite{yan_when_2025} \\
2025 & Beyond Human Demonstrations & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Simulated/Generated & Not explicitly specified & \cite{yang_beyond_2025} \\
2025 & EfficientVLA & Vision + Language + Action Trajectory & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{yang_efficientvla_2025} \\
2025 & Learning to Feel the Future & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Real-world & Not explicitly specified & \cite{ye_learning_2025} \\
2025 & VLA-R1 & Vision + Language + Action Trajectory + Force/Tactile/Audio & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{ye_vla-r1_2025} \\
2025 & DeepThinkVLA & Vision + Language + Action Trajectory + Force/Tactile/Audio & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{yin_deepthinkvla_2025} \\
2025 & ForceVLA & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Not explicitly specified & Not explicitly specified & \cite{yu_forcevla_2025} \\
2025 & AutoDrive-R\$\textasciicircum2\$ & Vision + Language + Action Trajectory + Force/Tactile/Audio & Autonomous driving & Not explicitly specified & Not explicitly specified & \cite{yuan_autodrive-r2_2025} \\
2025 & DepthVLA & Vision + Depth/3D + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{yuan_depthvla_2025} \\
2025 & RLinf-VLA & Vision + Language + Action Trajectory + Force/Tactile/Audio & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{zang_rlinf-vla_2025} \\
2025 & A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Real-world & Not explicitly specified & \cite{zhai_vision-language-action-critic_2025} \\
2025 & 4D-VLA & Vision + Depth/3D + Language + Action Trajectory & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{zhang_4d-vla_2025} \\
2025 & Align-Then-stEer & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{zhang_align-then-steer_2025} \\
2025 & DreamVLA & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{zhang_dreamvla_2025} \\
2025 & IRef-VLA & Vision + Depth/3D + Language + Action Trajectory & Navigation/interaction & Hybrid (real + simulated) & Not explicitly specified & \cite{zhang_iref-vla_2025} \\
2025 & Reasoning-VLA & Vision + Language + Action Trajectory + Force/Tactile/Audio & Autonomous driving & Not explicitly specified & Not explicitly specified & \cite{zhang_reasoning-vla_2025} \\
2025 & SafeVLA & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Real-world & Not explicitly specified & \cite{zhang_safevla_2025} \\
2025 & TA-VLA & Vision + Language + Action Trajectory + Force/Tactile/Audio & Autonomous driving & Not explicitly specified & Not explicitly specified & \cite{zhang_ta-vla_2025} \\
2025 & UP-VLA & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{zhang_up-vla_2025} \\
2025 & VTLA & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{zhang_vtla_2025} \\
2025 & VLA\textasciicircum2 & Vision + Language + Action Trajectory & Robotic manipulation & Simulated/Generated & Not explicitly specified & \cite{zhao_vla2_2025} \\
2025 & JARVIS & Language + Action Trajectory & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{zheng_jarvis_2025} \\
2025 & X-VLA & Vision + Language + Action Trajectory & General embodied tasks & Hybrid (real + simulated) & Not explicitly specified & \cite{zheng_x-vla_2025} \\
2025 & FlowVLA & Vision + Language + Action Trajectory & Robotic manipulation & Not explicitly specified & Not explicitly specified & \cite{zhong_flowvla_2025} \\
2025 & ChatVLA & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{zhou_chatvla_2025} \\
2025 & ObjectVLA & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{zhu_objectvla_2025} \\
2026 & \$π\_0\$ & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & Not explicitly specified & \cite{black__0_2026} \\
2026 & InternVLA-A1 & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{cai_internvla-a1_2026} \\
2026 & BridgeV2W & Vision + Action Trajectory & General embodied tasks & Real-world & Not explicitly specified & \cite{chen_bridgev2w_2026} \\
2026 & CombatVLA & Vision + Depth/3D + Language + Action Trajectory & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{chen_combatvla_2026} \\
2026 & Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing & Vision + Depth/3D + Language & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{fan_wow_2026} \\
2026 & Vision-Language-Action Models for Autonomous Driving & Vision + Language + Action Trajectory & Autonomous driving & Not explicitly specified & Not explicitly specified & \cite{hu_vision-language-action_2026} \\
2026 & Flow Equivariant World Models & Vision + Depth/3D & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{lillemark_flow_2026} \\
2026 & On-the-Fly VLA Adaptation via Test-Time Reinforcement Learning & Vision + Language + Action Trajectory + Force/Tactile/Audio & General embodied tasks & Real-world & Not explicitly specified & \cite{liu_--fly_2026} \\
2026 & What Can RL Bring to VLA Generalization\pi An Empirical Study & Vision + Language + Action Trajectory + Force/Tactile/Audio & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{liu_what_2026} \\
2026 & NitroGen & Vision + Depth/3D + Action Trajectory & General embodied tasks & Simulated/Generated & 40,000 hours; 000 hours & \cite{magne_nitrogen_2026} \\
2026 & Video Generation Models in Robotics -- Applications, Research Challenges, Future & Vision + Language + Action Trajectory + Force/Tactile/Audio & Autonomous driving & Simulated/Generated & Not explicitly specified & \cite{mei_video_2026} \\
2026 & ReWorld & Vision + Language + Force/Tactile/Audio & Robotic manipulation & Simulated/Generated & Not explicitly specified & \cite{peng_reworld_2026} \\
2026 & WorldBench & Vision + Action Trajectory & General embodied tasks & Real-world & Not explicitly specified & \cite{upadhyay_worldbench_2026} \\
2026 & A Mechanistic View on Video Generation as World Models & Vision + Language & General embodied tasks & Simulated/Generated & Not explicitly specified & \cite{wang_mechanistic_2026} \\
2026 & A Pragmatic VLA Foundation Model & Vision + Language + Action Trajectory & Robotic manipulation & Real-world & 20,000 hours; 000 hours & \cite{wu_pragmatic_2026} \\
2026 & Parallels Between VLA Model Post-Training and Human Motor Learning & Vision + Language + Action Trajectory & Robotic manipulation & Not explicitly specified & Not explicitly specified & \cite{xiang_parallels_2026} \\
2026 & DynamicVLA & Vision + Language + Action Trajectory & Robotic manipulation & Hybrid (real + simulated) & Not explicitly specified & \cite{xie_dynamicvla_2026} \\
2026 & LatentVLA & Vision + Language + Action Trajectory & Autonomous driving & Not explicitly specified & Not explicitly specified & \cite{xie_latentvla_2026} \\
2026 & Vlaser & Vision + Language + Action Trajectory & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{yang_vlaser_2026} \\
2026 & Dream-VL \\& Dream-VLA & Vision + Language + Action Trajectory & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{ye_dream-vl_2026} \\
2026 & Genie Sim 3.0 & Vision + Language & Robotic manipulation & Hybrid (real + simulated) & 10,000 hours; 000 hours & \cite{yin_genie_2026} \\
2026 & AC\textasciicircum2-VLA & Vision + Depth/3D + Language + Action Trajectory & Robotic manipulation & Not explicitly specified & Not explicitly specified & \cite{yu_ac2-vla_2026} \\
2026 & A Survey on Efficient Vision-Language-Action Models & Vision + Language + Action Trajectory & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{yu_survey_2026} \\
2026 & CompliantVLA-adaptor & Vision + Language + Action Trajectory + Force/Tactile/Audio & Robotic manipulation & Simulated/Generated & Not explicitly specified & \cite{zhang_compliantvla-adaptor_2026} \\
2026 & VLM4VLA & Vision + Depth/3D + Language + Action Trajectory & General embodied tasks & Not explicitly specified & Not explicitly specified & \cite{zhang_vlm4vla_2026} \\
\end{xltabular}
\end{landscape}
