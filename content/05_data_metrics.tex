\section{Data Resources and Evaluation Metrics}
\label{sec:data-metrics}

\subsection{Data Regimes}

Recent embodied research uses four complementary data regimes:
\begin{itemize}
    \item \textbf{Simulation-first corpora} for scalable policy/world-model pretraining.
    \item \textbf{Interactive benchmark suites} for closed-loop reproducibility.
    \item \textbf{Large offline robot datasets} for foundation model initialization.
    \item \textbf{Real-world deployment logs} for post-training and robustness analysis.
\end{itemize}

Representative resources include OpenVLA/Open-X style pipelines, DROID-scale data, and newer embodied world-model benchmarks focused on rollout quality and control relevance \citep{kim_openvla_2024,collaboration_open_2025,khazatsky_droid_2025,upadhyay_worldbench_2026,wu_what_2026}.

\textbf{Simulation-first corpora} remain the fastest path for broad pretraining and ablation-heavy development. \textbf{Interactive benchmark suites} improve comparability but can overfit to narrow task interfaces when evaluation protocols are static. \textbf{Offline robot datasets} enable large-scale behavioral priors but inherit teleoperation and sensor bias. \textbf{Real-world logs} are the only reliable source for intervention dynamics, recovery behavior, and edge-case calibration \citep{team_octo_2024,fei_libero-plus_2025,intelligence__06_2025,wu_pragmatic_2026}.

\subsection{Data Curation Dimensions}

Beyond raw scale, we find four curation dimensions that strongly affect downstream behavior:
\begin{enumerate}
    \item \textbf{Embodiment diversity} (single-arm, dual-arm, mobile manipulation, driving stacks).
    \item \textbf{Task horizon composition} (short atomic skills vs. multi-stage household workflows).
    \item \textbf{Interaction richness} (contact-heavy manipulation, tool use, intervention events).
    \item \textbf{Annotation granularity} (language, subgoal, proprioceptive traces, safety labels).
\end{enumerate}
Papers that only scale data volume without balancing these dimensions often improve headline benchmark averages but underperform in open-world deployment conditions \citep{intelligence__05_2025,intelligence__06_2025,li_vla-rft_2025,valle_evaluating_2025}.

\subsection{Metric Families}

We group evaluation metrics into five families:
\begin{enumerate}
    \item \textbf{Task success and completion quality} (success rate, throughput, long-horizon completion).
    \item \textbf{Control stability and safety} (collision, intervention, recovery latency).
    \item \textbf{Prediction fidelity} (perceptual quality, trajectory agreement, state consistency).
    \item \textbf{Generalization} (new scene, new object, new instruction, cross-embodiment transfer).
    \item \textbf{Efficiency} (token/action efficiency, runtime latency, memory/compute cost).
\end{enumerate}

Several recent papers explicitly report tradeoffs between closed-loop gains and compute/latency costs, making efficiency metrics first-class rather than optional \citep{pertsch_fast_2025,yang_efficientvla_2025,guan_efficient_2025,shen_efficient_2026}.

\subsection{Abstract-Level Quantitative Signals}

Although protocols differ across papers, several abstract-reported numbers illustrate why evaluation must go beyond a single success metric:
\begin{itemize}
    \item FAST reports up to $5\times$ training-time reduction under high-frequency dexterous settings \citep{pertsch_fast_2025}.
    \item RECAP ($\pi^*_{0.6}$) reports more than doubling throughput and roughly halving failure rate on difficult tasks \citep{intelligence__06_2025}.
    \item VLA-RFT reports surpassing strong supervised baselines with fewer than 400 fine-tuning steps in simulator-driven RL fine-tuning \citep{li_vla-rft_2025}.
    \item ConRFT reports evaluation on eight real-world manipulation tasks with a unified offline+online consistency objective \citep{chen_conrft_2025}.
    \item Valle et al.\ highlight that pure task success masks uncertainty and execution quality, motivating dedicated uncertainty and quality metrics \citep{valle_evaluating_2025}.
\end{itemize}

These signals jointly support the same conclusion: \textbf{evaluation must be multi-objective}, combining competence, reliability, and efficiency.

A minimal closed-loop metric set can be formalized as:
\begin{equation}
\mathrm{SR}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{1}[\text{task }i\text{ succeeds}],
\end{equation}
\begin{equation}
\mathrm{IR}=\frac{1}{N}\sum_{i=1}^{N}\frac{n_i^{\text{intervention}}}{T_i},
\end{equation}
\begin{equation}
\mathrm{RTF}=\frac{\text{inference\,+\,planning time}}{\text{control horizon time}},
\end{equation}
where SR measures task competence, IR captures autonomy reliability, and RTF captures real-time feasibility. This triad is often more diagnostic than isolated success-rate reporting.

\subsection{Evaluation Protocol Recommendations}

For reproducible and decision-relevant reporting, we recommend:
\begin{itemize}
    \item reporting at least one metric from each family (task, safety, prediction, generalization, efficiency),
    \item separating in-distribution and shifted-distribution performance,
    \item reporting intervention-aware curves (success vs. allowed interventions),
    \item documenting compute budget, control frequency, and model update policy.
\end{itemize}
These elements are increasingly present in recent benchmark-oriented work and should become default for embodied world-model evaluation \citep{upadhyay_worldbench_2026,wu_what_2026,wang_unified_2025}.

In addition, recent diagnostic benchmarks explicitly target disentangled physical understanding. WorldBench emphasizes concept-level disambiguation rather than entangled aggregate physics tests, making failure attribution more actionable for model iteration \citep{upadhyay_worldbench_2026}.

\subsection{Current Gaps}

Despite progress, metric mismatch remains common: image-level prediction quality may not imply physically correct interaction outcomes, and short-horizon gains may not transfer to multi-stage tasks \citep{gupta_essential_2024,valle_evaluating_2025,wang_unified_2025}. This gap motivates evaluation protocols that jointly report dynamics realism, decision quality, and deployment behavior.
