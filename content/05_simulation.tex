\section{Simulation, Digital Twins, and Synthetic Environments}
\label{sec:simulation}

The paradigm of Embodied AI is undergoing a fundamental shift from learning from static, finite datasets to learning in dynamic, infinite, and interactive environments. While real-world data remains the gold standard for fidelity, it is fundamentally constrained by the high cost of collection, safety risks associated with trial-and-error learning, and the inability to scale to long-tail scenarios \cite{liang_large_2025}\cite{lu_multimodal_2025}. As Vision-Language-Action (VLA) models scale to billions of parameters \cite{black__0_2026, intelligence__05_2025}, and initiatives like Open X-Embodiment \cite{oneill_open_2024} consolidate diverse datasets to build generalist policies \cite{li_towards_2024}, their hunger for data outpaces the capability of physical robot farms. Consequently, advanced simulation platforms, high-fidelity digital twins, and procedurally generated ``Infinite Worlds'' have emerged as critical pillars for training generalist world models \cite{ding_understanding_2025}\cite{fung_embodied_2025}. This transition mirrors the evolution in Large Language Models (LLMs), where synthetic data is increasingly used to refine reasoning; in robotics, simulation \textit{is} the engine of synthetic experience.

The evolution of robotic simulation can be traced through three distinct generations. The first generation (e.g., Gazebo, PyBullet) focused on kinematic accuracy and rigid body dynamics but lacked visual fidelity. The second generation (e.g., AI2-THOR, Habitat \cite{batra_rearrangement_2020}, iGibson) prioritized visual realism through 3D scanning and ray-tracing. We are now entering the third generation: \textbf{Neural and Generative Simulation}. This era is characterized by differentiable physics, neural rendering (NeRF/3DGS), and procedurally generated content that scales with compute \cite{gupta_essential_2024}.

In this section, we survey the state-of-the-art in this third generation of simulation. We categorize these developments into four key pillars:
\begin{enumerate}
    \item \textbf{Procedural Content Generation (PCG)}: Algorithms for creating unbounded, diverse environments (Section \ref{subsec:infinite_worlds}).
    \item \textbf{Digital Twins and Neural Rendering}: Techniques for creating photorealistic replicas of the real world with explicit physics (Section \ref{subsec:digital_twins}).
    \item \textbf{Sim-to-Real Mathematical Formalisms}: The theoretical underpinnings of Domain Randomization and System Identification (Section \ref{subsec:sim_to_real}).
    \item \textbf{Neural Simulators}: The emerging use of video generation models as learned physics engines (Section \ref{subsec:neural_simulators}).
\end{enumerate}

\subsection{Procedural Generation and Infinite Worlds}
\label{subsec:infinite_worlds}

The primary bottleneck in scaling embodied agents is not model architecture, but the scarcity of diverse 3D environments. Traditional simulators rely on manually designed assets or fixed 3D scans, which limits the agent's exposure to the ``long tail'' of real-world variations. The concept of ``Infinite Worlds'' addresses this by leveraging Procedural Content Generation (PCG) to create unbounded training environments, enabling open-ended learning \cite{wang_voyager_2023, sarch_open-ended_2023}.

\subsubsection{Algorithmic Foundations of PCG}
Procedural generation in robotics draws heavily from computer graphics but imposes stricter constraints on physical plausibility and semantic consistency.
\begin{itemize}
    \item \textbf{Wave Function Collapse (WFC)}: A constraint satisfaction algorithm that generates consistent tile-based layouts. In robotic simulation, WFC ensures that generated floorplans obey architectural rules (e.g., connectivity, accessibility), preventing dead ends that would confuse navigation agents \cite{dorbala_can_2023}.
    \item \textbf{L-Systems and Shape Grammars}: Employed to synthesize diverse organic structures and architectural elements. By varying production rules, a simulator can generate infinite variations of trees or buildings without storing meshes.
    \item \textbf{Fractal Noise Synthesis}: Essential for terrain generation, creating realistic heightmaps for outdoor navigation benchmarks utilized by legged robots \cite{li_vla-rft_2025}.
\end{itemize}

\textbf{Infinigen} \cite{yang_physcene_2024} exemplifies the state-of-the-art in this domain. Unlike prior systems, Infinigen synthesizes geometry and textures from scratch using randomized mathematical rules. This ``asset-free'' approach provides ``pixel-perfect'' ground truth for auxiliary objectives (depth, optical flow, segmentation) and prevents VLA models from overfitting to specific object instances \cite{huang_grounded_2023}.

\subsubsection{Generative Asset Synthesis}
Integration of Generative AI into simulation pipelines is accelerating. Models like Shap-E and Point-E allow simulators to conjure 3D assets from text descriptions on the fly:
\begin{equation}
\text{Asset}(c) = G_{\text{3D}}(T(c))
\end{equation}
where $c$ is a semantic category and $G_{\text{3D}}$ is a text-to-3D generator. This enables the creation of specific objects needed for instruction following tasks, such as ``pick up the cat-shaped mug'' \cite{dorbala_can_2023}.

The generative capabilities underpinning these assets trace back to foundational work in generative information extraction \cite{josifoski_genie_2022}, which has evolved into sophisticated systems capable of achieving human parity in content-grounded generation \cite{yehudai_genie_2024}. These advances allow simulators to populate worlds not just with objects, but with semantically rich, interactive content.

\subsubsection{City-Scale and Social Simulation}
The scope of simulation is expanding to city-scale digital twins. \textbf{Urban Generative Intelligence (UGI)} \cite{xu_urban_2023} represents a leap in this direction. UGI integrates Large Language Models (LLMs) to populate environments with reactive social agents \cite{zhang_building_2024}. In this framework, non-player characters (NPCs) are governed by LLM agents with explicit goals and personalities \cite{gao_dialfred_2022}, enabling robots to learn social navigation and intent inference in safety-critical urban scenarios.

\subsection{Digital Twins and Neural Rendering}
\label{subsec:digital_twins}

Digital Twins bridge the gap between procedural diversity and real-world fidelity \cite{liu_aligning_2025}. The field is witnessing a rapid migration from mesh-based rasterization to neural rendering techniques.

\subsubsection{The Rise of Neural Radiance Fields (NeRFs)}
Neural Radiance Fields (NeRFs) revolutionized scene reconstruction by encoding volume density $\sigma$ and color $\mathbf{c}$ in the weights of a Multi-Layer Perceptron (MLP):
\begin{equation}
F_\Theta(\mathbf{x}, \mathbf{d}) \to (\mathbf{c}, \sigma)
\end{equation}
While NeRFs capture complex, non-Lambertian surfaces better than traditional scanning, they suffer from slow inference speeds and implicit geometry, making physics integration difficult \cite{leonardis_embodied_2025}.

\subsubsection{3D Gaussian Splatting (3DGS): The New Standard}
\textbf{3D Gaussian Splatting (3DGS)} has emerged as a strong alternative for robotic simulation. 3DGS represents a scene as a set of anisotropic 3D Gaussians, offering explicit geometry for physics (collision detection via BVH) and real-time differentiable rendering.

\textbf{RoboGSim} \cite{tai_realmirror_2025} utilizes 3DGS to enable real-time, photorealistic reconstruction of real-world environments. By leveraging differentiability, RoboGSim allows for scene editing---removing or rearranging objects by manipulating Gaussians---turning static scans into dynamic training grounds.

\subsubsection{Neural Physics and Differentiable Simulation}
Differentiable physics engines (e.g., Brax, DiffTaichi) allow gradients to propagate from reward signals back to policy or system parameters \cite{brehmer_edgi_2023}. When combined with neural rendering, this enables end-to-end optimization of the entire pipeline, from pixels to torques \cite{song_llm-planner_2023}.

\subsection{Sim-to-Real Transfer: Mathematical Formalisms}
\label{subsec:sim_to_real}

The ultimate test of simulation is the successful transfer of learned policies to the physical world, governed by the trade-off between \textbf{Domain Randomization (DR)} and \textbf{System Identification (SysID)}.

\subsubsection{Domain Randomization (DR)}
DR creates a distribution of simulated environments to ensure policy invariance. Formally, we optimize policy $\pi_\theta$ over a distribution of environments $\mathcal{E}_\phi$:
\begin{equation}
\label{eq:dr_objective}
\max_\theta \mathbb{E}_{\phi \sim P(\Phi)} \left[ \mathbb{E}_{\tau \sim \pi_\theta(\cdot | \mathcal{E}_\phi)} [R(\tau)] \right]
\end{equation}
\textbf{Automatic Domain Randomization (ADR)} techniques adaptively update $P(\Phi)$ to maintain a curriculum. Recent embodied RL/VLA studies increasingly combine randomization and reward design to improve transfer \cite{li_vla-rft_2025}.

\subsubsection{System Identification (SysID)}
SysID optimizes simulation parameters $\phi$ to match real-world observations $y_{\text{real}}$:
\begin{equation}
\label{eq:sysid_objective}
\min_\phi \| y_{\text{sim}}(\phi, \pi) - y_{\text{real}} \|^2_2
\end{equation}
Recent advances allow this optimization via gradient descent, enabling world models to learn specific robot dynamics (e.g., motor backlash) directly from interaction data \cite{intelligence__06_2025}.

\subsubsection{Appearance Adaptation}
To bridge the visual gap, approaches use \textbf{Domain-Invariant Feature Learning}, training perception backbones to map both domains to a common latent space using contrastive losses or masked auto-encoding, minimizing Maximum Mean Discrepancy (MMD):
\begin{equation}
\mathcal{L}_{\text{adapt}} = \text{MMD}(f(X_{\text{sim}}), f(X_{\text{real}}))
\end{equation}

\subsubsection{Cross-Embodiment and High-Speed Adaptation}
Transferring policies to novel robot morphologies remains a distinct challenge. Research indicates that bringing foundation models like RT-1-X to new kinematics (e.g., SCARA robots) often necessitates targeted fine-tuning strategies \cite{salzer_bringing_2024}. Furthermore, closing the sim-to-real gap for dynamic tasks requires improving the speed and accuracy of predictive learning models \cite{yoshikawa_achieving_2024} to handle real-time constraints.

\subsection{Neural Simulators: The Video Generation Shift}
\label{subsec:neural_simulators}

A paradigm shift in 2025/2026 is the move to \textbf{Neural Simulators}---generative video models that predict future frames $x_{t+1}$ given current frame $x_t$ and action $a_t$:
\begin{equation}
x_{t+1} \sim P_\psi(x_{t+1} | x_t, a_t, c)
\end{equation}
These models offer universal physics (modeling fluids, soft bodies) and open-set generalization \cite{nottingham_embodied_2023}.

\textbf{BridgeV2W} \cite{chen_bridgev2w_2026} bridges video generation to embodied control via \textit{embodiment masks}, projecting the robot's kinematic chain into the video generator to enforce physical consistency. \textbf{DreamVLA} \cite{zhang_dreamvla_2025} utilizes diffusion backbones to predict future outcomes for long-horizon planning, allowing agents to perform MCTS in latent space.

Challenges include temporal consistency (drift in long rollouts), controllability, and lack of ground-truth state access \cite{wu_plan_2023}.

Beyond pure video prediction, the integration of reasoning and control is paramount. New frameworks leverage Vision Language World Models (VLWM) to perform explicit planning with reasoning \cite{chen_planning_2025}. To support efficient execution, techniques like RT-Trajectory \cite{gu_rt-trajectory_2023} utilize coarse trajectory sketches to guide generalization, while SARA-RT \cite{leal_sara-rt_2024} employs self-adaptive attention to scale performance. Crucially, recent work on ``knowledge insulating'' architectures \cite{driess_knowledge_2025} suggests that separating high-level semantic knowledge from low-level control dynamics can significantly improve generalization and training efficiency in these advanced VLA systems.

\subsection{Standardized Benchmarks and Evaluation}
\label{subsec:benchmarks}

Rigorous benchmarks are essential for evaluating generalist agents.

\begin{table}[ht]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l p{2.5cm} p{5.5cm} p{3cm}}
\toprule
\textbf{Benchmark} & \textbf{Primary Focus} & \textbf{Key Features} & \textbf{Notable Works} \\ \midrule
\textbf{LIBERO} & Lifelong Learning & Knowledge transfer across 90 tasks; tests catastrophic forgetting. & \cite{fei_libero-plus_2025}\cite{li_simplevla-rl_2025} \\ \midrule
\textbf{CALVIN} & Long-horizon & Language sequencing and continuous control from onboard sensors. & \cite{fan_long-vla_2025}\cite{lin_hif-vla_2025} \\ \midrule
\textbf{SimplerEnv} & Visual Matching & Real-to-Sim evaluation via photorealistic matching of real setups. & \cite{wu_what_2026}\cite{chen_villa-x_2025} \\ \midrule
\textbf{ManiSkill} & Dexterous Manip. & GPU-parallelized physics for generalizable skills. & \cite{sun_geovla_2025} \cite{zang_rlinf-vla_2025} \\ \midrule
\textbf{Behavior-1K} & Human-level Tasks & 1000+ everyday activities in realistic scenes. & \cite{li_embodied_2024} \\ \midrule
\textbf{RoboCasa} & Household Dynamics & Generative simulation of 100+ kitchen scenes. & \cite{tai_realmirror_2025} \\ \midrule
\textbf{TEACh} & Dialogue & Conversational agents for embodied tasks. & \cite{gao_dialfred_2022}\cite{zheng_jarvis_2025} \\ \midrule
\textbf{ALFRED} & Instruction Following & Long-horizon tasks with state changes. & \cite{gao_dialfred_2022} \\ \bottomrule
\end{tabular}
\caption{Comparison of Key Embodied AI Benchmarks (2025--2026).} \label{tab:benchmarks}
\end{table}

\textbf{LIBERO} \cite{fei_libero-plus_2025} evaluates knowledge transfer. \textbf{CALVIN} \cite{fan_long-vla_2025} benchmarks long-sequence instruction following. \textbf{SimplerEnv} \cite{wu_what_2026} addresses predictive evaluation. \textbf{RoboCasa} \cite{tai_realmirror_2025} and \textbf{Behavior-1K} \cite{li_embodied_2024} push semantic complexity. Minimizing the \textbf{Sim-to-Real Gap} remains the central objective:
\begin{equation}
\text{Gap} = \frac{\text{SR}_{\text{sim}} - \text{SR}_{\text{real}}}{\text{SR}_{\text{sim}}}
\end{equation}
