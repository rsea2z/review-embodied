cite_key,title,year,is_embodied,is_world_model,is_in_scope,time_in_scope,section_bucket,priority_tier,exclude_reason,source_pdf_path,notes
batra_rearrangement_2020,Rearrangement: {A} {Challenge} for {Embodied} {AI},2020,true,false,false,false,data-benchmark-eval,supporting,out_of_window,,
ahn_as_2022,"Do {As} {I} {Can}, {Not} {As} {I} {Say}: {Grounding} {Language} in {Robotic} {Affordances}",2022,true,false,false,false,foundation-definition,supporting,out_of_window,,
duan_survey_2022,A survey of embodied ai: {From} simulators to research tasks,2022,true,false,false,false,survey-meta,core,out_of_window,,
fan_minedojo_2022,{MineDojo}: {Building} {Open}-{Ended} {Embodied} {Agents} with {Internet}-{Scale} {Knowledge},2022,true,false,false,false,planning-reasoning,supporting,out_of_window,,
gadre_continuous_2022,Continuous scene representations for embodied ai,2022,true,false,false,false,foundation-definition,supporting,out_of_window,,
gao_dialfred_2022,Dialfred: {Dialogue}-enabled agents for embodied instruction following,2022,true,false,false,false,foundation-definition,supporting,out_of_window,,
huang_inner_2022,Inner {Monologue}: {Embodied} {Reasoning} through {Planning} with {Language} {Models},2022,true,false,false,false,planning-reasoning,supporting,out_of_window,,
huang_language_2022,Language models as zero-shot planners: {Extracting} actionable knowledge for embodied agents,2022,true,false,false,false,planning-reasoning,supporting,out_of_window,,
reed_generalist_2022,A {Generalist} {Agent},2022,true,false,false,false,foundation-definition,supporting,out_of_window,,
bousmalis_robocat_2023,{RoboCat}: {A} {Self}-{Improving} {Generalist} {Agent} for {Robotic} {Manipulation},2023,true,false,false,false,data-benchmark-eval,supporting,out_of_window,,
brehmer_edgi_2023,Edgi: {Equivariant} diffusion for planning with embodied agents,2023,true,false,false,false,planning-reasoning,supporting,out_of_window,,
brohan_rt-1_2023,{RT}-1: {Robotics} {Transformer} for {Real}-{World} {Control} at {Scale},2023,true,false,false,false,data-benchmark-eval,supporting,out_of_window,,
chebotar_q-transformer_2023,Q-{Transformer}: {Scalable} {Offline} {Reinforcement} {Learning} via {Autoregressive} {Q}-{Functions},2023,true,false,false,false,data-benchmark-eval,supporting,out_of_window,,
dasgupta_collaborating_2023,Collaborating with language models for embodied reasoning,2023,true,false,false,false,planning-reasoning,supporting,out_of_window,,
dorbala_can_2023,Can an embodied agent find your “cat-shaped mug”? llm-based zero-shot object navigation,2023,true,false,false,false,agent-architecture,supporting,out_of_window,,
driess_palm-e_2023,{PaLM}-{E}: {An} {Embodied} {Multimodal} {Language} {Model},2023,true,false,false,false,data-benchmark-eval,supporting,out_of_window,,
gu_rt-trajectory_2023,{RT}-{Trajectory}: {Robotic} {Task} {Generalization} via {Hindsight} {Trajectory} {Sketches},2023,true,false,false,false,foundation-definition,supporting,out_of_window,,
huang_grounded_2023,Grounded {Decoding}: {Guiding} {Text} {Generation} with {Grounded} {Models} for {Embodied} {Agents},2023,true,false,false,false,agent-architecture,supporting,out_of_window,,
huang_voxposer_2023,{VoxPoser}: {Composable} {3D} {Value} {Maps} for {Robotic} {Manipulation} with {Language} {Models},2023,true,true,false,false,world-model-core,core,out_of_window,,
jiang_vima_2023,{VIMA}: {General} {Robot} {Manipulation} with {Multimodal} {Prompts},2023,true,false,false,false,data-benchmark-eval,supporting,out_of_window,,
liang_code_2023,Code as {Policies}: {Language} {Model} {Programs} for {Embodied} {Control},2023,true,false,false,false,data-benchmark-eval,supporting,out_of_window,,
nottingham_embodied_2023,Do embodied agents dream of pixelated sheep: {Embodied} decision making using language guided world modelling,2023,true,true,false,false,world-model-core,core,out_of_window,,
sarch_open-ended_2023,Open-{Ended} {Instructable} {Embodied} {Agents} with {Memory}-{Augmented} {Large} {Language} {Models},2023,true,false,false,false,data-benchmark-eval,supporting,out_of_window,,
song_llm-planner_2023,{LLM}-{Planner}: {Few}-{Shot} {Grounded} {Planning} for {Embodied} {Agents} with {Large} {Language} {Models},2023,true,false,false,false,data-benchmark-eval,supporting,out_of_window,,
walke_bridgedata_2023,Bridgedata v2: {A} dataset for robot learning at scale,2023,true,false,false,false,data-benchmark-eval,supporting,out_of_window,,
wang_voyager_2023,Voyager: {An} {Open}-{Ended} {Embodied} {Agent} with {Large} {Language} {Models},2023,true,false,false,false,policy-learning,supporting,out_of_window,,
wu_embodied_2023,Embodied {Task} {Planning} with {Large} {Language} {Models},2023,true,false,false,false,data-benchmark-eval,supporting,out_of_window,,
wu_plan_2023,"Plan, {Eliminate}, and {Track} -- {Language} {Models} are {Good} {Teachers} for {Embodied} {Agents}",2023,true,false,false,false,data-benchmark-eval,supporting,out_of_window,,
wu_unleashing_2023,Unleashing {Large}-{Scale} {Video} {Generative} {Pre}-training for {Visual} {Robot} {Manipulation},2023,true,false,false,false,data-benchmark-eval,supporting,out_of_window,,
xu_urban_2023,Urban {Generative} {Intelligence} ({UGI}): {A} {Foundational} {Platform} for {Agents} in {Embodied} {City} {Environment},2023,true,false,false,false,agent-architecture,supporting,out_of_window,,
zhao_learning_2023,Learning {Fine}-{Grained} {Bimanual} {Manipulation} with {Low}-{Cost} {Hardware},2023,true,false,false,false,policy-learning,supporting,out_of_window,,
ahmaditeshnizi_optimus_2024,{OptiMUS}: {Scalable} {Optimization} {Modeling} with ({MI}){LP} {Solvers} and {Large} {Language} {Models},2024,true,false,true,true,data-benchmark-eval,supporting,,,
bruce_genie_2024,Genie: {Generative} interactive environments,2024,false,false,false,true,foundation-definition,supporting,not_embodied_related,,
cheang_gr-2_2024,{GR}-2: {A} {Generative} {Video}-{Language}-{Action} {Model} with {Web}-{Scale} {Knowledge} for {Robot} {Manipulation},2024,true,false,true,true,foundation-definition,supporting,,,
chi_diffusion_2024,Diffusion {Policy}: {Visuomotor} {Policy} {Learning} via {Action} {Diffusion},2024,true,false,true,true,data-benchmark-eval,supporting,,,
guo_embodied_2024,Embodied {LLM} {Agents} {Learn} to {Cooperate} in {Organized} {Teams},2024,true,false,true,true,planning-reasoning,supporting,,,
gupta_essential_2024,The {Essential} {Role} of {Causality} in {Foundation} {World} {Models} for {Embodied} {AI},2024,true,true,true,true,world-model-core,core,,,
hong_multiply_2024,Multiply: {A} multisensory object-centric embodied large language model in 3d world,2024,true,false,true,true,foundation-definition,supporting,,,
huang_copa_2024,{CoPa}: {General} {Robotic} {Manipulation} through {Spatial} {Constraints} of {Parts} with {Foundation} {Models},2024,true,false,true,true,planning-reasoning,supporting,,,
huang_embodied_2024,An {Embodied} {Generalist} {Agent} in {3D} {World},2024,true,false,true,true,data-benchmark-eval,supporting,,,
kazemi_learning_2024,Learning {Generative} {Interactive} {Environments} {By} {Trained} {Agent} {Exploration},2024,true,true,true,true,data-benchmark-eval,core,,,
kim_openvla_2024,{OpenVLA}: {An} {Open}-{Source} {Vision}-{Language}-{Action} {Model},2024,true,false,true,true,data-benchmark-eval,supporting,,,
leal_sara-rt_2024,Sara-rt: {Scaling} up robotics transformers with self-adaptive robust attention,2024,true,false,true,true,agent-architecture,supporting,,,
lee_behavior_2024,Behavior {Generation} with {Latent} {Actions},2024,true,false,true,true,data-benchmark-eval,supporting,,,
li_embodied_2024,Embodied agent interface: {Benchmarking} llms for embodied decision making,2024,true,false,true,true,data-benchmark-eval,supporting,,,
li_towards_2024,Towards {Generalist} {Robot} {Policies}: {What} {Matters} in {Building} {Vision}-{Language}-{Action} {Models},2024,true,false,true,true,data-benchmark-eval,supporting,,,
li_vision-language_2024,Vision-{Language} {Foundation} {Models} as {Effective} {Robot} {Imitators},2024,true,false,true,true,data-benchmark-eval,supporting,,,
lin_out_2024,"Out of {Many}, {One}: {Designing} and {Scaffolding} {Proteins} at the {Scale} of the {Structural} {Universe} with {Genie} 2",2024,true,false,true,true,data-benchmark-eval,supporting,,,
oneill_open_2024,Open x-embodiment: {Robotic} learning datasets and rt-x models: {Open} x-embodiment collaboration 0,2024,true,false,true,true,data-benchmark-eval,supporting,,,
salzer_bringing_2024,Bringing the {RT}-1-{X} {Foundation} {Model} to a {SCARA} robot,2024,true,false,true,true,data-benchmark-eval,supporting,,,
team_octo_2024,Octo: {An} {Open}-{Source} {Generalist} {Robot} {Policy},2024,true,false,true,true,data-benchmark-eval,supporting,,,
yang_embodied_2024,Embodied multi-modal agent trained by an llm from a parallel textworld,2024,true,false,true,true,agent-architecture,supporting,,,
yang_physcene_2024,Physcene: {Physically} interactable 3d scene synthesis for embodied ai,2024,true,false,true,true,foundation-definition,supporting,,,
yehudai_genie_2024,Genie: {Achieving} {Human} {Parity} in {Content}-{Grounded} {Datasets} {Generation},2024,true,false,true,true,data-benchmark-eval,supporting,,,
yoshikawa_achieving_2024,Achieving {Faster} and {More} {Accurate} {Operation} of {Deep} {Predictive} {Learning},2024,true,false,true,true,planning-reasoning,supporting,,,
zeng_learning_2024,Learning {Manipulation} by {Predicting} {Interaction},2024,true,false,true,true,data-benchmark-eval,supporting,,,
zhang_building_2024,Building {Cooperative} {Embodied} {Agents} {Modularly} with {Large} {Language} {Models},2024,true,false,true,true,planning-reasoning,supporting,,,
zhen_3d-vla_2024,{3D}-{VLA}: {A} {3D} {Vision}-{Language}-{Action} {Generative} {World} {Model},2024,true,true,true,true,data-benchmark-eval,core,,,
argus_cvla_2025,{cVLA}: {Towards} {Efficient} {Camera}-{Space} {VLAs},2025,true,false,true,true,data-benchmark-eval,supporting,,,
bai_evolve-vla_2025,{EVOLVE}-{VLA}: {Test}-{Time} {Training} from {Environment} {Feedback} for {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,agent-architecture,supporting,,,
bendikas_focusing_2025,Focusing on {What} {Matters}: {Object}-{Agent}-centric {Tokenization} for {Vision} {Language} {Action} models,2025,true,false,true,true,agent-architecture,supporting,,,
berg_semantic_2025,Semantic {World} {Models},2025,true,true,true,true,world-model-core,core,,,
bhat_3d_2025,{3D} {CAVLA}: {Leveraging} {Depth} and {3D} {Context} to {Generalize} {Vision} {Language} {Action} {Models} for {Unseen} {Tasks},2025,true,false,true,true,data-benchmark-eval,supporting,,,
bi_motus_2025,Motus: {A} {Unified} {Latent} {Action} {World} {Model},2025,true,true,true,true,world-model-core,core,,,
bi_vla-touch_2025,{VLA}-{Touch}: {Enhancing} {Vision}-{Language}-{Action} {Models} with {Dual}-{Level} {Tactile} {Feedback},2025,true,false,true,true,data-benchmark-eval,supporting,,,
budzianowski_edgevla_2025,{EdgeVLA}: {Efficient} {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,planning-reasoning,supporting,,,
cen_rynnvla-002_2025,{RynnVLA}-002: {A} {Unified} {Vision}-{Language}-{Action} and {World} {Model},2025,true,true,true,true,data-benchmark-eval,core,,,
cen_worldvla_2025,{WorldVLA}: {Towards} {Autoregressive} {Action} {World} {Model},2025,true,true,true,true,world-model-core,core,,,
chen_conrft_2025,{ConRFT}: {A} {Reinforced} {Fine}-tuning {Method} for {VLA} {Models} via {Consistency} {Policy},2025,true,false,true,true,policy-learning,supporting,,,
chen_goal-vla_2025,Goal-{VLA}: {Image}-{Generative} {VLMs} as {Object}-{Centric} {World} {Models} {Empowering} {Zero}-shot {Robot} {Manipulation},2025,true,true,true,true,world-model-core,core,,,
chen_internvla-m1_2025,{InternVLA}-{M1}: {A} {Spatially} {Guided} {Vision}-{Language}-{Action} {Framework} for {Generalist} {Robot} {Policy},2025,true,false,true,true,planning-reasoning,supporting,,,
chen_planning_2025,Planning with {Reasoning} using {Vision} {Language} {World} {Model},2025,true,true,true,true,data-benchmark-eval,core,,,
chen_unified_2025,Unified {Diffusion} {VLA}: {Vision}-{Language}-{Action} {Model} via {Joint} {Discrete} {Denoising} {Diffusion} {Process},2025,true,false,true,true,data-benchmark-eval,supporting,,,
chen_villa-x_2025,villa-{X}: {Enhancing} {Latent} {Action} {Modeling} in {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,planning-reasoning,supporting,,,
chi_impromptu_2025,Impromptu {VLA}: {Open} {Weights} and {Open} {Data} for {Driving} {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,data-benchmark-eval,supporting,,,
collaboration_open_2025,Open {X}-{Embodiment}: {Robotic} {Learning} {Datasets} and {RT}-{X} {Models},2025,true,false,true,true,data-benchmark-eval,supporting,,,
cui_end--end_2025,End-to-{End} {Dexterous} {Arm}-{Hand} {VLA} {Policies} via {Shared} {Autonomy}: {VR} {Teleoperation} {Augmented} by {Autonomous} {Hand} {VLA} {Policy} for {Efficient} {Data} {Collection},2025,true,false,true,true,data-benchmark-eval,supporting,,,
deng_graspvla_2025,{GraspVLA}: a {Grasping} {Foundation} {Model} {Pre}-trained on {Billion}-scale {Synthetic} {Action} {Data},2025,true,false,true,true,data-benchmark-eval,supporting,,,
dey_revla_2025,Revla: {Reverting} visual domain limitation of robotic foundation models,2025,true,false,true,true,agent-architecture,supporting,,,
din_vision_2025,Vision {Language} {Action} {Models} in {Robotic} {Manipulation}: {A} {Systematic} {Review},2025,true,false,true,true,data-benchmark-eval,supporting,,,
ding_humanoid-vla_2025,Humanoid-{VLA}: {Towards} {Universal} {Humanoid} {Control} with {Visual} {Integration},2025,true,false,true,true,data-benchmark-eval,supporting,,,
ding_understanding_2025,Understanding {World} or {Predicting} {Future}? {A} {Comprehensive} {Survey} of {World} {Models},2025,true,true,true,true,survey-meta,core,,,
dolgopolyi_bridging_2025,"Bridging {Perception}, {Language}, and {Action}: {A} {Survey} and {Bibliometric} {Analysis} of {VLM} \& {VLA} {Systems}",2025,true,false,true,true,survey-meta,core,,,
dong_vita-vla_2025,{VITA}-{VLA}: {Efficiently} {Teaching} {Vision}-{Language} {Models} to {Act} via {Action} {Expert} {Distillation},2025,true,false,true,true,policy-learning,supporting,,,
driess_knowledge_2025,"Knowledge {Insulating} {Vision}-{Language}-{Action} {Models}: {Train} {Fast}, {Run} {Fast}, {Generalize} {Better}",2025,true,false,true,true,planning-reasoning,supporting,,,
du_himoe-vla_2025,{HiMoE}-{VLA}: {Hierarchical} {Mixture}-of-{Experts} for {Generalist} {Vision}-{Language}-{Action} {Policies},2025,true,false,true,true,data-benchmark-eval,supporting,,,
fan_interleave-vla_2025,Interleave-{VLA}: {Enhancing} {Robot} {Manipulation} with {Interleaved} {Image}-{Text} {Instructions},2025,true,false,true,true,data-benchmark-eval,supporting,,,
fan_long-vla_2025,Long-{VLA}: {Unleashing} {Long}-{Horizon} {Capability} of {Vision} {Language} {Action} {Model} for {Robot} {Manipulation},2025,true,false,true,true,data-benchmark-eval,supporting,,,
fang_dualvla_2025,{DualVLA}: {Building} a {Generalizable} {Embodied} {Agent} via {Partial} {Decoupling} of {Reasoning} and {Action},2025,true,false,true,true,data-benchmark-eval,supporting,,,
fang_sqap-vla_2025,{SQAP}-{VLA}: {A} {Synergistic} {Quantization}-{Aware} {Pruning} {Framework} for {High}-{Performance} {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,agent-architecture,supporting,,,
fei_libero-plus_2025,{LIBERO}-{Plus}: {In}-depth {Robustness} {Analysis} of {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,data-benchmark-eval,supporting,,,
feng_embodied_2025,Embodied {AI}: {From} {LLMs} to {World} {Models} [{Feature}],2025,true,true,true,true,world-model-core,core,,,
feng_spatial-aware_2025,Spatial-{Aware} {VLA} {Pretraining} through {Visual}-{Physical} {Alignment} from {Human} {Videos},2025,true,false,true,true,planning-reasoning,supporting,,,
fu_mergevla_2025,{MergeVLA}: {Cross}-{Skill} {Model} {Merging} {Toward} a {Generalist} {Vision}-{Language}-{Action} {Agent},2025,true,false,true,true,policy-learning,supporting,,,
fung_embodied_2025,Embodied {AI} {Agents}: {Modeling} the {World},2025,true,true,true,true,world-model-core,core,,,
gao_vla-os_2025,{VLA}-{OS}: {Structuring} and {Dissecting} {Planning} {Representations} and {Paradigms} in {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,planning-reasoning,supporting,,,
goyal_vla-0_2025,{VLA}-0: {Building} {State}-of-the-{Art} {VLAs} with {Zero} {Modification},2025,true,false,true,true,data-benchmark-eval,supporting,,,
grover_enhancing_2025,Enhancing {Generalization} in {Vision}-{Language}-{Action} {Models} by {Preserving} {Pretrained} {Representations},2025,true,false,true,true,data-benchmark-eval,supporting,,,
gu_manualvla_2025,{ManualVLA}: {A} {Unified} {VLA} {Model} for {Chain}-of-{Thought} {Manual} {Generation} and {Robotic} {Manipulation},2025,true,false,true,true,planning-reasoning,supporting,,,
guan_efficient_2025,Efficient {Vision}-{Language}-{Action} {Models} for {Embodied} {Manipulation}: {A} {Systematic} {Survey},2025,true,false,true,true,survey-meta,core,,,
guo_improving_2025,Improving {Vision}-{Language}-{Action} {Model} with {Online} {Reinforcement} {Learning},2025,true,false,true,true,data-benchmark-eval,supporting,,,
guo_omnivla_2025,{OmniVLA}: {Physically}-{Grounded} {Multimodal} {VLA} with {Unified} {Multi}-{Sensor} {Perception} for {Robotic} {Manipulation},2025,true,false,true,true,agent-architecture,supporting,,,
guo_vdrive_2025,{VDRive}: {Leveraging} {Reinforced} {VLA} and {Diffusion} {Policy} for {End}-to-end {Autonomous} {Driving},2025,true,false,true,true,data-benchmark-eval,supporting,,,
guo_vla-reasoner_2025,{VLA}-{Reasoner}: {Empowering} {Vision}-{Language}-{Action} {Models} with {Reasoning} via {Online} {Monte} {Carlo} {Tree} {Search},2025,true,true,true,true,world-model-core,core,,,
han_percept-wam_2025,Percept-{WAM}: {Perception}-{Enhanced} {World}-{Awareness}-{Action} {Model} for {Robust} {End}-to-{End} {Autonomous} {Driving},2025,true,false,true,true,data-benchmark-eval,supporting,,,
hancock_actions_2025,Actions as {Language}: {Fine}-{Tuning} {VLMs} into {VLAs} {Without} {Catastrophic} {Forgetting},2025,true,false,true,true,data-benchmark-eval,supporting,,,
hancock_run-time_2025,Run-time observation interventions make vision-language-action models more visually robust,2025,true,false,true,true,agent-architecture,supporting,,,
hannus_ia-vla_2025,{IA}-{VLA}: {Input} {Augmentation} for {Vision}-{Language}-{Action} models in settings with semantically complex tasks,2025,true,false,true,true,data-benchmark-eval,supporting,,,
hao_driveaction_2025,{DriveAction}: {A} {Benchmark} for {Exploring} {Human}-like {Driving} {Decisions} in {VLA} {Models},2025,true,false,true,true,data-benchmark-eval,supporting,,,
haon_mechanistic_2025,Mechanistic interpretability for steering vision-language-action models,2025,true,false,true,true,policy-learning,supporting,,,
hirose_omnivla_2025,{OmniVLA}: {An} {Omni}-{Modal} {Vision}-{Language}-{Action} {Model} for {Robot} {Navigation},2025,true,false,true,true,data-benchmark-eval,supporting,,,
hsieh_what_2025,Do what? {Teaching} vision-language-action models to reject the impossible,2025,true,false,true,true,agent-architecture,supporting,,,
hu_joint_2025,Joint {Optimization} of {Fine}-grained {Representation} and {Workflow} {Orchestration} in {Metaverse} {Articulated} {Manipulation} {Auto}-generation by {VLA} {Method},2025,true,false,true,true,planning-reasoning,supporting,,,
hu_sample-efficient_2025,Sample-{Efficient} {Robot} {Skill} {Learning} for {Construction} {Tasks}: {Benchmarking} {Hierarchical} {Reinforcement} {Learning} and {Vision}-{Language}-{Action} {VLA} {Model},2025,true,false,true,true,data-benchmark-eval,supporting,,,
huang_adapower_2025,{AdaPower}: {Specializing} {World} {Foundation} {Models} for {Predictive} {Manipulation},2025,true,true,true,true,data-benchmark-eval,core,,,
huang_graphcot-vla_2025,{GraphCoT}-{VLA}: {A} {3D} {Spatial}-{Aware} {Reasoning} {Vision}-{Language}-{Action} {Model} for {Robotic} {Manipulation} with {Ambiguous} {Instructions},2025,true,false,true,true,planning-reasoning,supporting,,,
huang_tactile-vla_2025,Tactile-{VLA}: {Unlocking} {Vision}-{Language}-{Action} {Model}'s {Physical} {Knowledge} for {Tactile} {Generalization},2025,true,false,true,true,planning-reasoning,supporting,,,
hung_nora-15_2025,{NORA}-1.5: {A} {Vision}-{Language}-{Action} {Model} {Trained} using {World} {Model}- and {Action}-based {Preference} {Rewards},2025,true,true,true,true,data-benchmark-eval,core,,,
hung_nora_2025,{NORA}: {A} {Small} {Open}-{Sourced} {Generalist} {Vision} {Language} {Action} {Model} for {Embodied} {Tasks},2025,true,false,true,true,planning-reasoning,supporting,,,
intelligence__05_2025,\$π\_\{0.5\}\$: a {Vision}-{Language}-{Action} {Model} with {Open}-{World} {Generalization},2025,true,false,true,true,agent-architecture,supporting,,,
intelligence__06_2025,\$π{\textasciicircum}\{*\}\_\{0.6\}\$: a {VLA} {That} {Learns} {From} {Experience},2025,true,false,true,true,policy-learning,supporting,,,
jabbour_dont_2025,Don't {Run} with {Scissors}: {Pruning} {Breaks} {VLA} {Models} but {They} {Can} {Be} {Recovered},2025,true,false,true,true,agent-architecture,supporting,,,
jang_contextvla_2025,{ContextVLA}: {Vision}-{Language}-{Action} {Model} with {Amortized} {Multi}-{Frame} {Context},2025,true,false,true,true,agent-architecture,supporting,,,
jiang_better_2025,"The {Better} {You} {Learn}, {The} {Smarter} {You} {Prune}: {Towards} {Efficient} {Vision}-language-action {Models} via {Differentiable} {Token} {Pruning}",2025,true,false,true,true,data-benchmark-eval,supporting,,,
jiang_galaxea_2025,Galaxea {Open}-{World} {Dataset} and {G0} {Dual}-{System} {VLA} {Model},2025,true,false,true,true,data-benchmark-eval,supporting,,,
jiang_irl-vla_2025,{IRL}-{VLA}: {Training} an {Vision}-{Language}-{Action} {Policy} via {Reward} {World} {Model},2025,true,true,true,true,data-benchmark-eval,core,,,
jiang_survey_2025,A {Survey} on {Vision}-{Language}-{Action} {Models} for {Autonomous} {Driving},2025,true,false,true,true,survey-meta,core,,,
jiang_wholebodyvla_2025,{WholeBodyVLA}: {Towards} {Unified} {Latent} {VLA} for {Whole}-{Body} {Loco}-{Manipulation} {Control},2025,true,false,true,true,data-benchmark-eval,supporting,,,
jin_dual-actor_2025,Dual-{Actor} {Fine}-{Tuning} of {VLA} {Models}: {A} {Talk}-and-{Tweak} {Human}-in-the-{Loop} {Approach},2025,true,false,true,true,data-benchmark-eval,supporting,,,
julg_refined_2025,Refined {Policy} {Distillation}: {From} {VLA} {Generalists} to {RL} {Experts},2025,true,false,true,true,data-benchmark-eval,supporting,,,
kachaev_dont_2025,Don't {Blind} {Your} {VLA}: {Aligning} {Visual} {Representations} for {OOD} {Generalization},2025,true,false,true,true,policy-learning,supporting,,,
khazatsky_droid_2025,{DROID}: {A} {Large}-{Scale} {In}-{The}-{Wild} {Robot} {Manipulation} {Dataset},2025,true,false,true,true,data-benchmark-eval,supporting,,,
kim_fine-tuning_2025,Fine-{Tuning} {Vision}-{Language}-{Action} {Models}: {Optimizing} {Speed} and {Success},2025,true,false,true,true,data-benchmark-eval,supporting,,,
koo_retovla_2025,{RetoVLA}: {Reusing} {Register} {Tokens} for {Spatial} {Reasoning} in {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,planning-reasoning,supporting,,,
leonardis_embodied_2025,Embodied {Understanding} of {Driving} {Scenarios},2025,true,false,true,true,foundation-definition,supporting,,,
li_3ds-vla_2025,3ds-vla: {A} 3d spatial-aware vision language action model for robust multi-task manipulation,2025,true,false,true,true,agent-architecture,supporting,,,
li_coa-vla_2025,Coa-vla: {Improving} vision-language-action models via visual-text chain-of-affordance,2025,true,false,true,true,agent-architecture,supporting,,,
li_cogvla_2025,{CogVLA}: {Cognition}-{Aligned} {Vision}-{Language}-{Action} {Model} via {Instruction}-{Driven} {Routing} \& {Sparsification},2025,true,false,true,true,data-benchmark-eval,supporting,,,
li_comprehensive_2025,A {Comprehensive} {Survey} on {World} {Models} for {Embodied} {AI},2025,true,true,true,true,survey-meta,core,,,
li_controlvla_2025,{ControlVLA}: {Few}-shot {Object}-centric {Adaptation} for {Pre}-trained {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,policy-learning,supporting,,,
li_drivevla-w0_2025,{DriveVLA}-{W0}: {World} {Models} {Amplify} {Data} {Scaling} {Law} in {Autonomous} {Driving},2025,true,true,true,true,data-benchmark-eval,core,,,
li_embodied_2025,Embodied {Multi}-{Agent} {Systems}: {A} {Review},2025,true,false,true,true,foundation-definition,supporting,,,
li_hamster_2025,{HAMSTER}: {Hierarchical} {Action} {Models} {For} {Open}-{World} {Robot} {Manipulation},2025,true,false,true,true,data-benchmark-eval,supporting,,,
li_jarvis-vla_2025,{JARVIS}-{VLA}: {Post}-{Training} {Large}-{Scale} {Vision} {Language} {Models} to {Play} {Visual} {Games} with {Keyboards} and {Mouse},2025,true,false,true,true,data-benchmark-eval,supporting,,,
li_map-vla_2025,{MAP}-{VLA}: {Memory}-{Augmented} {Prompting} for {Vision}-{Language}-{Action} {Model} in {Robotic} {Manipulation},2025,true,false,true,true,data-benchmark-eval,supporting,,,
li_mimicdreamer_2025,{MimicDreamer}: {Aligning} {Human} and {Robot} {Demonstrations} for {Scalable} {VLA} {Training},2025,true,false,true,true,policy-learning,supporting,,,
li_qdepth-vla_2025,{QDepth}-{VLA}: {Quantized} {Depth} {Prediction} as {Auxiliary} {Supervision} for {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,data-benchmark-eval,supporting,,,
li_robonurse-vla_2025,Robonurse-vla: {Robotic} scrub nurse system based on vision-language-action model,2025,true,false,true,true,agent-architecture,supporting,,,
li_scalable_2025,Scalable {Vision}-{Language}-{Action} {Model} {Pretraining} for {Robotic} {Manipulation} with {Real}-{Life} {Human} {Activity} {Videos},2025,true,false,true,true,data-benchmark-eval,supporting,,,
li_simplevla-rl_2025,{SimpleVLA}-{RL}: {Scaling} {VLA} {Training} via {Reinforcement} {Learning},2025,true,false,true,true,planning-reasoning,supporting,,,
li_spatial_2025,Spatial {Forcing}: {Implicit} {Spatial} {Representation} {Alignment} for {Vision}-language-action {Model},2025,true,false,true,true,data-benchmark-eval,supporting,,,
li_survey_2025,Survey of {Vision}-{Language}-{Action} {Models} for {Embodied} {Manipulation},2025,true,false,true,true,survey-meta,core,,,
li_switchvla_2025,{SwitchVLA}: {Execution}-{Aware} {Task} {Switching} for {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,planning-reasoning,supporting,,,
li_towards_2025,Towards {Deploying} {VLA} without {Fine}-{Tuning}: {Plug}-and-{Play} {Inference}-{Time} {VLA} {Policy} {Steering} via {Embodied} {Evolutionary} {Diffusion},2025,true,false,true,true,policy-learning,supporting,,,
li_urbanvla_2025,{UrbanVLA}: {A} {Vision}-{Language}-{Action} {Model} for {Urban} {Micromobility},2025,true,false,true,true,policy-learning,supporting,,,
li_vla-rft_2025,{VLA}-{RFT}: {Vision}-{Language}-{Action} {Reinforcement} {Fine}-tuning with {Verified} {Rewards} in {World} {Simulators},2025,true,true,true,true,world-model-core,core,,,
li_vla_2025,{VLA} {Models} {Are} {More} {Generalizable} {Than} {You} {Think}: {Revisiting} {Physical} and {Spatial} {Modeling},2025,true,false,true,true,policy-learning,supporting,,,
liang_discrete_2025,Discrete {Diffusion} {VLA}: {Bringing} {Discrete} {Diffusion} to {Action} {Decoding} in {Vision}-{Language}-{Action} {Policies},2025,true,false,true,true,data-benchmark-eval,supporting,,,
liang_large_2025,Large {Model} {Empowered} {Embodied} {AI}: {A} {Survey} on {Decision}-{Making} and {Embodied} {Learning},2025,true,true,true,true,survey-meta,core,,,
liang_pixelvla_2025,{PixelVLA}: {Advancing} {Pixel}-level {Understanding} in {Vision}-{Language}-{Action} {Model},2025,true,false,true,true,data-benchmark-eval,supporting,,,
liao_genie_2025,Genie {Envisioner}: {A} {Unified} {World} {Foundation} {Platform} for {Robotic} {Manipulation},2025,true,false,true,true,data-benchmark-eval,supporting,,,
lin_evo-0_2025,Evo-0: {Vision}-{Language}-{Action} {Model} with {Implicit} {Spatial} {Understanding},2025,true,false,true,true,data-benchmark-eval,supporting,,,
lin_evo-1_2025,Evo-1: {Lightweight} {Vision}-{Language}-{Action} {Model} with {Preserved} {Semantic} {Alignment},2025,true,false,true,true,data-benchmark-eval,supporting,,,
lin_hif-vla_2025,"{HiF}-{VLA}: {Hindsight}, {Insight} and {Foresight} through {Motion} {Representation} for {Vision}-{Language}-{Action} {Models}",2025,true,false,true,true,data-benchmark-eval,supporting,,,
lin_vote_2025,{VOTE}: {Vision}-{Language}-{Action} {Optimization} with {Trajectory} {Ensemble} {Voting},2025,true,false,true,true,agent-architecture,supporting,,,
liu_aligning_2025,Aligning {Cyber} {Space} with {Physical} {World}: {A} {Comprehensive} {Survey} on {Embodied} {AI},2025,true,true,true,true,survey-meta,core,,,
liu_eva-vla_2025,Eva-{VLA}: {Evaluating} {Vision}-{Language}-{Action} {Models}' {Robustness} {Under} {Real}-{World} {Physical} {Variations},2025,true,false,true,true,data-benchmark-eval,supporting,,,
liu_evovla_2025,{EvoVLA}: {Self}-{Evolving} {Vision}-{Language}-{Action} {Model},2025,true,false,true,true,data-benchmark-eval,supporting,,,
liu_hybridvla_2025,{HybridVLA}: {Collaborative} {Diffusion} and {Autoregression} in a {Unified} {Vision}-{Language}-{Action} {Model},2025,true,false,true,true,planning-reasoning,supporting,,,
liu_hybridvla_2025-1,{HybridVLA}: {Collaborative} {Autoregression} and {Diffusion} in a {Unified} {Vision}-{Language}-{Action} {Model},2025,true,false,true,true,agent-architecture,supporting,,,
liu_mla_2025,{MLA}: {A} {Multisensory} {Language}-{Action} {Model} for {Multimodal} {Understanding} and {Forecasting} in {Robotic} {Manipulation},2025,true,true,true,true,data-benchmark-eval,core,,,
liu_trackvla_2025,{TrackVLA}++: {Unleashing} {Reasoning} and {Memory} {Capabilities} in {VLA} {Models} for {Embodied} {Visual} {Tracking},2025,true,false,true,true,data-benchmark-eval,supporting,,,
liu_ttf-vla_2025,{TTF}-{VLA}: {Temporal} {Token} {Fusion} via {Pixel}-{Attention} {Integration} for {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,agent-architecture,supporting,,,
liu_vla-mark_2025,{VLA}-{Mark}: {A} cross modal watermark for large vision-language alignment models,2025,true,false,true,true,policy-learning,supporting,,,
lu_multimodal_2025,Multimodal {Data} {Storage} and {Retrieval} for {Embodied} {AI}: {A} {Survey},2025,true,false,true,true,survey-meta,core,,,
lu_vla-rl_2025,{VLA}-{RL}: {Towards} {Masterful} and {General} {Robotic} {Manipulation} with {Scalable} {Reinforcement} {Learning},2025,true,false,true,true,policy-learning,supporting,,,
lykov_cognitivedrone_2025,{CognitiveDrone}: {A} {VLA} {Model} and {Evaluation} {Benchmark} for {Real}-{Time} {Cognitive} {Task} {Solving} and {Reasoning} in {UAVs},2025,true,false,true,true,data-benchmark-eval,supporting,,,
neary_improving_2025,Improving {Pre}-{Trained} {Vision}-{Language}-{Action} {Policies} with {Model}-{Based} {Search},2025,true,false,true,true,planning-reasoning,supporting,,,
neau_grasp-vla_2025,{GraSP}-{VLA}: {Graph}-based {Symbolic} {Action} {Representation} for {Long}-{Horizon} {Planning} with {VLA} {Policies},2025,true,false,true,true,planning-reasoning,supporting,,,
nvidia_cosmos_2025,Cosmos {World} {Foundation} {Model} {Platform} for {Physical} {AI},2025,true,true,true,true,world-model-core,core,,,
nvidia_gr00t_2025,{GR00T} {N1}: {An} {Open} {Foundation} {Model} for {Generalist} {Humanoid} {Robots},2025,true,false,true,true,data-benchmark-eval,supporting,,,
park_acg_2025,{ACG}: {Action} {Coherence} {Guidance} for {Flow}-based {VLA} models,2025,true,false,true,true,policy-learning,supporting,,,
patratskiy_spatial_2025,Spatial {Traces}: {Enhancing} {VLA} {Models} with {Spatial}-{Temporal} {Understanding},2025,true,false,true,true,agent-architecture,supporting,,,
peng_counterfactual_2025,Counterfactual {VLA}: {Self}-{Reflective} {Vision}-{Language}-{Action} {Model} with {Adaptive} {Reasoning},2025,true,false,true,true,data-benchmark-eval,supporting,,,
pertsch_fast_2025,{FAST}: {Efficient} {Action} {Tokenization} for {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,agent-architecture,supporting,,,
pugacheva_bring_2025,"Bring the {Apple}, {Not} the {Sofa}: {Impact} of {Irrelevant} {Context} in {Embodied} {AI} {Commands} on {VLA} {Models}",2025,true,false,true,true,agent-architecture,supporting,,,
qian_wristworld_2025,{WristWorld}: {Generating} {Wrist}-{Views} via {4D} {World} {Models} for {Robotic} {Manipulation},2025,true,true,true,true,data-benchmark-eval,core,,,
qu_spatialvla_2025,{SpatialVLA}: {Exploring} {Spatial} {Representations} for {Visual}-{Language}-{Action} {Model},2025,true,false,true,true,data-benchmark-eval,supporting,,,
savov_exploration-driven_2025,Exploration-{Driven} {Generative} {Interactive} {Environments},2025,false,false,false,true,foundation-definition,supporting,not_embodied_related,,
sendai_leave_2025,Leave {No} {Observation} {Behind}: {Real}-time {Correction} for {VLA} {Action} {Chunks},2025,true,false,true,true,agent-architecture,supporting,,,
seong_vla-r_2025,{VLA}-{R}: {Vision}-{Language} {Action} {Retrieval} toward {Open}-{World} {End}-to-{End} {Autonomous} {Driving},2025,true,false,true,true,planning-reasoning,supporting,,,
serpiva_racevla_2025,{RaceVLA}: {VLA}-based {Racing} {Drone} {Navigation} with {Human}-like {Behaviour},2025,true,false,true,true,data-benchmark-eval,supporting,,,
shao_large_2025,Large {VLM}-based {Vision}-{Language}-{Action} {Models} for {Robotic} {Manipulation}: {A} {Survey},2025,true,true,true,true,survey-meta,core,,,
shukor_smolvla_2025,{SmolVLA}: {A} {Vision}-{Language}-{Action} {Model} for {Affordable} and {Efficient} {Robotics},2025,true,false,true,true,data-benchmark-eval,supporting,,,
singh_og-vla_2025,{OG}-{VLA}: {Orthographic} {Image} {Generation} for {3D}-{Aware} {Vision}-{Language} {Action} {Model},2025,true,false,true,true,data-benchmark-eval,supporting,,,
song_ceed-vla_2025,{CEED}-{VLA}: {Consistency} {Vision}-{Language}-{Action} {Model} with {Early}-{Exit} {Decoding},2025,true,false,true,true,planning-reasoning,supporting,,,
song_pd-vla_2025,{PD}-{VLA}: {Accelerating} {Vision}-{Language}-{Action} {Model} {Integrated} with {Action} {Chunking} via {Parallel} {Decoding},2025,true,false,true,true,agent-architecture,supporting,,,
song_rationalvla_2025,{RationalVLA}: {A} {Rational} {Vision}-{Language}-{Action} {Model} with {Dual} {System},2025,true,false,true,true,data-benchmark-eval,supporting,,,
song_reconvla_2025,{ReconVLA}: {Reconstructive} {Vision}-{Language}-{Action} {Model} as {Effective} {Robot} {Perceiver},2025,true,false,true,true,data-benchmark-eval,supporting,,,
sun_collabvla_2025,{CollabVLA}: {Self}-{Reflective} {Vision}-{Language}-{Action} {Model} {Dreaming} {Together} with {Human},2025,true,false,true,true,planning-reasoning,supporting,,,
sun_geovla_2025,{GeoVLA}: {Empowering} {3D} {Representations} in {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,data-benchmark-eval,supporting,,,
syed_expres-vla_2025,{ExpReS}-{VLA}: {Specializing} {Vision}-{Language}-{Action} {Models} {Through} {Experience} {Replay} and {Retrieval},2025,true,false,true,true,data-benchmark-eval,supporting,,,
tai_realmirror_2025,"{RealMirror}: {A} {Comprehensive}, {Open}-{Source} {Vision}-{Language}-{Action} {Platform} for {Embodied} {AI}",2025,true,false,true,true,survey-meta,supporting,,,
tan_interactive_2025,Interactive {Post}-{Training} for {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,agent-architecture,supporting,,,
tan_latent_2025,Latent {Chain}-of-{Thought} {World} {Modeling} for {End}-to-{End} {Driving},2025,true,true,true,true,data-benchmark-eval,core,,,
tarasov_nina_2025,{NinA}: {Normalizing} {Flows} in {Action}. {Training} {VLA} {Models} with {Normalizing} {Flows},2025,true,false,true,true,data-benchmark-eval,supporting,,,
team_gigabrain-0_2025,{GigaBrain}-0: {A} {World} {Model}-{Powered} {Vision}-{Language}-{Action} {Model},2025,true,true,true,true,world-model-core,core,,,
team_gigaworld-0_2025,{GigaWorld}-0: {World} {Models} as {Data} {Engine} to {Empower} {Embodied} {AI},2025,true,true,true,true,data-benchmark-eval,core,,,
tharwat_latent_2025,Latent {Action} {Pretraining} {Through} {World} {Modeling},2025,true,true,true,true,data-benchmark-eval,core,,,
turgunbaev_perception_2025,From {Perception} to {Action} with {Integrated} {VLA} {Systems},2025,true,false,true,true,agent-architecture,supporting,,,
valle_evaluating_2025,Evaluating {Uncertainty} and {Quality} of {Visual} {Language} {Action}-enabled {Robots},2025,true,false,true,true,data-benchmark-eval,supporting,,,
wan_worldagen_2025,{WorldAgen}: {Unified} {State}-{Action} {Prediction} with {Test}-{Time} {World} {Model} {Training},2025,true,true,true,true,world-model-core,core,,,
wang_exploring_2025,Exploring the adversarial vulnerabilities of vision-language-action models in robotics,2025,true,false,true,true,agent-architecture,supporting,,,
wang_genie_2025,Genie: {A} generalizable navigation system for in-the-wild environments,2025,true,false,true,true,foundation-definition,supporting,,,
wang_spec-vla_2025,Spec-vla: speculative decoding for vision-language-action models with relaxed acceptance,2025,true,false,true,true,agent-architecture,supporting,,,
wang_specprune-vla_2025,{SpecPrune}-{VLA}: {Accelerating} {Vision}-{Language}-{Action} {Models} via {Action}-{Aware} {Self}-{Speculative} {Pruning},2025,true,false,true,true,agent-architecture,supporting,,,
wang_unified_2025,Unified {Vision}-{Language}-{Action} {Model},2025,true,true,true,true,data-benchmark-eval,core,,,
wang_vla-adapter_2025,{VLA}-{Adapter}: {An} {Effective} {Paradigm} for {Tiny}-{Scale} {Vision}-{Language}-{Action} {Model},2025,true,false,true,true,data-benchmark-eval,supporting,,,
wang_vla_2025,{VLA} {Model} {Post}-{Training} via {Action}-{Chunked} {PPO} and {Self} {Behavior} {Cloning},2025,true,false,true,true,data-benchmark-eval,supporting,,,
wang_vlatest_2025,{VLATest}: {Testing} and {Evaluating} {Vision}-{Language}-{Action} {Models} for {Robotic} {Manipulation},2025,true,false,true,true,planning-reasoning,supporting,,,
wang_vq-vla_2025,{VQ}-{VLA}: {Improving} {Vision}-{Language}-{Action} {Models} via {Scaling} {Vector}-{Quantized} {Action} {Tokenizers},2025,true,false,true,true,data-benchmark-eval,supporting,,,
wei_audio-vla_2025,Audio-{VLA}: {Adding} {Contact} {Audio} {Perception} to {Vision}-{Language}-{Action} {Model} for {Robotic} {Manipulation},2025,true,false,true,true,data-benchmark-eval,supporting,,,
wen_dexvla_2025,{DexVLA}: {Vision}-{Language} {Model} with {Plug}-{In} {Diffusion} {Expert} for {General} {Robot} {Control},2025,true,false,true,true,agent-architecture,supporting,,,
wen_diffusionvla_2025,{DiffusionVLA}: {Scaling} {Robot} {Foundation} {Models} via {Unified} {Diffusion} and {Autoregression},2025,true,false,true,true,agent-architecture,supporting,,,
wen_dvla_2025,{dVLA}: {Diffusion} {Vision}-{Language}-{Action} {Model} with {Multimodal} {Chain}-of-{Thought},2025,true,false,true,true,data-benchmark-eval,supporting,,,
wen_llada-vla_2025,{LLaDA}-{VLA}: {Vision} {Language} {Diffusion} {Action} {Models},2025,true,false,true,true,agent-architecture,supporting,,,
wen_tinyvla_2025,"Tinyvla: {Towards} fast, data-efficient vision-language-action models for robotic manipulation",2025,true,false,true,true,agent-architecture,supporting,,,
won_dual-stream_2025,Dual-{Stream} {Diffusion} for {World}-{Model} {Augmented} {Vision}-{Language}-{Action} {Model},2025,true,false,true,true,data-benchmark-eval,supporting,,,
wu_momanipvla_2025,Momanipvla: {Transferring} vision-language-action models for general mobile manipulation,2025,true,false,true,true,agent-architecture,supporting,,,
xiang_vla_2025,{VLA} {Model}-{Expert} {Collaboration} for {Bi}-directional {Manipulation} {Learning},2025,true,false,true,true,agent-architecture,supporting,,,
xiao_world-env_2025,World-{Env}: {Leveraging} {World} {Model} as a {Virtual} {Environment} for {VLA} {Post}-{Training},2025,true,true,true,true,data-benchmark-eval,core,,,
xiong_hypervla_2025,{HyperVLA}: {Efficient} {Inference} in {Vision}-{Language}-{Action} {Models} via {Hypernetworks},2025,true,false,true,true,agent-architecture,supporting,,,
xu_model-agnostic_2025,Model-agnostic {Adversarial} {Attack} and {Defense} for {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,data-benchmark-eval,supporting,,,
xu_stare-vla_2025,{STARE}-{VLA}: {Progressive} {Stage}-{Aware} {Reinforcement} for {Fine}-{Tuning} {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,policy-learning,supporting,,,
xu_vla-cache_2025,{VLA}-{Cache}: {Efficient} {Vision}-{Language}-{Action} {Manipulation} via {Adaptive} {Token} {Caching},2025,true,false,true,true,planning-reasoning,supporting,,,
xu_wam-diff_2025,{WAM}-{Diff}: {A} {Masked} {Diffusion} {VLA} {Framework} with {MoE} and {Online} {Reinforcement} {Learning} for {Autonomous} {Driving},2025,true,false,true,true,planning-reasoning,supporting,,,
xue_leverb_2025,{LeVERB}: {Humanoid} {Whole}-{Body} {Control} with {Latent} {Vision}-{Language} {Instruction},2025,true,false,true,true,data-benchmark-eval,supporting,,,
yan_when_2025,When {Alignment} {Fails}: {Multimodal} {Adversarial} {Attacks} on {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,data-benchmark-eval,supporting,,,
yang_beyond_2025,Beyond {Human} {Demonstrations}: {Diffusion}-{Based} {Reinforcement} {Learning} to {Generate} {Data} for {VLA} {Training},2025,true,false,true,true,data-benchmark-eval,supporting,,,
yang_efficientvla_2025,{EfficientVLA}: {Training}-{Free} {Acceleration} and {Compression} for {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,data-benchmark-eval,supporting,,,
yang_fpc-vla_2025,{FPC}-{VLA}: {A} {Vision}-{Language}-{Action} {Framework} with a {Supervisor} for {Failure} {Prediction} and {Correction},2025,true,false,true,true,data-benchmark-eval,supporting,,,
ye_learning_2025,Learning to {Feel} the {Future}: {DreamTacVLA} for {Contact}-{Rich} {Manipulation},2025,true,true,true,true,data-benchmark-eval,core,,,
ye_vla-r1_2025,{VLA}-{R1}: {Enhancing} {Reasoning} in {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,data-benchmark-eval,supporting,,,
yin_deepthinkvla_2025,{DeepThinkVLA}: {Enhancing} {Reasoning} {Capability} of {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,data-benchmark-eval,supporting,,,
yu_forcevla_2025,{ForceVLA}: {Enhancing} {VLA} {Models} with a {Force}-aware {MoE} for {Contact}-rich {Manipulation},2025,true,false,true,true,data-benchmark-eval,supporting,,,
yuan_autodrive-r2_2025,{AutoDrive}-{R}\${\textasciicircum}2\$: {Incentivizing} {Reasoning} and {Self}-{Reflection} {Capacity} for {VLA} {Model} in {Autonomous} {Driving},2025,true,false,true,true,data-benchmark-eval,supporting,,,
yuan_depthvla_2025,{DepthVLA}: {Enhancing} {Vision}-{Language}-{Action} {Models} with {Depth}-{Aware} {Spatial} {Reasoning},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zang_rlinf-vla_2025,{RLinf}-{VLA}: {A} {Unified} and {Efficient} {Framework} for {VLA}+{RL} {Training},2025,true,false,true,true,planning-reasoning,supporting,,,
zhai_vision-language-action-critic_2025,A {Vision}-{Language}-{Action}-{Critic} {Model} for {Robotic} {Real}-{World} {Reinforcement} {Learning},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zhang_4d-vla_2025,{4D}-{VLA}: {Spatiotemporal} {Vision}-{Language}-{Action} {Pretraining} with {Cross}-{Scene} {Calibration},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zhang_align-then-steer_2025,Align-{Then}-{stEer}: {Adapting} the {Vision}-{Language} {Action} {Models} through {Unified} {Latent} {Guidance},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zhang_balancing_2025,Balancing {Signal} and {Variance}: {Adaptive} {Offline} {RL} {Post}-{Training} for {VLA} {Flow} {Models},2025,true,false,true,true,policy-learning,supporting,,,
zhang_dreamvla_2025,{DreamVLA}: {A} {Vision}-{Language}-{Action} {Model} {Dreamed} with {Comprehensive} {World} {Knowledge},2025,true,true,true,true,survey-meta,core,,,
zhang_inspire_2025,{InSpire}: {Vision}-{Language}-{Action} {Models} with {Intrinsic} {Spatial} {Reasoning},2025,true,false,true,true,planning-reasoning,supporting,,,
zhang_iref-vla_2025,{IRef}-{VLA}: {A} {Benchmark} for {Interactive} {Referential} {Grounding} with {Imperfect} {Language} in {3D} {Scenes},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zhang_mole-vla_2025,{MoLe}-{VLA}: {Dynamic} {Layer}-skipping {Vision} {Language} {Action} {Model} via {Mixture}-of-{Layers} for {Efficient} {Robot} {Manipulation},2025,true,false,true,true,planning-reasoning,supporting,,,
zhang_reasoning-vla_2025,Reasoning-{VLA}: {A} {Fast} and {General} {Vision}-{Language}-{Action} {Reasoning} {Model} for {Autonomous} {Driving},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zhang_reinbot_2025,{ReinboT}: {Amplifying} {Robot} {Visual}-{Language} {Manipulation} with {Reinforcement} {Learning},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zhang_robustvla_2025,{RobustVLA}: {Robustness}-{Aware} {Reinforcement} {Post}-{Training} for {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,policy-learning,supporting,,,
zhang_safevla_2025,{SafeVLA}: {Towards} {Safety} {Alignment} of {Vision}-{Language}-{Action} {Model} via {Constrained} {Learning},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zhang_step_2025,A {Step} {Toward} {World} {Models}: {A} {Survey} on {Robotic} {Manipulation},2025,true,true,true,true,survey-meta,core,,,
zhang_ta-vla_2025,{TA}-{VLA}: {Elucidating} the {Design} {Space} of {Torque}-aware {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zhang_up-vla_2025,{UP}-{VLA}: {A} {Unified} {Understanding} and {Prediction} {Model} for {Embodied} {Agent},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zhang_vtla_2025,{VTLA}: {Vision}-{Tactile}-{Language}-{Action} {Model} with {Preference} {Learning} for {Insertion} {Manipulation},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zhao_cot-vla_2025,Cot-vla: {Visual} chain-of-thought reasoning for vision-language-action models,2025,true,false,true,true,planning-reasoning,supporting,,,
zhao_more_2025,{MoRE}: {Unlocking} {Scalability} in {Reinforcement} {Learning} for {Quadruped} {Vision}-{Language}-{Action} {Models},2025,true,false,true,true,planning-reasoning,supporting,,,
zhao_vla2_2025,{VLA}{\textasciicircum}2: {Empowering} {Vision}-{Language}-{Action} {Models} with an {Agentic} {Framework} for {Unseen} {Concept} {Manipulation},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zheng_jarvis_2025,{JARVIS}: {A} {Neuro}-{Symbolic} {Commonsense} {Reasoning} {Framework} for {Conversational} {Embodied} {Agents},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zheng_x-vla_2025,X-{VLA}: {Soft}-{Prompted} {Transformer} as {Scalable} {Cross}-{Embodiment} {Vision}-{Language}-{Action} {Model},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zhong_flowvla_2025,{FlowVLA}: {Visual} {Chain} of {Thought}-based {Motion} {Reasoning} for {Vision}-{Language}-{Action} {Models},2025,true,true,true,true,data-benchmark-eval,core,,,
zhong_survey_2025,A {Survey} on {Vision}-{Language}-{Action} {Models}: {An} {Action} {Tokenization} {Perspective},2025,true,false,true,true,survey-meta,core,,,
zhou_chatvla-2_2025,{ChatVLA}-2: {Vision}-{Language}-{Action} {Model} with {Open}-{World} {Embodied} {Reasoning} from {Pretrained} {Knowledge},2025,true,false,true,true,planning-reasoning,supporting,,,
zhou_chatvla_2025,{ChatVLA}: {Unified} {Multimodal} {Understanding} and {Robot} {Control} with {Vision}-{Language}-{Action} {Model},2025,true,false,true,true,data-benchmark-eval,supporting,,,
zhu_objectvla_2025,{ObjectVLA}: {End}-to-{End} {Open}-{World} {Object} {Manipulation} {Without} {Demonstration},2025,true,false,true,true,policy-learning,supporting,,,
zhu_wmpo_2025,{WMPO}: {World} {Model}-based {Policy} {Optimization} for {Vision}-{Language}-{Action} {Models},2025,true,true,true,true,world-model-core,core,,,
black__0_2026,\$π\_0\$: {A} {Vision}-{Language}-{Action} {Flow} {Model} for {General} {Robot} {Control},2026,true,false,true,true,data-benchmark-eval,supporting,,,
cai_internvla-a1_2026,"{InternVLA}-{A1}: {Unifying} {Understanding}, {Generation} and {Action} for {Robotic} {Manipulation}",2026,true,true,true,true,data-benchmark-eval,core,,,
chen_bridgev2w_2026,{BridgeV2W}: {Bridging} {Video} {Generation} {Models} to {Embodied} {World} {Models} via {Embodiment} {Masks},2026,true,true,true,true,data-benchmark-eval,core,,,
chen_combatvla_2026,{CombatVLA}: {An} {Efficient} {Vision}-{Language}-{Action} {Model} for {Combat} {Tasks} in {3D} {Action} {Role}-{Playing} {Games},2026,true,false,true,true,data-benchmark-eval,supporting,,,
fan_wow_2026,"Wow, wo, val! {A} {Comprehensive} {Embodied} {World} {Model} {Evaluation} {Turing} {Test}",2026,true,true,true,true,survey-meta,core,,,
guo_flowdreamer_2026,Flowdreamer: {A} rgb-d world model with flow-based motion representations for robot manipulation,2026,true,true,true,true,world-model-core,core,,,
hu_vision-language-action_2026,"Vision-{Language}-{Action} {Models} for {Autonomous} {Driving}: {Past}, {Present}, and {Future}",2026,true,false,true,true,data-benchmark-eval,supporting,,,
jian_pi-vla_2026,{PI}-{VLA}: {Adaptive} {Symmetry}-{Aware} {Decision}-{Making} for {Long}-{Horizon} {Vision}–{Language}–{Action} {Manipulation},2026,true,true,true,true,data-benchmark-eval,core,,,
li_pointvla_2026,Pointvla: {Injecting} the 3d world into vision-language-action models,2026,true,false,true,true,agent-architecture,supporting,,,
li_reflection-based_2026,Reflection-{Based} {Task} {Adaptation} for {Self}-{Improving} {VLA},2026,true,false,true,true,planning-reasoning,supporting,,,
lillemark_flow_2026,Flow {Equivariant} {World} {Models}: {Memory} for {Partially} {Observed} {Dynamic} {Environments},2026,true,true,true,true,data-benchmark-eval,core,,,
liu_--fly_2026,On-the-{Fly} {VLA} {Adaptation} via {Test}-{Time} {Reinforcement} {Learning},2026,true,false,true,true,policy-learning,supporting,,,
liu_what_2026,What {Can} {RL} {Bring} to {VLA} {Generalization}? {An} {Empirical} {Study},2026,true,false,true,true,data-benchmark-eval,supporting,,,
magne_nitrogen_2026,{NitroGen}: {An} {Open} {Foundation} {Model} for {Generalist} {Gaming} {Agents},2026,true,false,true,true,data-benchmark-eval,supporting,,,
mei_video_2026,"Video {Generation} {Models} in {Robotics} -- {Applications}, {Research} {Challenges}, {Future} {Directions}",2026,true,true,true,true,data-benchmark-eval,core,,,
peng_reworld_2026,{ReWorld}: {Multi}-{Dimensional} {Reward} {Modeling} for {Embodied} {World} {Models},2026,true,true,true,true,data-benchmark-eval,core,,,
ren_aligning_2026,Aligning {Agentic} {World} {Models} via {Knowledgeable} {Experience} {Learning},2026,true,true,true,true,data-benchmark-eval,core,,,
sapkota_vision-language-action_2026,"Vision-{Language}-{Action} ({VLA}) {Models}: {Concepts}, {Progress}, {Applications} and {Challenges}",2026,true,false,true,true,planning-reasoning,supporting,,,
shah_learning_2026,Learning {Action}-{Conditioned} {World} {Models} for {Cataract} {Surgery} from {Unlabeled} {Videos},2026,true,true,true,true,world-model-core,core,,,
shen_efficient_2026,An {Efficient} and {Multi}-{Modal} {Navigation} {System} with {One}-{Step} {World} {Model},2026,true,true,true,true,world-model-core,core,,,
upadhyay_worldbench_2026,{WorldBench}: {Disambiguating} {Physics} for {Diagnostic} {Evaluation} of {World} {Models},2026,true,true,true,true,data-benchmark-eval,core,,,
wang_mechanistic_2026,A {Mechanistic} {View} on {Video} {Generation} as {World} {Models}: {State} and {Dynamics},2026,true,true,true,true,data-benchmark-eval,core,,,
wu_pragmatic_2026,A {Pragmatic} {VLA} {Foundation} {Model},2026,true,false,true,true,data-benchmark-eval,supporting,,,
wu_visual_2026,Visual {Generation} {Unlocks} {Human}-{Like} {Reasoning} through {Multimodal} {World} {Models},2026,true,true,true,true,data-benchmark-eval,core,,,
wu_what_2026,Do {What} {You} {Say}: {Steering} {Vision}-{Language}-{Action} {Models} via {Runtime} {Reasoning}-{Action} {Alignment} {Verification},2026,true,false,true,true,data-benchmark-eval,supporting,,,
xiang_parallels_2026,"Parallels {Between} {VLA} {Model} {Post}-{Training} and {Human} {Motor} {Learning}: {Progress}, {Challenges}, and {Trends}",2026,true,false,true,true,data-benchmark-eval,supporting,,,
xie_dynamicvla_2026,{DynamicVLA}: {A} {Vision}-{Language}-{Action} {Model} for {Dynamic} {Object} {Manipulation},2026,true,false,true,true,data-benchmark-eval,supporting,,,
xie_latentvla_2026,{LatentVLA}: {Efficient} {Vision}-{Language} {Models} for {Autonomous} {Driving} via {Latent} {Action} {Prediction},2026,true,false,true,true,data-benchmark-eval,supporting,,,
yang_vlaser_2026,Vlaser: {Vision}-{Language}-{Action} {Model} with {Synergistic} {Embodied} {Reasoning},2026,true,false,true,true,data-benchmark-eval,supporting,,,
ye_dream-vl_2026,Dream-{VL} \& {Dream}-{VLA}: {Open} {Vision}-{Language} and {Vision}-{Language}-{Action} {Models} with {Diffusion} {Language} {Model} {Backbone},2026,true,false,true,true,data-benchmark-eval,supporting,,,
yin_genie_2026,Genie {Sim} 3.0 : {A} {High}-{Fidelity} {Comprehensive} {Simulation} {Platform} for {Humanoid} {Robot},2026,true,false,true,true,survey-meta,supporting,,,
yu_ac2-vla_2026,{AC}{\textasciicircum}2-{VLA}: {Action}-{Context}-{Aware} {Adaptive} {Computation} in {Vision}-{Language}-{Action} {Models} for {Efficient} {Robotic} {Manipulation},2026,true,false,true,true,data-benchmark-eval,supporting,,,
yu_survey_2026,A {Survey} on {Efficient} {Vision}-{Language}-{Action} {Models},2026,true,false,true,true,survey-meta,core,,,
zhang_compliantvla-adaptor_2026,{CompliantVLA}-adaptor: {VLM}-{Guided} {Variable} {Impedance} {Action} for {Safe} {Contact}-{Rich} {Manipulation},2026,true,false,true,true,data-benchmark-eval,supporting,,,
zhang_vlm4vla_2026,{VLM4VLA}: {Revisiting} {Vision}-{Language}-{Models} in {Vision}-{Language}-{Action} {Models},2026,true,false,true,true,data-benchmark-eval,supporting,,,
zhong_acot-vla_2026,{ACoT}-{VLA}: {Action} {Chain}-of-{Thought} for {Vision}-{Language}-{Action} {Models},2026,true,false,true,true,planning-reasoning,supporting,,,
zhou_digital_2026,Digital {Twin} {AI}: {Opportunities} and {Challenges} from {Large} {Language} {Models} to {World} {Models},2026,true,true,true,true,world-model-core,core,,,
