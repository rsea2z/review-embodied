# The Dawn of Embodied Intelligence: A Deep Dive into World Model Datasets and Training Paradigms in 2025-2026

## Introduction: Towards the Inner Simulator of Embodied Intelligence

Embodied Artificial Intelligence (Embodied AI) represents a profound paradigm shift in the pursuit of Artificial General Intelligence (AGI). It posits that true intelligence cannot arise from processing static data alone, but must be grounded in active interaction with the world through a physical or virtual body [[34](https://mn.cs.tsinghua.edu.cn/xinwang/PDF/papers/2025_Embodied%20AI%20from%20LLMs%20to%20World%20Models.pdf)]. At the core of this paradigm lies a critical component: the world model. A world model functions as an internal simulator, enabling an agent to understand, predict, and reason about the dynamics of its environment [[31](https://arxiv.org/abs/2510.16732)]. By learning to anticipate how its actions will reshape future states, an embodied agent can plan, make decisions, and adapt to novel situations far more effectively than through purely reactive policies. This capability to perform "what-if" or counterfactual reasoning is what distinguishes a truly intelligent agent from a simple stimulus-response machine. As we move through 2025 and into 2026, the field of embodied intelligence is converging on a central insight: scaling world models in both data and architectural sophistication is the most promising path toward more robust, adaptive, and general-purpose AI systems [[35](https://www.oreateai.com/blog/the-ai-revolution-of-2025-embodied-intelligence-and-world-models-take-center-stage/8636c0975d48b1b4d99c2781f2ece5fa)]. This report provides a comprehensive review of the latest advancements in this critical area, focusing on the datasets that are fueling this progress and the novel training methods that are shaping the capabilities of next-generation world models. We will dissect the major sources of data, from large-scale robotic manipulation to egocentric video, and analyze the training paradigms, including the innovative use of reinforcement learning with verifiable rewards, that are pushing the boundaries of what these models can achieve.

The evolution of world models has been remarkable. Initially conceived as compact dynamics functions within reinforcement learning loops, they have since blossomed into rich, high-dimensional generative models capable of simulating complex visual and temporal phenomena [[4](https://medium.com/@unverciftci/world-models-in-2025-from-code-to-imagination-2ab362a2c0df)]. The ultimate goal of a world model extends beyond simply capturing the statistical distribution of data; it is to provide an accurate, physics-consistent, and task-relevant simulation of the environment that can be used for downstream decision-making [[0](https://arxiv.org/html/2505.13934v2)]. This requires a fundamental rethinking of both the data used to train these models and the objectives used to optimize them. The traditional approach of training via surrogate objectives like Maximum Likelihood Estimation (MLE), while effective for building foundational probabilistic models, often misaligns with the ultimate task-specific goals, such as high-accuracy prediction or perceptual fidelity [[0](https://arxiv.org/html/2505.13934v2)]. This misalignment has led to a surge of research into new training paradigms that directly optimize for the metrics that matter, as well as a concerted effort to build larger, more diverse, and more physically meaningful datasets. The year 2025 has become a pivotal moment, marked by the release of massive robotic datasets and the emergence of training frameworks that treat world model development not just as a supervised learning problem, but as an active, goal-driven optimization process. This report will navigate this landscape, synthesizing the key findings from recent top-tier conference papers and preprints to provide a clear picture of the state-of-the-art and the open challenges that lie ahead in the quest to build truly intelligent embodied agents.

## The Data Engine: Fueling World Models with Large-Scale, Diverse Experiences

The performance and capabilities of a world model are fundamentally constrained by the quality, scale, and diversity of the data it is trained on. A model can only learn to simulate dynamics it has witnessed, and its generalization is directly tied to the breadth of experiences encoded in its training set. Recognizing this, the field has seen a monumental push towards creating and curating massive datasets that capture the rich complexity of the real world. These datasets are no longer small, task-specific collections but are evolving into large-scale, multi-modal, and multi-scenario repositories designed to be the "ImageNet moment" for embodied AI [[10](https://github.com/OpenDriveLab/AgiBot-World)]. The current landscape of world model datasets can be broadly categorized into two major domains: large-scale, in-the-wild robotic manipulation datasets and massive egocentric video collections. Each domain offers unique insights and poses distinct challenges, together providing a comprehensive foundation for training world models that can understand both fine-grained object interactions and broad environmental navigation. The construction of these datasets involves sophisticated data collection strategies, from adversarial data collection to enhance information density, to meticulous annotation and cleaning pipelines to ensure data quality, reflecting a growing understanding that data curation is as critical as model architecture in the pursuit of powerful world models.

The first major pillar of data for world models comes from large-scale robotic manipulation platforms. These datasets aim to capture the physical interactions between robots and objects, providing the ground truth for learning causality, dynamics, and the effects of actions. A landmark example in 2025 is the **AgiBot World Colosseo** platform. This is not merely a dataset but a full-stack ecosystem designed to advance bimanual manipulation in scalable and intelligent embodied systems [[10](https://github.com/OpenDriveLab/AgiBot-World)]. The scale of this initiative is unprecedented, comprising over **1,000,000 real-world robot manipulation trajectories** collected by 100 robots across more than 100 1:1 replicated real-life scenarios spanning five target domains [[80](https://arxiv.org/abs/2503.06669)]. The dataset is released in two tiers: AgiBot World Alpha, a curated subset with 92,214 trajectories, and the massive AgiBot World Beta, containing the full corpus of data [[10](https://github.com/OpenDriveLab/AgiBot-World)]. The construction of this dataset was driven by a key insight: diversity is paramount for scalable robotic manipulation [[89](https://scholar.google.com/citations?user=hZtOnecAAAAJ&hl=en)]. To maximize this, the project employed an "Adversarial Data Collection" (ADC) approach. This method involves introducing dynamic disturbances during data collection, which significantly enhances the information density and diversity of each trajectory. The primary benefit of ADC is that it strengthens a model's generalization capabilities and robustness, while simultaneously reducing post-training data requirements and model training costs by presenting a wider array of edge cases and perturbations [[11](https://huggingface.co/datasets/agibot-world/AgiBotWorldChallenge-2025)]. The AgiBot World dataset is accompanied by foundation models like GO-1, pretrained on this massive corpus, establishing a complete pipeline from data to model and benchmark, and is poised to become a cornerstone resource for the community [[10](https://github.com/OpenDriveLab/AgiBot-World)].

Another cornerstone dataset in this category is **DROID (Distributed Robot Interaction Dataset)**. While originally introduced in late 2023, its influence and usage have continued to grow significantly into 2025, making it a critical component of the current world model landscape [[51](https://arxiv.org/abs/2403.12945)]. DROID was created to address the need for diverse, "in-the-wild" robot manipulation data, moving beyond constrained laboratory environments. It comprises **76,000 demonstration trajectories**, equating to **350 hours of interaction data**, collected across **564 unique scenes** and **86 different tasks** by 50 data collectors worldwide [[50](https://droid-dataset.github.io)]. The dataset's value lies in its broad coverage of environments, camera viewpoints, and object categories, which has been proven to be highly effective for training robust and generalizable robot policies [[56](https://www.emergentmind.com/topics/droid-a-large-scale-in-the-wild-robot-manipulation-dataset)]. In 2025, DROID has served as the primary training ground for advanced controllable world models like **Ctrl-World** [[12](https://arxiv.org/html/2510.10125v1)]. The Ctrl-World model, for instance, was explicitly trained on the DROID dataset (specifically noting 95k trajectories, likely including an updated or filtered subset) to develop its ability to generate spatially and temporally consistent multi-view predictions for robot manipulation [[55](https://openreview.net/forum?id=748bHL2BAv)]. The dataset's structure, which includes multi-view camera images, robot proprioceptive data, and actions, provides the necessary multi-modal inputs for training sophisticated world models that need to understand both the global context and the fine-grained details of manipulation tasks. Similarly, **BridgeData V2**, another large-scale dataset released in 2023, remains a vital resource in 2025. It contains **50,365 demonstrations** of 13 skills across 24 environments, offering more than seven times the data of its predecessor [[106](https://proceedings.mlr.press/v229/walke23a/walke23a.pdf)]. BridgeData V2 continues to be used for pretraining world models and latent action representations, demonstrating its enduring utility for scalable robot learning research [[110](https://arxiv.org/pdf/2509.18428)].

| Dataset Name      | Year Introduced     | Primary Domain       | Data Volume & Scope                                          | Data Collection & Construction Methodology                   | Key Reference                                                |
| :---------------- | :------------------ | :------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| **AgiBot World**  | 2025                | Robotic Manipulation | 1,000,000+ trajectories across 217 tasks and 5 domains.      | Collected from 100 robots in 100+ replicated real-world scenarios using an Adversarial Data Collection (ADC) strategy to increase diversity and robustness. | [[80](https://arxiv.org/abs/2503.06669)], [[10](https://github.com/OpenDriveLab/AgiBot-World)] |
| **DROID**         | 2024 (Used in 2025) | Robotic Manipulation | 76,000 trajectories (350 hours) across 564 scenes and 86 tasks. | "In-the-wild" data collection by 50 data collectors across North America, Asia, and Europe, focusing on diverse environments and tasks. | [[51](https://arxiv.org/abs/2403.12945)], [[50](https://droid-dataset.github.io)] |
| **BridgeData V2** | 2023 (Used in 2025) | Robotic Manipulation | 50,365 demonstrations of 13 skills across 24 environments.   | Large-scale teleoperated data collection designed for scalable robot learning, focusing on a diverse set of manipulation behaviors. | [[106](https://proceedings.mlr.press/v229/walke23a/walke23a.pdf)] |
| **Ego4D**         | 2021 (Used in 2025) | Egocentric Video     | 3,670 hours of daily-life activity video across 74 locations. | Large-scale collection of first-person videos, providing a broad understanding of human activities and environmental dynamics. | [[67](https://www.computer.org/csdl/journal/tp/2025/11/10611736/1YTJvDcduIE)] |
| **EgoVid-5M**     | 2024 (Used in 2025) | Egocentric Video     | 5 million curated egocentric video clips.                    | Meticulously curated from source Ego4D videos, with detailed action annotations (kinematic control and text descriptions) and a specialized data cleansing pipeline for egocentric scenarios. | [[72](https://arxiv.org/abs/2411.08380)], [[70](https://github.com/JeffWang987/EgoVid)] |

The second major pillar of data comes from massive egocentric video datasets, which are crucial for training world models focused on navigation, human-scene interaction, and understanding broader environmental dynamics. While **Ego4D** was introduced in 2021, its scale and scope have made it an indispensable resource for world model research in 2025 [[67](https://www.computer.org/csdl/journal/tp/2025/11/10611736/1YTJvDcduIE)]. Ego4D comprises a staggering **3,670 hours** of daily-life egocentric video captured across 74 diverse locations worldwide [[61](https://arxiv.org/html/2412.03572v2)]. Its sheer scale provides a rich source of information about how humans interact with environments, how scenes change over time, and the dynamics of navigation. In 2025, world models like the **Navigation World Model (NWM)** have leveraged Ego4D to improve their performance in unseen environments. By training on the vast, unlabeled, and action-reward-free video data from Ego4D, NWM can learn powerful visual priors that enable it to imagine plausible trajectories from a single input image, demonstrating the value of large-scale, diverse video data for building generalizable world simulators [[19](https://openaccess.thecvf.com/content/CVPR2025/papers/Bar_Navigation_World_Models_CVPR_2025_paper.pdf)]. Building upon the foundation of Ego4D, a new dataset specifically tailored for egocentric video generation, **EgoVid-5M**, was introduced in late 2024 and has become prominent in 2025 [[72](https://arxiv.org/abs/2411.08380)]. EgoVid-5M addresses the need for high-quality, action-conditioned video data. It consists of **5 million meticulously curated egocentric video clips**, sourced from Ego4D, but enriched with detailed action annotations [[70](https://github.com/JeffWang987/EgoVid)]. These annotations are multi-layered, including both low-level kinematic control (e.g., ego-view translation and rotation parameters) and high-level textual descriptions of actions. Furthermore, the creators of EgoVid-5M implemented a specialized data cleansing pipeline designed for egocentric scenarios, focusing on ensuring frame consistency, action coherence, and motion smoothness. This emphasis on data quality, driven by the understanding that data quality significantly influences the effectiveness of generative models, represents a maturation in dataset construction methodology [[70](https://github.com/JeffWang987/EgoVid)]. Together, these robotic and egocentric datasets form a powerful and complementary data engine, providing the raw material necessary to train world models that can both understand the physics of object manipulation and navigate the complexities of the broader world.

## Sculpting Reality: Advanced Training Paradigms for World Models

Possessing a massive, diverse dataset is only the first step; the methodology used to train a world model on this data is equally, if not more, critical in determining its ultimate capabilities. The traditional paradigm of training generative models, including world models, has relied heavily on surrogate objectives like Maximum Likelihood Estimation (MLE) or variants thereof. While these methods are effective for learning the general statistical properties of a dataset, they suffer from a fundamental misalignment with the ultimate goals of a world model. The true objective is not just to model a data distribution, but to create an accurate simulator that excels at task-specific metrics such as high-fidelity prediction, long-horizon consistency, and physical plausibility [[0](https://arxiv.org/html/2505.13934v2)]. Standard MLE objectives, for instance, are known to produce blurry predictions in video models and can lead to issues like repetition and hallucination in language models [[0](https://arxiv.org/html/2505.13934v2)]. The year 2025 has seen a significant shift towards addressing this misalignment, with the emergence of novel training paradigms that directly optimize for desired performance metrics. Chief among these is the use of Reinforcement Learning with Verifiable Rewards (RLVR), which reframes world model training as an active optimization problem. Alongside this, architectural innovations like Conditional Diffusion Transformers and object-centric models are shaping how world models represent and simulate the world, moving beyond passive prediction towards active, controllable, and structured simulation.

A revolutionary development in 2025 is the application of **Reinforcement Learning with Verifiable Rewards (RLVR)** to world model training, most notably exemplified by the **RLVR-World** framework [[0](https://arxiv.org/html/2505.13934v2)]. The core insight of RLVR-World is that the ultimate objective of a world model is to produce accurate predictions, and accuracy is a verifiable metric that can serve as a reward signal. Instead of relying on a surrogate loss like MLE, RLVR-World formulates world model training as a reinforcement learning problem where the reward is computed by directly comparing the model's generated predictions against the ground truth. This approach elegantly bypasses the misalignment problem inherent in MLE. The framework first unifies world modeling across different modalities (language, video) into a general autoregressive sequence modeling formulation. States and actions are tokenized, and the model is tasked with predicting the next state token sequence [[0](https://arxiv.org/html/2505.13934v2)]. After an initial pre-training phase using MLE, the model undergoes a crucial post-training phase using RLVR. In this phase, given an input (current state and action), the model generates a group of candidate predictions. Each prediction is then decoded (e.g., into a text state or a video frame), and a reward is calculated based on a task-specific, verifiable metric. For language-based world models, this reward can be simple prediction accuracy. For video-based world models, it can be a perceptual metric like LPIPS (Learned Perceptual Image Patch Similarity), which measures perceptual similarity between images [[0](https://arxiv.org/html/2505.13934v2)]. This reward is then used to update the model's parameters via a reinforcement learning algorithm, specifically Group Relative Policy Optimization (GRPO), a variant of PPO that estimates advantages relative to a group of sampled responses. The results are striking. RLVR-World achieved significant gains, such as a **+30.7% improvement in accuracy** on text-based game state prediction and a **+9.2% relative improvement on LPIPS** for robot manipulation trajectory prediction, often with just a few hundred RLVR gradient steps, compared to the hundreds of thousands required for MLE training to achieve similar gains [[0](https://arxiv.org/html/2505.13934v2)]. This paradigm shift represents a move from passive data fitting to active, goal-directed model sculpting, ensuring the world model is optimized for its intended purpose: accurate simulation.

Beyond the RLVR paradigm, significant architectural innovations are shaping the capabilities of world models, particularly in the areas of controllability and structured representation. **Ctrl-World** is a prime example of a model designed for controllable, multi-view generation to support generalist robot policies [[12](https://arxiv.org/html/2510.10125v1)]. The key challenge Ctrl-World addresses is creating a simulator that can reliably handle multi-step interactions with modern VLA (Vision-Language-Action) policies. This requires supporting multi-view prediction (e.g., third-person and wrist cameras), fine-grained action control, and long-horizon consistency. To achieve this, Ctrl-World introduces three key architectural components built upon a pre-trained video diffusion backbone. First, **joint multi-view prediction** concatenates input images from different camera views and predicts them all together. This not only meets the input requirements of VLA policies but also significantly reduces hallucinations by providing a more comprehensive visual representation of the scene, especially crucial during contact-rich interactions where a single view might be ambiguous. Second, to achieve **fine-grained action control**, Ctrl-World employs frame-level action conditioning. The action sequence output by the policy is transformed into Cartesian-space robot poses and is fed into the model via frame-wise cross-attention within its spatial transformer. This tightly aligns visual dynamics with control signals, ensuring that each generated frame accurately reflects the causal effect of its corresponding action. Third, to maintain **long-horizon temporal consistency**, the model uses a **pose-conditioned memory retrieval mechanism**. This involves adding sparse history frames into the context and projecting their corresponding robot arm poses into each frame. This allows the model to attend to relevant past states, anchoring future predictions and preventing drift over long rollouts. Trained on the DROID dataset, Ctrl-World demonstrates the ability to generate coherent trajectories for over 20 seconds in novel scenarios, enabling new workflows for policy evaluation and improvement entirely within imagination space [[12](https://arxiv.org/html/2510.10125v1)].

Another significant architectural direction is the development of **object-centric world models**, which aim to build more structured, interpretable, and generalizable simulators by explicitly representing objects and their interactions. The **Factored Interactive Object-Centric World Model (FIOC-WM)** is a leading example in this area, slated for presentation at NeurIPS 2025 [[29](https://arxiv.org/abs/2511.02225)]. FIOC-WM addresses a key limitation of most object-centric RL methods, which factor state by individual objects but leave their interactions implicit. By learning structured representations of both objects *and* their interactions, FIOC-WM aims to improve sample efficiency and generalization. The framework first learns object-centric latents and an interaction structure directly from pixels, leveraging pre-trained vision encoders. The learned world model then decomposes tasks into composable interaction primitives. A hierarchical policy is trained on top of this structured world model: a high-level policy selects the type and order of interactions, while a low-level policy executes them. This explicit, modular learning of interactions is posited as crucial for robust and transferable control. While specific dataset sizes used in its experiments are detailed in the full paper, the model is evaluated on simulated robotic and embodied-AI benchmarks, where it demonstrates improved policy-learning sample efficiency and generalization over world-model baselines that lack such explicit interaction modeling [[29](https://arxiv.org/abs/2511.02225)]. This approach represents a move towards more cognitively plausible world models that understand the world as a collection of interacting entities, rather than an unstructured pixel field.

Finally, the success of large-scale video generation models has inspired their use as powerful backbones for world models in navigation. The **Navigation World Model (NWM)** introduced at CVPR 2025 is a controllable video generation model based on a **Conditional Diffusion Transformer (CDiT)** [[19](https://openaccess.thecvf.com/content/CVPR2025/papers/Bar_Navigation_World_Models_CVPR_2025_paper.pdf)]. NWM is trained on a diverse collection of egocentric videos from both human and robotic agents, scaled up to 1 billion parameters. Its key innovation is the design of the CDiT architecture, whose computational complexity is linear with respect to the number of context frames, making it more efficient and scalable than standard DiT (Diffusion Transformer) models [[19](https://openaccess.thecvf.com/content/CVPR2025/papers/Bar_Navigation_World_Models_CVPR_2025_paper.pdf)]. NWM is conditioned on past observations and navigation actions to predict future visual observations. After training, it can be used for planning by simulating trajectories and evaluating whether they achieve a desired goal, effectively functioning as an imagination-based Model Predictive Control (MPC) system. This demonstrates how powerful, pre-trained generative video models can be adapted and conditioned to serve as effective world models for specific embodied tasks like navigation, leveraging their strong visual priors for planning and decision-making in both familiar and unfamiliar environments [[19](https://openaccess.thecvf.com/content/CVPR2025/papers/Bar_Navigation_World_Models_CVPR_2025_paper.pdf)].

## Conclusion: The Path to General-Purpose Inner Simulators

The advancements in world model research throughout 2025 and into 2026 paint a clear and compelling picture: the field is undergoing a rapid transformation from passive, predictive models towards active, goal-oriented, and general-purpose inner simulators. This evolution is being driven by a powerful synergy between unprecedented data resources and innovative training paradigms. On the data front, the release of massive, multi-modal datasets like AgiBot World, combined with the continued leverage of extensive resources like DROID and Ego4D, has provided the raw material necessary to train models that can understand the intricate physics of manipulation and the broad dynamics of navigation. The meticulous construction of these datasets, employing strategies like adversarial data collection and specialized cleaning pipelines, reflects a growing sophistication in how we curate experiences for AI. However, the availability of data alone is insufficient. The true breakthroughs are emerging from novel training methodologies that directly align model optimization with the end goals of embodied intelligence. The RLVR-World framework's use of Reinforcement Learning with Verifiable Rewards marks a paradigm shift, moving beyond the limitations of surrogate objectives to directly sculpt world models for accuracy and fidelity. This is complemented by architectural innovations like Ctrl-World and FIOC-WM, which are tackling the critical challenges of controllability, multi-modal consistency, and structured, object-centric reasoning.

Despite this remarkable progress, the path towards truly general-purpose world models is still fraught with significant challenges. A recent comprehensive survey on world models for embodied AI highlights several key open questions that define the frontier of research [[31](https://arxiv.org/abs/2510.16732)]. First is the **scarcity of unified datasets**. While we have large datasets for specific domains like manipulation or navigation, we lack a single, massive dataset that unifies these diverse experiences, which could be crucial for training truly generalist world models. Second, there is a critical need for new **evaluation metrics** that assess physical consistency and causal reasoning over simple pixel fidelity. A model can generate visually plausible frames that violate the laws of physics, and current metrics often fail to catch such failures. Third, the **trade-off between model performance and computational efficiency** remains a major hurdle, especially for real-time control on resource-constrained robotic platforms. Finally, the core modeling difficulty of achieving **long-horizon temporal consistency while mitigating error accumulation** continues to be a fundamental challenge. Small prediction errors compound over time, causing long simulations to drift into incoherence.

Looking ahead, the future of world models lies in addressing these challenges by integrating the diverse strands of current research. We foresee the emergence of hybrid models that combine the structured, object-centric reasoning of FIOC-WM with the powerful generative capabilities of large-scale diffusion models like NWM, all trained and fine-tuned using objective-driven paradigms like RLVR. The ultimate goal is to create a single, unified world model that can serve as a general-purpose inner simulator for a wide variety of embodied tasks, from dexterous manipulation to complex navigation in novel environments. Achieving this will not only be a monumental technical achievement but will also represent a significant leap towards Artificial General Intelligence, endowing machines with the ability to understand, reason about, and interact with the physical world in ways that are truly human-like. The work of 2025-2026 has laid a strong foundation, and the coming years promise to be an exciting journey towards realizing this vision of truly intelligent, embodied agents.

## References

[0] Jialong Wu, Shaofeng Yin, Ningya Feng, Mingsheng Long. RLVR-World: Training World Models with Reinforcement Learning. https://arxiv.org/html/2505.13934v2. 2025.

[4] unverciftci. World Models in 2025: From Code to Imagination. https://medium.com/@unverciftci/world-models-in-2025-from-code-to-imagination-2ab362a2c0df.

[10] OpenDriveLab/AgiBot-World: [IROS 2025 Best Paper Award Finalist & IEEE TRO 2026] The Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems. https://github.com/OpenDriveLab/AgiBot-World.

[11] agibot-world/AgiBotWorldChallenge-2025 Â· Datasets at Hugging Face. https://huggingface.co/datasets/agibot-world/AgiBotWorldChallenge-2025.

[12] Yanjiang Guo, Lucy Xiaoyang Shi, Jianyu Chen, Chelsea Finn. Ctrl-World: A Controllable Generative World Model for Robot Manipulation. https://arxiv.org/html/2510.10125v1. 2025.

[19] Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, Yann LeCun. Navigation World Models. https://openaccess.thecvf.com/content/CVPR2025/papers/Bar_Navigation_World_Models_CVPR_2025_paper.pdf. 2025.

[29] Fan Feng, Phillip Lippe, Sara Magliacane. Learning Interactive World Model for Object-Centric Reinforcement Learning. https://arxiv.org/abs/2511.02225. 2025.

[31] Xinqing Li. A Comprehensive Survey on World Models for Embodied AI. https://arxiv.org/abs/2510.16732. 2025.

[34] T Feng. Embodied AI: From LLMs to World Models. https://mn.cs.tsinghua.edu.cn/xinwang/PDF/papers/2025_Embodied%20AI%20from%20LLMs%20to%20World%20Models.pdf.

[35] Oreateai. Embodied Intelligence and World Models Take Center Stage. https://www.oreateai.com/blog/the-ai-revolution-of-2025-embodied-intelligence-and-world-models-take-center-stage/8636c0975d48b1b4d99c2781f2ece5fa.

[50] DROID: A Large-Scale In-the-Wild Robot Manipulation Dataset. https://droid-dataset.github.io.

[51] A Khazatsky. A Large-Scale In-The-Wild Robot Manipulation Dataset. https://arxiv.org/abs/2403.12945. 2024.

[55] A Controllable Generative World Model for Robot Manipulation. https://openreview.net/forum?id=748bHL2BAv.

[56] DROID: In-The-Wild Robot Manipulation Dataset. https://www.emergentmind.com/topics/droid-a-large-scale-in-the-wild-robot-manipulation-dataset.

[61] Navigation World Models. https://arxiv.org/html/2412.03572v2.

[67] K Grauman. Ego4D: Around the World in 3600 Hours of Egocentric Video. https://www.computer.org/csdl/journal/tp/2025/11/10611736/1YTJvDcduIE. 2025.

[70] [Nips 2025] EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation. https://github.com/JeffWang987/EgoVid.

[72] X Wang. EgoVid-5M: A Large-Scale Video-Action Dataset. https://arxiv.org/abs/2411.08380. 2024.

[80] Q Bu. AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems. https://arxiv.org/abs/2503.06669. 2025.

[89] Shenyuan Gao. AgiBot World Colosseo: A Large-Scale Manipulation Platform for Scalable and Intelligent Embodied Systems. https://scholar.google.com/citations?user=hZtOnecAAAAJ&hl=en. 2025.

[106] HR Walke. BridgeData V2: A Dataset for Robot Learning at Scale. https://proceedings.mlr.press/v229/walke23a/walke23a.pdf. 2023.

[110] B Tharwat. Latent Action Pretraining Through World Modeling. https://arxiv.org/pdf/2509.18428. 2025.