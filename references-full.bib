
@misc{black__0_2026,
	title = {\$\pi \_0\$: {A} {Vision}-{Language}-{Action} {Flow} {Model} for {General} {Robot} {Control}},
	shorttitle = {\$\pi \_0\$},
	url = {http://arxiv.org/abs/2410.24164},
	doi = {10.48550/arXiv.2410.24164},
	abstract = {Robot learning holds tremendous promise to unlock the full potential of flexible, general, and dexterous robot systems, as well as to address some of the deepest questions in artificial intelligence. However, bringing robot learning to the level of generality required for effective real-world systems faces major obstacles in terms of data, generalization, and robustness. In this paper, we discuss how generalist robot policies (i.e., robot foundation models) can address these challenges, and how we can design effective generalist robot policies for complex and highly dexterous tasks. We propose a novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge. We then discuss how this model can be trained on a large and diverse dataset from multiple dexterous robot platforms, including single-arm robots, dual-arm robots, and mobile manipulators. We evaluate our model in terms of its ability to perform tasks in zero shot after pre-training, follow language instructions from people and from a high-level VLM policy, and its ability to acquire new skills via fine-tuning. Our results cover a wide variety of tasks, such as laundry folding, table cleaning, and assembling boxes.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and Jakubczak, Szymon and Jones, Tim and Ke, Liyiming and Levine, Sergey and Li-Bell, Adrian and Mothukuri, Mohith and Nair, Suraj and Pertsch, Karl and Shi, Lucy Xiaoyang and Tanner, James and Vuong, Quan and Walling, Anna and Wang, Haohuan and Zhilinsky, Ury},
	month = jan,
	year = {2026},
	note = {arXiv:2410.24164 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\Q9NRAR2B\\Black 等 - 2026 - \$\pi _0\$ A Vision-Language-Action Flow Model for General Robot Control.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\FW4TJBHP\\2410.html:text/html},
}

@misc{pertsch_fast_2025,
	title = {{FAST}: {Efficient} {Action} {Tokenization} for {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{FAST}},
	url = {http://arxiv.org/abs/2501.09747},
	doi = {10.48550/arXiv.2501.09747},
	abstract = {Autoregressive sequence models, such as Transformer-based vision-language action (VLA) policies, can be tremendously effective for capturing complex and generalizable robotic behaviors. However, such models require us to choose a tokenization of our continuous action signals, which determines how the discrete symbols predicted by the model map to continuous robot actions. We find that current approaches for robot action tokenization, based on simple per-dimension, per-timestep binning schemes, typically perform poorly when learning dexterous skills from high-frequency robot data. To address this challenge, we propose a new compression-based tokenization scheme for robot actions, based on the discrete cosine transform. Our tokenization approach, Frequency-space Action Sequence Tokenization (FAST), enables us to train autoregressive VLAs for highly dexterous and high-frequency tasks where standard discretization methods fail completely. Based on FAST, we release FAST+, a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies. Finally, we show that, when combined with the pi0 VLA, our method can scale to training on 10k hours of robot data and match the performance of diffusion VLAs, while reducing training time by up to 5x.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Pertsch, Karl and Stachowicz, Kyle and Ichter, Brian and Driess, Danny and Nair, Suraj and Vuong, Quan and Mees, Oier and Finn, Chelsea and Levine, Sergey},
	month = jan,
	year = {2025},
	note = {arXiv:2501.09747 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\49L88YDL\\Pertsch 等 - 2025 - FAST Efficient Action Tokenization for Vision-Language-Action Models.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\KUYZPMVV\\2501.html:text/html},
}

@misc{intelligence__05_2025,
	title = {\$\pi \_\{0.5\}\$: a {Vision}-{Language}-{Action} {Model} with {Open}-{World} {Generalization}},
	shorttitle = {\$\pi \_\{0.5\}\$},
	url = {http://arxiv.org/abs/2504.16054},
	doi = {10.48550/arXiv.2504.16054},
	abstract = {In order for robots to be useful, they must perform practically relevant tasks in the real world, outside of the lab. While vision-language-action (VLA) models have demonstrated impressive results for end-to-end robot control, it remains an open question how far such models can generalize in the wild. We describe \$\pi \_\{0.5\}\$, a new model based on \$\pi \_\{0\}\$ that uses co-training on heterogeneous tasks to enable broad generalization. \$\pi \_\{0.5\}\${\textbackslash} uses data from multiple robots, high-level semantic prediction, web data, and other sources to enable broadly generalizable real-world robotic manipulation. Our system uses a combination of co-training and hybrid multi-modal examples that combine image observations, language commands, object detections, semantic subtask prediction, and low-level actions. Our experiments show that this kind of knowledge transfer is essential for effective generalization, and we demonstrate for the first time that an end-to-end learning-enabled robotic system can perform long-horizon and dexterous manipulation skills, such as cleaning a kitchen or bedroom, in entirely new homes.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Intelligence, Physical and Black, Kevin and Brown, Noah and Darpinian, James and Dhabalia, Karan and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Galliker, Manuel Y. and Ghosh, Dibya and Groom, Lachy and Hausman, Karol and Ichter, Brian and Jakubczak, Szymon and Jones, Tim and Ke, Liyiming and LeBlanc, Devin and Levine, Sergey and Li-Bell, Adrian and Mothukuri, Mohith and Nair, Suraj and Pertsch, Karl and Ren, Allen Z. and Shi, Lucy Xiaoyang and Smith, Laura and Springenberg, Jost Tobias and Stachowicz, Kyle and Tanner, James and Vuong, Quan and Walke, Homer and Walling, Anna and Wang, Haohuan and Yu, Lili and Zhilinsky, Ury},
	month = apr,
	year = {2025},
	note = {arXiv:2504.16054 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\XIPJ7WNP\\Intelligence 等 - 2025 - \$\pi _ 0.5 \$ a Vision-Language-Action Model with Open-World Generalization.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\F2DZBTAB\\2504.html:text/html},
}

@misc{intelligence__06_2025,
	title = {\$\pi {\textasciicircum}\{*\}\_\{0.6\}\$: a {VLA} {That} {Learns} {From} {Experience}},
	shorttitle = {\$\pi {\textasciicircum}\{*\}\_\{0.6\}\$},
	url = {http://arxiv.org/abs/2511.14759},
	doi = {10.48550/arXiv.2511.14759},
	abstract = {We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call \$\pi {\textasciicircum}\{*\}\_\{0.6\}\$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the \$\pi {\textasciicircum}\{*\}\_\{0.6\}\$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Intelligence, Physical and Amin, Ali and Aniceto, Raichelle and Balakrishna, Ashwin and Black, Kevin and Conley, Ken and Connors, Grace and Darpinian, James and Dhabalia, Karan and DiCarlo, Jared and Driess, Danny and Equi, Michael and Esmail, Adnan and Fang, Yunhao and Finn, Chelsea and Glossop, Catherine and Godden, Thomas and Goryachev, Ivan and Groom, Lachy and Hancock, Hunter and Hausman, Karol and Hussein, Gashon and Ichter, Brian and Jakubczak, Szymon and Jen, Rowan and Jones, Tim and Katz, Ben and Ke, Liyiming and Kuchi, Chandra and Lamb, Marinda and LeBlanc, Devin and Levine, Sergey and Li-Bell, Adrian and Lu, Yao and Mano, Vishnu and Mothukuri, Mohith and Nair, Suraj and Pertsch, Karl and Ren, Allen Z. and Sharma, Charvi and Shi, Lucy Xiaoyang and Smith, Laura and Springenberg, Jost Tobias and Stachowicz, Kyle and Stoeckle, Will and Swerdlow, Alex and Tanner, James and Torne, Marcel and Vuong, Quan and Walling, Anna and Wang, Haohuan and Williams, Blake and Yoo, Sukwon and Yu, Lili and Zhilinsky, Ury and Zhou, Zhiyuan},
	month = nov,
	year = {2025},
	note = {arXiv:2511.14759 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\FQF7D6FP\\Intelligence 等 - 2025 - \$\pi ^ _ 0.6 \$ a VLA That Learns From Experience.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\6T2PXK7T\\2511.html:text/html},
}

@misc{liang_large_2025,
	title = {Large {Model} {Empowered} {Embodied} {AI}: {A} {Survey} on {Decision}-{Making} and {Embodied} {Learning}},
	shorttitle = {Large {Model} {Empowered} {Embodied} {AI}},
	url = {http://arxiv.org/abs/2508.10399},
	doi = {10.48550/arXiv.2508.10399},
	abstract = {Embodied AI aims to develop intelligent systems with physical forms capable of perceiving, decision-making, acting, and learning in real-world environments, providing a promising way to Artificial General Intelligence (AGI). Despite decades of explorations, it remains challenging for embodied agents to achieve human-level intelligence for general-purpose tasks in open dynamic environments. Recent breakthroughs in large models have revolutionized embodied AI by enhancing perception, interaction, planning and learning. In this article, we provide a comprehensive survey on large model empowered embodied AI, focusing on autonomous decision-making and embodied learning. We investigate both hierarchical and end-to-end decision-making paradigms, detailing how large models enhance high-level planning, low-level execution, and feedback for hierarchical decision-making, and how large models enhance Vision-Language-Action (VLA) models for end-to-end decision making. For embodied learning, we introduce mainstream learning methodologies, elaborating on how large models enhance imitation learning and reinforcement learning in-depth. For the first time, we integrate world models into the survey of embodied AI, presenting their design methods and critical roles in enhancing decision-making and learning. Though solid advances have been achieved, challenges still exist, which are discussed at the end of this survey, potentially as the further research directions.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Liang, Wenlong and Zhou, Rui and Ma, Yang and Zhang, Bing and Li, Songlin and Liao, Yijia and Kuang, Ping},
	month = aug,
	year = {2025},
	note = {arXiv:2508.10399 [cs]
version: 1},
	keywords = {Computer Science - Robotics},
	file = {Full Text PDF:E\:\\Users\\AresZz\\Zotero\\storage\\TRG6P47D\\Liang 等 - 2025 - Large Model Empowered Embodied AI A Survey on Decision-Making and Embodied Learning.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\F5CQZD23\\2508.html:text/html},
}

@misc{lu_multimodal_2025,
	title = {Multimodal {Data} {Storage} and {Retrieval} for {Embodied} {AI}: {A} {Survey}},
	shorttitle = {Multimodal {Data} {Storage} and {Retrieval} for {Embodied} {AI}},
	url = {http://arxiv.org/abs/2508.13901},
	doi = {10.48550/arXiv.2508.13901},
	abstract = {Embodied AI (EAI) agents continuously interact with the physical world, generating vast, heterogeneous multimodal data streams that traditional management systems are ill-equipped to handle. In this survey, we first systematically evaluate five storage architectures (Graph Databases, Multi-Model Databases, Data Lakes, Vector Databases, and Time-Series Databases), focusing on their suitability for addressing EAI's core requirements, including physical grounding, low-latency access, and dynamic scalability. We then analyze five retrieval paradigms (Fusion Strategy-Based Retrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based Retrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based Optimization), revealing a fundamental tension between achieving long-term semantic coherence and maintaining real-time responsiveness. Based on this comprehensive analysis, we identify key bottlenecks, spanning from the foundational Physical Grounding Gap to systemic challenges in cross-modal integration, dynamic adaptation, and open-world generalization. Finally, we outline a forward-looking research agenda encompassing physics-aware data models, adaptive storage-retrieval co-optimization, and standardized benchmarking, to guide future research toward principled data management solutions for EAI. Our survey is based on a comprehensive review of more than 180 related studies, providing a rigorous roadmap for designing the robust, high-performance data management frameworks essential for the next generation of autonomous embodied systems.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Lu, Yihao and Tang, Hao},
	month = aug,
	year = {2025},
	note = {arXiv:2508.13901 [cs]
version: 1},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:E\:\\Users\\AresZz\\Zotero\\storage\\G52CRYHE\\Lu和Tang - 2025 - Multimodal Data Storage and Retrieval for Embodied AI A Survey.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\9YQ3IETC\\2508.html:text/html},
}

@misc{chen_bridgev2w_2026,
	title = {{BridgeV2W}: {Bridging} {Video} {Generation} {Models} to {Embodied} {World} {Models} via {Embodiment} {Masks}},
	shorttitle = {{BridgeV2W}},
	url = {http://arxiv.org/abs/2602.03793},
	doi = {10.48550/arXiv.2602.03793},
	abstract = {Embodied world models have emerged as a promising paradigm in robotics, most of which leverage large-scale Internet videos or pretrained video generation models to enrich visual and motion priors. However, they still face key challenges: a misalignment between coordinate-space actions and pixel-space videos, sensitivity to camera viewpoint, and non-unified architectures across embodiments. To this end, we present BridgeV2W, which converts coordinate-space actions into pixel-aligned embodiment masks rendered from the URDF and camera parameters. These masks are then injected into a pretrained video generation model via a ControlNet-style pathway, which aligns the action control signals with predicted videos, adds view-specific conditioning to accommodate camera viewpoints, and yields a unified world model architecture across embodiments. To mitigate overfitting to static backgrounds, BridgeV2W further introduces a flow-based motion loss that focuses on learning dynamic and task-relevant regions. Experiments on single-arm (DROID) and dual-arm (AgiBot-G1) datasets, covering diverse and challenging conditions with unseen viewpoints and scenes, show that BridgeV2W improves video generation quality compared to prior state-of-the-art methods. We further demonstrate the potential of BridgeV2W on downstream real-world tasks, including policy evaluation and goal-conditioned planning. More results can be found on our project website at https://BridgeV2W.github.io .},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Chen, Yixiang and Li, Peiyan and Yang, Jiabing and He, Keji and Wu, Xiangnan and Xu, Yuan and Wang, Kai and Liu, Jing and Liu, Nianfeng and Huang, Yan and Wang, Liang},
	month = feb,
	year = {2026},
	note = {arXiv:2602.03793 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\UGL4ZWK3\\Chen 等 - 2026 - BridgeV2W Bridging Video Generation Models to Embodied World Models via Embodiment Masks.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\W622GL8B\\2602.html:text/html},
}

@misc{batra_rearrangement_2020,
	title = {Rearrangement: {A} {Challenge} for {Embodied} {AI}},
	shorttitle = {Rearrangement},
	url = {http://arxiv.org/abs/2011.01975},
	doi = {10.48550/arXiv.2011.01975},
	abstract = {We describe a framework for research and evaluation in Embodied AI. Our proposal is based on a canonical task: Rearrangement. A standard task can focus the development of new techniques and serve as a source of trained models that can be transferred to other settings. In the rearrangement task, the goal is to bring a given physical environment into a specified state. The goal state can be specified by object poses, by images, by a description in language, or by letting the agent experience the environment in the goal state. We characterize rearrangement scenarios along different axes and describe metrics for benchmarking rearrangement performance. To facilitate research and exploration, we present experimental testbeds of rearrangement scenarios in four different simulation environments. We anticipate that other datasets will be released and new simulation platforms will be built to support training of rearrangement agents and their deployment on physical systems.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Batra, Dhruv and Chang, Angel X. and Chernova, Sonia and Davison, Andrew J. and Deng, Jia and Koltun, Vladlen and Levine, Sergey and Malik, Jitendra and Mordatch, Igor and Mottaghi, Roozbeh and Savva, Manolis and Su, Hao},
	month = nov,
	year = {2020},
	note = {arXiv:2011.01975 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\KY54HMBJ\\Batra 等 - 2020 - Rearrangement A Challenge for Embodied AI.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\MML3TKBW\\2011.html:text/html},
}

@misc{liu_aligning_2025,
	title = {Aligning {Cyber} {Space} with {Physical} {World}: {A} {Comprehensive} {Survey} on {Embodied} {AI}},
	shorttitle = {Aligning {Cyber} {Space} with {Physical} {World}},
	url = {http://arxiv.org/abs/2407.06886},
	doi = {10.48550/arXiv.2407.06886},
	abstract = {Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications (e.g., intelligent mechatronics systems, smart manufacturing) that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for embodied agents. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss potential future directions. We hope this survey will serve as a foundational reference for the research community. The associated project can be found at https://github.com/HCPLab-SYSU/Embodied\_AI\_Paper\_List.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Liu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang},
	month = aug,
	year = {2025},
	note = {arXiv:2407.06886 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\HGJZVDZN\\Liu 等 - 2025 - Aligning Cyber Space with Physical World A Comprehensive Survey on Embodied AI.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\JPPQ2JZQ\\2407.html:text/html},
}

@misc{gupta_essential_2024,
	title = {The {Essential} {Role} of {Causality} in {Foundation} {World} {Models} for {Embodied} {AI}},
	url = {http://arxiv.org/abs/2402.06665},
	doi = {10.48550/arXiv.2402.06665},
	abstract = {Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents will require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions and are therefore insufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitating meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook for future research.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Gupta, Tarun and Gong, Wenbo and Ma, Chao and Pawlowski, Nick and Hilmkil, Agrin and Scetbon, Meyer and Rigter, Marc and Famoti, Ade and Llorens, Ashley Juan and Gao, Jianfeng and Bauer, Stefan and Kragic, Danica and Schölkopf, Bernhard and Zhang, Cheng},
	month = apr,
	year = {2024},
	note = {arXiv:2402.06665 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\EW7J9TJM\\Gupta 等 - 2024 - The Essential Role of Causality in Foundation World Models for Embodied AI.pdf:application/pdf},
}

@article{li_comprehensive_2025,
	title = {A {Comprehensive} {Survey} on {World} {Models} for {Embodied} {AI}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2510.16732},
	doi = {10.48550/ARXIV.2510.16732},
	abstract = {Embodied AI requires agents that perceive, act, and anticipate how actions reshape future world states. World models serve as internal simulators that capture environment dynamics, enabling forward and counterfactual rollouts to support perception, prediction, and decision making. This survey presents a unified framework for world models in embodied AI. Specifically, we formalize the problem setting and learning objectives, and propose a three-axis taxonomy encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2) Temporal Modeling, Sequential Simulation and Inference vs. Global Difference Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We systematize data resources and metrics across robotics, autonomous driving, and general video settings, covering pixel prediction quality, state-level understanding, and task performance. Furthermore, we offer a quantitative comparison of state-of-the-art models and distill key open challenges, including the scarcity of unified datasets and the need for evaluation metrics that assess physical consistency over pixel fidelity, the trade-off between model performance and the computational efficiency required for real-time control, and the core modeling difficulty of achieving long-horizon temporal consistency while mitigating error accumulation. Finally, we maintain a curated bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Xinqing and He, Xin and Zhang, Le and Wu, Min and Li, Xiaoli and Liu, Yun},
	year = {2025},
	note = {Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	file = {Full Text PDF:E\:\\Users\\AresZz\\Zotero\\storage\\CK4QZCM9\\Li 等 - 2025 - A Comprehensive Survey on World Models for Embodied AI.pdf:application/pdf},
}

@article{ding_understanding_2025,
	title = {Understanding {World} or {Predicting} {Future}? {A} {Comprehensive} {Survey} of {World} {Models}},
	volume = {58},
	issn = {0360-0300},
	shorttitle = {Understanding {World} or {Predicting} {Future}?},
	url = {https://dl.acm.org/doi/10.1145/3746449},
	doi = {10.1145/3746449},
	abstract = {The concept of world models has garnered significant attention due to advancements in multimodal large language models such as GPT-4 and video generation models such as Sora, which are central to the pursuit of artificial general intelligence. This survey offers a comprehensive review of the literature on world models. Generally, world models are regarded as tools for either understanding the present state of the world or predicting its future dynamics. This review presents a systematic categorization of world models, emphasizing two primary functions: (1) constructing internal representations to understand the mechanisms of the world, and (2) predicting future states to simulate and guide decision-making. Initially, we examine the current progress in these two categories. We then explore the application of world models in key domains, including autonomous driving, robotics, and social simulacra, with a focus on how each domain utilizes these aspects. Finally, we outline key challenges and provide insights into potential future research directions. We summarize the representative articles along with their code repositories in .},
	number = {3},
	urldate = {2026-02-04},
	journal = {ACM Comput. Surv.},
	author = {Ding, Jingtao and Zhang, Yunke and Shang, Yu and Zhang, Yuheng and Zong, Zefang and Feng, Jie and Yuan, Yuan and Su, Hongyuan and Li, Nian and Sukiennik, Nicholas and Xu, Fengli and Li, Yong},
	month = sep,
	year = {2025},
	pages = {57:1--57:38},
	file = {Full Text PDF:E\:\\Users\\AresZz\\Zotero\\storage\\YAEK74E9\\Ding 等 - 2025 - Understanding World or Predicting Future A Comprehensive Survey of World Models.pdf:application/pdf},
}

@misc{fung_embodied_2025,
	title = {Embodied {AI} {Agents}: {Modeling} the {World}},
	shorttitle = {Embodied {AI} {Agents}},
	url = {http://arxiv.org/abs/2506.22355},
	doi = {10.48550/arXiv.2506.22355},
	abstract = {This paper describes our research on AI agents embodied in visual, virtual or physical forms, enabling them to interact with both users and their environments. These agents, which include virtual avatars, wearable devices, and robots, are designed to perceive, learn and act within their surroundings, which makes them more similar to how humans learn and interact with the environments as compared to disembodied agents. We propose that the development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously. World modeling encompasses the integration of multimodal perception, planning through reasoning for action and control, and memory to create a comprehensive understanding of the physical world. Beyond the physical world, we also propose to learn the mental world model of users to enable better human-agent collaboration.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Fung, Pascale and Bachrach, Yoram and Celikyilmaz, Asli and Chaudhuri, Kamalika and Chen, Delong and Chung, Willy and Dupoux, Emmanuel and Gong, Hongyu and Jégou, Hervé and Lazaric, Alessandro and Majumdar, Arjun and Madotto, Andrea and Meier, Franziska and Metze, Florian and Morency, Louis-Philippe and Moutakanni, Théo and Pino, Juan and Terver, Basile and Tighe, Joseph and Tomasello, Paden and Malik, Jitendra},
	month = jul,
	year = {2025},
	note = {arXiv:2506.22355 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\RTC52HFI\\Fung 等 - 2025 - Embodied AI Agents Modeling the World.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\JHGVTRLZ\\2506.html:text/html},
}

@misc{zhang_building_2024,
	title = {Building {Cooperative} {Embodied} {Agents} {Modularly} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.02485},
	doi = {10.48550/arXiv.2307.02485},
	abstract = {In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Hongxin and Du, Weihua and Shan, Jiaming and Zhou, Qinhong and Du, Yilun and Tenenbaum, Joshua B. and Shu, Tianmin and Gan, Chuang},
	month = feb,
	year = {2024},
	note = {arXiv:2307.02485 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\GHVUSI2C\\Zhang 等 - 2024 - Building Cooperative Embodied Agents Modularly with Large Language Models.pdf:application/pdf},
}

@article{li_embodied_2024,
	title = {Embodied agent interface: {Benchmarking} llms for embodied decision making},
	volume = {37},
	shorttitle = {Embodied agent interface},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/b631da756d1573c24c9ba9c702fde5a9-Abstract-Datasets_and_Benchmarks_Track.html},
	urldate = {2026-02-04},
	journal = {Advances in Neural Information Processing Systems},
	author = {Li, Manling and Zhao, Shiyu and Wang, Qineng and Wang, Kangrui and Zhou, Yu and Srivastava, Sanjana and Gokmen, Cem and Lee, Tony and Li, Erran Li and Zhang, Ruohan},
	year = {2024},
	pages = {100428--100534},
	file = {Full Text PDF:E\:\\Users\\AresZz\\Zotero\\storage\\8BY2IRIR\\Li 等 - 2024 - Embodied agent interface Benchmarking llms for embodied decision making.pdf:application/pdf},
}

@inproceedings{nottingham_embodied_2023,
	title = {Do embodied agents dream of pixelated sheep: {Embodied} decision making using language guided world modelling},
	shorttitle = {Do embodied agents dream of pixelated sheep},
	url = {https://proceedings.mlr.press/v202/nottingham23a.html},
	urldate = {2026-02-04},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nottingham, Kolby and Ammanabrolu, Prithviraj and Suhr, Alane and Choi, Yejin and Hajishirzi, Hannaneh and Singh, Sameer and Fox, Roy},
	year = {2023},
	pages = {26311--26325},
	file = {Full Text PDF:E\:\\Users\\AresZz\\Zotero\\storage\\2RQKZ3QP\\Nottingham 等 - 2023 - Do embodied agents dream of pixelated sheep Embodied decision making using language guided world mo.pdf:application/pdf},
}

@article{brehmer_edgi_2023,
	title = {Edgi: {Equivariant} diffusion for planning with embodied agents},
	volume = {36},
	shorttitle = {Edgi},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/c95c049637c5c549c2a08e8d6dcbca4b-Abstract-Conference.html},
	urldate = {2026-02-04},
	journal = {Advances in Neural Information Processing Systems},
	author = {Brehmer, Johann and Bose, Joey and De Haan, Pim and Cohen, Taco S.},
	year = {2023},
	pages = {63818--63834},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\L8EFX4JH\\Brehmer 等 - 2023 - Edgi Equivariant diffusion for planning with embodied agents.pdf:application/pdf},
}

@misc{wu_plan_2023,
	title = {Plan, {Eliminate}, and {Track} -- {Language} {Models} are {Good} {Teachers} for {Embodied} {Agents}},
	url = {http://arxiv.org/abs/2305.02412},
	doi = {10.48550/arXiv.2305.02412},
	abstract = {Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15\% improvement over SOTA for generalization to human goal specifications.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Wu, Yue and Min, So Yeon and Bisk, Yonatan and Salakhutdinov, Ruslan and Azaria, Amos and Li, Yuanzhi and Mitchell, Tom and Prabhumoye, Shrimai},
	month = may,
	year = {2023},
	note = {arXiv:2305.02412 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\C4XDSUYF\\Wu 等 - 2023 - Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents.pdf:application/pdf},
}

@misc{wang_voyager_2023,
	title = {Voyager: {An} {Open}-{Ended} {Embodied} {Agent} with {Large} {Language} {Models}},
	shorttitle = {Voyager},
	url = {http://arxiv.org/abs/2305.16291},
	doi = {10.48550/arXiv.2305.16291},
	abstract = {We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
	month = oct,
	year = {2023},
	note = {arXiv:2305.16291 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\A5WCLH9V\\Wang 等 - 2023 - Voyager An Open-Ended Embodied Agent with Large Language Models.pdf:application/pdf},
}

@inproceedings{huang_language_2022,
	title = {Language models as zero-shot planners: {Extracting} actionable knowledge for embodied agents},
	shorttitle = {Language models as zero-shot planners},
	url = {https://proceedings.mlr.press/v162/huang22a.html},
	urldate = {2026-02-04},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
	year = {2022},
	pages = {9118--9147},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\DPCJQCCY\\Huang 等 - 2022 - Language models as zero-shot planners Extracting actionable knowledge for embodied agents.pdf:application/pdf},
}

@misc{zheng_jarvis_2025,
	title = {{JARVIS}: {A} {Neuro}-{Symbolic} {Commonsense} {Reasoning} {Framework} for {Conversational} {Embodied} {Agents}},
	shorttitle = {{JARVIS}},
	url = {http://arxiv.org/abs/2208.13266},
	doi = {10.48550/arXiv.2208.13266},
	abstract = {Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain. To benefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents. First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations. Then the symbolic module reasons for sub-goal planning and action generation based on task- and action-level common sense. Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from 6.1{\textbackslash}\% to 15.8{\textbackslash}\%). Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings. Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zheng, Kaizhi and Zhou, Kaiwen and Gu, Jing and Fan, Yue and Wang, Jialu and Di, Zonglin and He, Xuehai and Wang, Xin Eric},
	month = sep,
	year = {2025},
	note = {arXiv:2208.13266 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\6V7B9J6K\\Zheng 等 - 2025 - JARVIS A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents.pdf:application/pdf},
}

@article{dorbala_can_2023,
	title = {Can an embodied agent find your “cat-shaped mug”? llm-based zero-shot object navigation},
	volume = {9},
	shorttitle = {Can an embodied agent find your “cat-shaped mug”?},
	url = {https://ieeexplore.ieee.org/abstract/document/10373065/},
	number = {5},
	urldate = {2026-02-04},
	journal = {IEEE Robotics and Automation Letters},
	publisher = {IEEE},
	author = {Dorbala, Vishnu Sashank and Mullen, James F. and Manocha, Dinesh},
	year = {2023},
	pages = {4083--4090},
	file = {Full Text PDF:E\:\\Users\\AresZz\\Zotero\\storage\\WDVSR8ER\\Dorbala 等 - 2023 - Can an embodied agent find your “cat-shaped mug” llm-based zero-shot object navigation.pdf:application/pdf},
}

@article{li_embodied_2025,
	title = {Embodied {Multi}-{Agent} {Systems}: {A} {Review}},
	volume = {12},
	shorttitle = {Embodied {Multi}-{Agent} {Systems}},
	url = {https://ieeexplore.ieee.org/abstract/document/11036708/},
	number = {6},
	urldate = {2026-02-04},
	journal = {IEEE/CAA Journal of Automatica Sinica},
	publisher = {IEEE},
	author = {Li, Zhuo and Wu, Weiran and Guo, Yunlong and Sun, Jian and Han, Qing-Long},
	year = {2025},
	pages = {1095--1116},
	file = {Full Text PDF:E\:\\Users\\AresZz\\Zotero\\storage\\56ECU8U7\\Li 等 - 2025 - Embodied Multi-Agent Systems A Review.pdf:application/pdf},
}

@inproceedings{yang_embodied_2024,
	title = {Embodied multi-modal agent trained by an llm from a parallel textworld},
	url = {http://openaccess.thecvf.com/content/CVPR2024/html/Yang_Embodied_Multi-Modal_Agent_trained_by_an_LLM_from_a_Parallel_CVPR_2024_paper.html},
	urldate = {2026-02-04},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Yang, Yijun and Zhou, Tianyi and Li, Kanxue and Tao, Dapeng and Li, Lusong and Shen, Li and He, Xiaodong and Jiang, Jing and Shi, Yuhui},
	year = {2024},
	pages = {26275--26285},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\WGD4HUWZ\\Yang 等 - 2024 - Embodied multi-modal agent trained by an llm from a parallel textworld.pdf:application/pdf},
}

@article{gao_dialfred_2022,
	title = {Dialfred: {Dialogue}-enabled agents for embodied instruction following},
	volume = {7},
	shorttitle = {Dialfred},
	url = {https://ieeexplore.ieee.org/abstract/document/9837390/},
	number = {4},
	urldate = {2026-02-04},
	journal = {IEEE Robotics and Automation Letters},
	publisher = {IEEE},
	author = {Gao, Xiaofeng and Gao, Qiaozi and Gong, Ran and Lin, Kaixiang and Thattai, Govind and Sukhatme, Gaurav S.},
	year = {2022},
	pages = {10049--10056},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\I32TF2VK\\Gao 等 - 2022 - Dialfred Dialogue-enabled agents for embodied instruction following.pdf:application/pdf},
}

@misc{xu_urban_2023,
	title = {Urban {Generative} {Intelligence} ({UGI}): {A} {Foundational} {Platform} for {Agents} in {Embodied} {City} {Environment}},
	shorttitle = {Urban {Generative} {Intelligence} ({UGI})},
	url = {http://arxiv.org/abs/2312.11813},
	doi = {10.48550/arXiv.2312.11813},
	abstract = {Urban environments, characterized by their complex, multi-layered networks encompassing physical, social, economic, and environmental dimensions, face significant challenges in the face of rapid urbanization. These challenges, ranging from traffic congestion and pollution to social inequality, call for advanced technological interventions. Recent developments in big data, artificial intelligence, urban computing, and digital twins have laid the groundwork for sophisticated city modeling and simulation. However, a gap persists between these technological capabilities and their practical implementation in addressing urban challenges in an systemic-intelligent way. This paper proposes Urban Generative Intelligence (UGI), a novel foundational platform integrating Large Language Models (LLMs) into urban systems to foster a new paradigm of urban intelligence. UGI leverages CityGPT, a foundation model trained on city-specific multi-source data, to create embodied agents for various urban tasks. These agents, operating within a textual urban environment emulated by city simulator and urban knowledge graph, interact through a natural language interface, offering an open platform for diverse intelligent and embodied agent development. This platform not only addresses specific urban issues but also simulates complex urban systems, providing a multidisciplinary approach to understand and manage urban complexity. This work signifies a transformative step in city science and urban intelligence, harnessing the power of LLMs to unravel and address the intricate dynamics of urban systems. The code repository with demonstrations will soon be released here https://github.com/tsinghua-fib-lab/UGI.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Xu, Fengli and Zhang, Jun and Gao, Chen and Feng, Jie and Li, Yong},
	month = dec,
	year = {2023},
	note = {arXiv:2312.11813 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\Z87KY77I\\Xu 等 - 2023 - Urban Generative Intelligence (UGI) A Foundational Platform for Agents in Embodied City Environment.pdf:application/pdf},
}

@misc{wu_embodied_2023,
	title = {Embodied {Task} {Planning} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.01848},
	doi = {10.48550/arXiv.2307.01848},
	abstract = {Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments. Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the list of existing objects in the scene for GPT-3.5 to generate a large number of instructions and corresponding planned actions. The generated data is leveraged for grounded plan tuning of pre-trained LLMs. During inference, we discover the objects in the scene by extending open-vocabulary object detectors to multi-view RGB images collected in different achievable locations. Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Wu, Zhenyu and Wang, Ziwei and Xu, Xiuwei and Lu, Jiwen and Yan, Haibin},
	month = jul,
	year = {2023},
	note = {arXiv:2307.01848 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\9JDAVPSG\\Wu 等 - 2023 - Embodied Task Planning with Large Language Models.pdf:application/pdf},
}

@article{duan_survey_2022,
	title = {A survey of embodied ai: {From} simulators to research tasks},
	volume = {6},
	shorttitle = {A survey of embodied ai},
	url = {https://ieeexplore.ieee.org/abstract/document/9687596/},
	number = {2},
	urldate = {2026-02-04},
	journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
	publisher = {IEEE},
	author = {Duan, Jiafei and Yu, Samson and Tan, Hui Li and Zhu, Hongyuan and Tan, Cheston},
	year = {2022},
	pages = {230--244},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\FNSWW5RZ\\Duan 等 - 2022 - A survey of embodied ai From simulators to research tasks.pdf:application/pdf},
}

@incollection{leonardis_embodied_2025,
	address = {Cham},
	title = {Embodied {Understanding} of {Driving} {Scenarios}},
	volume = {15120},
	isbn = {978-3-031-73032-0 978-3-031-73033-7},
	url = {https://link.springer.com/10.1007/978-3-031-73033-7_8},
	doi = {10.1007/978-3-031-73033-7_8},
	language = {en},
	urldate = {2026-02-04},
	booktitle = {Computer {Vision} – {ECCV} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Zhou, Yunsong and Huang, Linyan and Bu, Qingwen and Zeng, Jia and Li, Tianyu and Qiu, Hang and Zhu, Hongzi and Guo, Minyi and Qiao, Yu and Li, Hongyang},
	editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
	year = {2025},
	doi = {10.1007/978-3-031-73033-7_8},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {129--148},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\B3SIG2HG\\Zhou 等 - 2025 - Embodied Understanding of Driving Scenarios.pdf:application/pdf},
}

@misc{huang_embodied_2024,
	title = {An {Embodied} {Generalist} {Agent} in {3D} {World}},
	url = {http://arxiv.org/abs/2311.12871},
	doi = {10.48550/arXiv.2311.12871},
	abstract = {Leveraging massive knowledge from large language models (LLMs), recent machine learning models show notable successes in general-purpose task solving in diverse domains such as computer vision and robotics. However, several significant challenges remain: (i) most of these models rely on 2D images yet exhibit a limited capacity for 3D input; (ii) these models rarely explore the tasks inherently defined in 3D world, e.g., 3D grounding, embodied reasoning and acting. We argue these limitations significantly hinder current models from performing real-world tasks and approaching general intelligence. To this end, we introduce LEO, an embodied multi-modal generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in the 3D world. LEO is trained with a unified task interface, model architecture, and objective in two stages: (i) 3D vision-language (VL) alignment and (ii) 3D vision-language-action (VLA) instruction tuning. We collect large-scale datasets comprising diverse object-level and scene-level tasks, which require considerable understanding of and interaction with the 3D world. Moreover, we meticulously design an LLM-assisted pipeline to produce high-quality 3D VL data. Through extensive experiments, we demonstrate LEO's remarkable proficiency across a wide spectrum of tasks, including 3D captioning, question answering, embodied reasoning, navigation and manipulation. Our ablative studies and scaling analyses further provide valuable insights for developing future embodied generalist agents. Code and data are available on project page.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Huang, Jiangyong and Yong, Silong and Ma, Xiaojian and Linghu, Xiongkun and Li, Puhao and Wang, Yan and Li, Qing and Zhu, Song-Chun and Jia, Baoxiong and Huang, Siyuan},
	month = may,
	year = {2024},
	note = {arXiv:2311.12871 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\JQ3B3U8U\\Huang 等 - 2024 - An Embodied Generalist Agent in 3D World.pdf:application/pdf},
}

@inproceedings{yang_physcene_2024,
	title = {Physcene: {Physically} interactable 3d scene synthesis for embodied ai},
	shorttitle = {Physcene},
	url = {http://openaccess.thecvf.com/content/CVPR2024/html/Yang_PhyScene_Physically_Interactable_3D_Scene_Synthesis_for_Embodied_AI_CVPR_2024_paper.html},
	urldate = {2026-02-04},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Yang, Yandan and Jia, Baoxiong and Zhi, Peiyuan and Huang, Siyuan},
	year = {2024},
	pages = {16262--16272},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\GMGVYDK6\\Yang 等 - 2024 - Physcene Physically interactable 3d scene synthesis for embodied ai.pdf:application/pdf},
}

@misc{guo_embodied_2024,
	title = {Embodied {LLM} {Agents} {Learn} to {Cooperate} in {Organized} {Teams}},
	url = {http://arxiv.org/abs/2403.12482},
	doi = {10.48550/arXiv.2403.12482},
	abstract = {Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Guo, Xudong and Huang, Kaixuan and Liu, Jiale and Fan, Wenhui and Vélez, Natalia and Wu, Qingyun and Wang, Huazheng and Griffiths, Thomas L. and Wang, Mengdi},
	month = may,
	year = {2024},
	note = {arXiv:2403.12482 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\T6VGR7RC\\Guo 等 - 2024 - Embodied LLM Agents Learn to Cooperate in Organized Teams.pdf:application/pdf},
}

@misc{dasgupta_collaborating_2023,
	title = {Collaborating with language models for embodied reasoning},
	url = {http://arxiv.org/abs/2302.00763},
	doi = {10.48550/arXiv.2302.00763},
	abstract = {Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Dasgupta, Ishita and Kaeser-Chen, Christine and Marino, Kenneth and Ahuja, Arun and Babayan, Sheila and Hill, Felix and Fergus, Rob},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00763 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\FFV2V5TV\\Dasgupta 等 - 2023 - Collaborating with language models for embodied reasoning.pdf:application/pdf},
}

@inproceedings{hong_multiply_2024,
	title = {Multiply: {A} multisensory object-centric embodied large language model in 3d world},
	shorttitle = {Multiply},
	url = {http://openaccess.thecvf.com/content/CVPR2024/html/Hong_MultiPLY_A_Multisensory_Object-Centric_Embodied_Large_Language_Model_in_3D_CVPR_2024_paper.html},
	urldate = {2026-02-04},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Hong, Yining and Zheng, Zishuo and Chen, Peihao and Wang, Yian and Li, Junyan and Gan, Chuang},
	year = {2024},
	pages = {26406--26416},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\XJH3AK7W\\Hong 等 - 2024 - Multiply A multisensory object-centric embodied large language model in 3d world.pdf:application/pdf},
}

@inproceedings{gadre_continuous_2022,
	title = {Continuous scene representations for embodied ai},
	url = {http://openaccess.thecvf.com/content/CVPR2022/html/Gadre_Continuous_Scene_Representations_for_Embodied_AI_CVPR_2022_paper.html},
	urldate = {2026-02-04},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Gadre, Samir Yitzhak and Ehsani, Kiana and Song, Shuran and Mottaghi, Roozbeh},
	year = {2022},
	pages = {14849--14859},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\IIY7V5MW\\Gadre 等 - 2022 - Continuous scene representations for embodied ai.pdf:application/pdf},
}

@misc{wu_pragmatic_2026,
	title = {A {Pragmatic} {VLA} {Foundation} {Model}},
	url = {http://arxiv.org/abs/2601.18692},
	doi = {10.48550/arXiv.2601.18692},
	abstract = {Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5{\textasciitilde}2.8\${\textbackslash}times\$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Wu, Wei and Lu, Fan and Wang, Yunnan and Yang, Shuai and Liu, Shi and Wang, Fangjing and Zhu, Qian and Sun, He and Wang, Yong and Ma, Shuailei and Ren, Yiyu and Zhang, Kejia and Yu, Hui and Zhao, Jingmei and Zhou, Shuai and Qiu, Zhenqi and Xiong, Houlong and Wang, Ziyu and Wang, Zechen and Cheng, Ran and Li, Yong-Lu and Huang, Yongtao and Zhu, Xing and Shen, Yujun and Zheng, Kecheng},
	month = jan,
	year = {2026},
	note = {arXiv:2601.18692 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\4LYR7NFP\\Wu 等 - 2026 - A Pragmatic VLA Foundation Model.pdf:application/pdf},
}

@misc{zhong_acot-vla_2026,
	title = {{ACoT}-{VLA}: {Action} {Chain}-of-{Thought} for {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{ACoT}-{VLA}},
	url = {http://arxiv.org/abs/2601.11404},
	doi = {10.48550/arXiv.2601.11404},
	abstract = {Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5\%, 84.1\%, and 47.4\% on LIBERO, LIBERO-Plus and VLABench, respectively.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhong, Linqing and Liu, Yi and Wei, Yifei and Xiong, Ziyu and Yao, Maoqing and Liu, Si and Ren, Guanghui},
	month = jan,
	year = {2026},
	note = {arXiv:2601.11404 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\66BL2Z2R\\Zhong 等 - 2026 - ACoT-VLA Action Chain-of-Thought for Vision-Language-Action Models.pdf:application/pdf},
}

@misc{zhang_vlm4vla_2026,
	title = {{VLM4VLA}: {Revisiting} {Vision}-{Language}-{Models} in {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{VLM4VLA}},
	url = {http://arxiv.org/abs/2601.03309},
	doi = {10.48550/arXiv.2601.03309},
	abstract = {Vision-Language-Action (VLA) models, which integrate pretrained large Vision-Language Models (VLM) into their policy backbone, are gaining significant attention for their promising generalization capabilities. This paper revisits a fundamental yet seldom systematically studied question: how VLM choice and competence translate to downstream VLA policies performance? We introduce VLM4VLA, a minimal adaptation pipeline that converts general-purpose VLMs into VLA policies using only a small set of new learnable parameters for fair and efficient comparison. Despite its simplicity, VLM4VLA proves surprisingly competitive with more sophisticated network designs. Through extensive empirical studies on various downstream tasks across three benchmarks, we find that while VLM initialization offers a consistent benefit over training from scratch, a VLM's general capabilities are poor predictors of its downstream task performance. This challenges common assumptions, indicating that standard VLM competence is necessary but insufficient for effective embodied control. We further investigate the impact of specific embodied capabilities by fine-tuning VLMs on seven auxiliary embodied tasks (e.g., embodied QA, visual pointing, depth estimation). Contrary to intuition, improving a VLM's performance on specific embodied skills does not guarantee better downstream control performance. Finally, modality-level ablations identify the visual module in VLM, rather than the language component, as the primary performance bottleneck. We demonstrate that injecting control-relevant supervision into the vision encoder of the VLM yields consistent gains, even when the encoder remains frozen during downstream fine-tuning. This isolates a persistent domain gap between current VLM pretraining objectives and the requirements of embodied action-planning.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Jianke and Chen, Xiaoyu and Wang, Qiuyue and Li, Mingsheng and Guo, Yanjiang and Hu, Yucheng and Zhang, Jiajun and Bai, Shuai and Lin, Junyang and Chen, Jianyu},
	month = jan,
	year = {2026},
	note = {arXiv:2601.03309 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\MHQJ68CC\\Zhang 等 - 2026 - VLM4VLA Revisiting Vision-Language-Models in Vision-Language-Action Models.pdf:application/pdf},
}

@article{li_pointvla_2026,
	title = {Pointvla: {Injecting} the 3d world into vision-language-action models},
	volume = {11},
	shorttitle = {Pointvla},
	url = {https://ieeexplore.ieee.org/abstract/document/11346992/},
	number = {3},
	urldate = {2026-02-04},
	journal = {IEEE Robotics and Automation Letters},
	publisher = {IEEE},
	author = {Li, Chengmeng and Wen, Junjie and Peng, Yaxin and Peng, Yan and Zhu, Yichen},
	year = {2026},
	pages = {2506--2513},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\PY7QIQ69\\Li 等 - 2026 - Pointvla Injecting the 3d world into vision-language-action models.pdf:application/pdf},
}

@misc{liu_--fly_2026,
	title = {On-the-{Fly} {VLA} {Adaptation} via {Test}-{Time} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2601.06748},
	doi = {10.48550/arXiv.2601.06748},
	abstract = {Vision-Language-Action models have recently emerged as a powerful paradigm for general-purpose robot learning, enabling agents to map visual observations and natural-language instructions into executable robotic actions. Though popular, they are primarily trained via supervised fine-tuning or training-time reinforcement learning, requiring explicit fine-tuning phases, human interventions, or controlled data collection. Consequently, existing methods remain unsuitable for challenging simulated- or physical-world deployments, where robots must respond autonomously and flexibly to evolving environments. To address this limitation, we introduce a Test-Time Reinforcement Learning for VLAs (TT-VLA), a framework that enables on-the-fly policy adaptation during inference. TT-VLA formulates a dense reward mechanism that leverages step-by-step task-progress signals to refine action policies during test time while preserving the SFT/RL-trained priors, making it an effective supplement to current VLA models. Empirical results show that our approach enhances overall adaptability, stability, and task success in dynamic, previously unseen scenarios under simulated and real-world settings. We believe TT-VLA offers a principled step toward self-improving, deployment-ready VLAs.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Liu, Changyu and Liu, Yiyang and Wang, Taowen and Zhuang, Qiao and Liang, James Chenhao and Yang, Wenhao and Xu, Renjing and Wang, Qifan and Liu, Dongfang and Han, Cheng},
	month = jan,
	year = {2026},
	note = {arXiv:2601.06748 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\BPRL9G7N\\Liu 等 - 2026 - On-the-Fly VLA Adaptation via Test-Time Reinforcement Learning.pdf:application/pdf},
}

@misc{zhang_compliantvla-adaptor_2026,
	title = {{CompliantVLA}-adaptor: {VLM}-{Guided} {Variable} {Impedance} {Action} for {Safe} {Contact}-{Rich} {Manipulation}},
	shorttitle = {{CompliantVLA}-adaptor},
	url = {http://arxiv.org/abs/2601.15541},
	doi = {10.48550/arXiv.2601.15541},
	abstract = {We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (e.g., RDT, Pi0, OpenVLA-oft) typically output position, but lack force-aware adaptation, leading to unsafe or failed interactions in physical tasks involving contact, compliance, or uncertainty. In the proposed CompliantVLA-adaptor, a VLM interprets task context from images and natural language to adapt the stiffness and damping parameters of a VIC controller. These parameters are further regulated using real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms the VLA baselines on a suite of complex contact-rich tasks, both in simulation and on real hardware, with improved success rates and reduced force violations. The overall success rate across all tasks increases from 9.86{\textbackslash}\% to 17.29{\textbackslash}\%, presenting a promising path towards safe contact-rich manipulation using VLAs. We release our code, prompts, and force-torque-impedance-scenario context datasets at https://sites.google.com/view/compliantvla.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Heng and Huang, Wei-Hsing and Tong, Qiyi and Solak, Gokhan and Liu, Puze and Liu, Sheng and Peters, Jan and Ajoudani, Arash},
	month = jan,
	year = {2026},
	note = {arXiv:2601.15541 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\G3X958MQ\\Zhang 等 - 2026 - CompliantVLA-adaptor VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation.pdf:application/pdf},
}

@misc{yu_ac2-vla_2026,
	title = {{AC}{\textasciicircum}2-{VLA}: {Action}-{Context}-{Aware} {Adaptive} {Computation} in {Vision}-{Language}-{Action} {Models} for {Efficient} {Robotic} {Manipulation}},
	shorttitle = {{AC}{\textasciicircum}2-{VLA}},
	url = {http://arxiv.org/abs/2601.19634},
	doi = {10.48550/arXiv.2601.19634},
	abstract = {Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC{\textasciicircum}2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC{\textasciicircum}2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC{\textasciicircum}2-VLA achieves up to a 1.79{\textbackslash}times speedup while reducing FLOPs to 29.4\% of the dense baseline, with comparable task success.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Yu, Wenda and Wang, Tianshi and Li, Fengling and Li, Jingjing and Zhu, Lei},
	month = jan,
	year = {2026},
	note = {arXiv:2601.19634 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Multimedia},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\TNUIRLVJ\\Yu 等 - 2026 - AC^2-VLA Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient R.pdf:application/pdf},
}

@misc{xie_dynamicvla_2026,
	title = {{DynamicVLA}: {A} {Vision}-{Language}-{Action} {Model} for {Dynamic} {Object} {Manipulation}},
	shorttitle = {{DynamicVLA}},
	url = {http://arxiv.org/abs/2601.22153},
	doi = {10.48550/arXiv.2601.22153},
	abstract = {Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Xie, Haozhe and Wen, Beichen and Zheng, Jiarui and Chen, Zhaoxi and Hong, Fangzhou and Diao, Haiwen and Liu, Ziwei},
	month = jan,
	year = {2026},
	note = {arXiv:2601.22153 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\WSC4EQ7S\\Xie 等 - 2026 - DynamicVLA A Vision-Language-Action Model for Dynamic Object Manipulation.pdf:application/pdf},
}

@misc{li_vla-rft_2025,
	title = {{VLA}-{RFT}: {Vision}-{Language}-{Action} {Reinforcement} {Fine}-tuning with {Verified} {Rewards} in {World} {Simulators}},
	shorttitle = {{VLA}-{RFT}},
	url = {http://arxiv.org/abs/2510.00406},
	doi = {10.48550/arXiv.2510.00406},
	abstract = {Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Hengtao and Ding, Pengxiang and Suo, Runze and Wang, Yihao and Ge, Zirui and Zang, Dongyuan and Yu, Kexian and Sun, Mingyang and Zhang, Hongyin and Wang, Donglin and Su, Weihua},
	month = oct,
	year = {2025},
	note = {arXiv:2510.00406 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\C49I8A7F\\Li 等 - 2025 - VLA-RFT Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators.pdf:application/pdf},
}

@misc{xiao_world-env_2025,
	title = {World-{Env}: {Leveraging} {World} {Model} as a {Virtual} {Environment} for {VLA} {Post}-{Training}},
	shorttitle = {World-{Env}},
	url = {http://arxiv.org/abs/2509.24948},
	doi = {10.48550/arXiv.2509.24948},
	abstract = {Vision-Language-Action (VLA) models trained via imitation learning suffer from significant performance degradation in data-scarce scenarios due to their reliance on large-scale demonstration datasets. Although reinforcement learning (RL)-based post-training has proven effective in addressing data scarcity, its application to VLA models is hindered by the non-resettable nature of real-world environments. This limitation is particularly critical in high-risk domains such as industrial automation, where interactions often induce state changes that are costly or infeasible to revert. Furthermore, existing VLA approaches lack a reliable mechanism for detecting task completion, leading to redundant actions that reduce overall task success rates. To address these challenges, we propose World-Env, an RL-based post-training framework that replaces physical interaction with a low-cost, world model-based virtual simulator. World-Env consists of two key components: (1) a video-based world simulator that generates temporally consistent future visual observations, and (2) a vision-language model (VLM)-guided instant reflector that provides continuous reward signals and predicts action termination. This simulated environment enables VLA models to safely explore and generalize beyond their initial imitation learning distribution. Our method achieves notable performance gains with as few as five expert demonstrations per task. Experiments on complex robotic manipulation tasks demonstrate that World-Env effectively overcomes the data inefficiency, safety constraints, and inefficient execution of conventional VLA models that rely on real-world interaction, offering a practical and scalable solution for post-training in resource-constrained settings. Our code is available at https://github.com/amap-cvlab/world-env.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Xiao, Junjin and Yang, Yandan and Chang, Xinyuan and Chen, Ronghan and Xiong, Feng and Xu, Mu and Zheng, Wei-Shi and Zhang, Qing},
	month = nov,
	year = {2025},
	note = {arXiv:2509.24948 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\5GDGUQPB\\Xiao 等 - 2025 - World-Env Leveraging World Model as a Virtual Environment for VLA Post-Training.pdf:application/pdf},
}

@misc{li_drivevla-w0_2025,
	title = {{DriveVLA}-{W0}: {World} {Models} {Amplify} {Data} {Scaling} {Law} in {Autonomous} {Driving}},
	shorttitle = {{DriveVLA}-{W0}},
	url = {http://arxiv.org/abs/2510.12796},
	doi = {10.48550/arXiv.2510.12796},
	abstract = {Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence. However, VLA models are limited by a ``supervision deficit'': the vast model capacity is supervised by sparse, low-dimensional actions, leaving much of their representational power underutilized. To remedy this, we propose {\textbackslash}textbf\{DriveVLA-W0\}, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment. We showcase the paradigm's versatility by instantiating it for two dominant VLA archetypes: an autoregressive world model for VLAs that use discrete visual tokens, and a diffusion world model for those operating on continuous visual features. Building on the rich representations learned from world modeling, we introduce a lightweight action expert to address the inference latency for real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a 680x larger in-house dataset demonstrate that DriveVLA-W0 significantly outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling law, showing that performance gains accelerate as the training dataset size increases.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Yingyan and Shang, Shuyao and Liu, Weisong and Zhan, Bing and Wang, Haochen and Wang, Yuqi and Chen, Yuntao and Wang, Xiaoman and An, Yasong and Tang, Chufeng and Hou, Lu and Fan, Lue and Zhang, Zhaoxiang},
	month = dec,
	year = {2025},
	note = {arXiv:2510.12796 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\FU5IVQZ9\\Li 等 - 2025 - DriveVLA-W0 World Models Amplify Data Scaling Law in Autonomous Driving.pdf:application/pdf},
}

@misc{guo_vla-reasoner_2025,
	title = {{VLA}-{Reasoner}: {Empowering} {Vision}-{Language}-{Action} {Models} with {Reasoning} via {Online} {Monte} {Carlo} {Tree} {Search}},
	shorttitle = {{VLA}-{Reasoner}},
	url = {http://arxiv.org/abs/2509.22643},
	doi = {10.48550/arXiv.2509.22643},
	abstract = {Vision-Language-Action models (VLAs) achieve strong performance in general robotic manipulation tasks by scaling imitation learning. However, existing VLAs are limited to predicting short-sighted next-action, which struggle with long-horizon trajectory tasks due to incremental deviations. To address this problem, we propose a plug-in framework named VLA-Reasoner that effectively empowers off-the-shelf VLAs with the capability of foreseeing future states via test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible action trajectories where involved actions are rationales to generate future states via a world model, which enables VLA-Reasoner to foresee and reason potential outcomes and search for the optimal actions. We further leverage Monte Carlo Tree Search (MCTS) to improve search efficiency in large action spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a confidence sampling mechanism based on Kernel Density Estimation (KDE), to enable efficient exploration in MCTS without redundant VLA queries. We evaluate intermediate states in MCTS via an offline reward shaping strategy, to score predicted futures and correct deviations with long-term feedback. We conducted extensive experiments in both simulators and the real world, demonstrating that our proposed VLA-Reasoner achieves significant improvements over the state-of-the-art VLAs. Our method highlights a potential pathway toward scalable test-time computation of robotic manipulation.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Guo, Wenkai and Lu, Guanxing and Deng, Haoyuan and Wu, Zhenyu and Tang, Yansong and Wang, Ziwei},
	month = sep,
	year = {2025},
	note = {arXiv:2509.22643 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\KRZME2Q2\\Guo 等 - 2025 - VLA-Reasoner Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Se.pdf:application/pdf},
}

@misc{jiang_galaxea_2025,
	title = {Galaxea {Open}-{World} {Dataset} and {G0} {Dual}-{System} {VLA} {Model}},
	url = {http://arxiv.org/abs/2509.00576},
	doi = {10.48550/arXiv.2509.00576},
	abstract = {We present Galaxea Open-World Dataset, a large-scale, diverse collection of robot behaviors recorded in authentic human living and working environments. All demonstrations are gathered using a consistent robotic embodiment, paired with precise subtask-level language annotations to facilitate both training and evaluation. Building on this dataset, we introduce G0, a dual-system framework that couples a Vision-Language Model (VLM) for multimodal planning with a Vision-Language-Action (VLA) model for fine-grained execution. G0 is trained using a three-stage curriculum: cross-embodiment pre-training, single-embodiment pre-training, and task-specific post-training. A comprehensive benchmark spanning tabletop manipulation, few-shot learning, and long-horizon mobile manipulation, demonstrates the effectiveness of our approach. In particular, we find that the single-embodiment pre-training stage, together with the Galaxea Open-World Dataset, plays a critical role in achieving strong performance.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Jiang, Tao and Yuan, Tianyuan and Liu, Yicheng and Lu, Chenhao and Cui, Jianning and Liu, Xiao and Cheng, Shuiqi and Gao, Jiyang and Xu, Huazhe and Zhao, Hang},
	month = aug,
	year = {2025},
	note = {arXiv:2509.00576 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\AAGN6H98\\Jiang 等 - 2025 - Galaxea Open-World Dataset and G0 Dual-System VLA Model.pdf:application/pdf},
}

@misc{liu_eva-vla_2025,
	title = {Eva-{VLA}: {Evaluating} {Vision}-{Language}-{Action} {Models}' {Robustness} {Under} {Real}-{World} {Physical} {Variations}},
	shorttitle = {Eva-{VLA}},
	url = {http://arxiv.org/abs/2509.18953},
	doi = {10.48550/arXiv.2509.18953},
	abstract = {Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60\%, with object transformations causing up to 97.8\% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Liu, Hanqing and Long, Jiahuan and Wu, Junqi and Hou, Jiacheng and Tang, Huili and Jiang, Tingsong and Zhou, Weien and Yao, Wen},
	month = sep,
	year = {2025},
	note = {arXiv:2509.18953 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\KLXIHF3G\\Liu 等 - 2025 - Eva-VLA Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations.pdf:application/pdf},
}

@misc{team_gigabrain-0_2025,
	title = {{GigaBrain}-0: {A} {World} {Model}-{Powered} {Vision}-{Language}-{Action} {Model}},
	shorttitle = {{GigaBrain}-0},
	url = {http://arxiv.org/abs/2510.19430},
	doi = {10.48550/arXiv.2510.19430},
	abstract = {Training Vision-Language-Action (VLA) models for generalist robots typically requires large-scale real-world robot data, which is expensive and time-consuming to collect. The inefficiency of physical data collection severely limits the scalability, and generalization capacity of current VLA systems. To address this challenge, we introduce GigaBrain-0, a novel VLA foundation model empowered by world model-generated data (e.g., video generation, real2real transfer, human transfer, view transfer, sim2real transfer data). By leveraging world models to generate diverse data at scale, GigaBrain-0 significantly reduces reliance on real robot data while improving cross-task generalization. Our approach further improves policy robustness through RGBD input modeling and embodied Chain-of-Thought (CoT) supervision, enabling the model to reason about spatial geometry, object states, and long-horizon dependencies during task execution. This leads to substantial gains in real-world performance on dexterous, long-horizon, and mobile manipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves superior generalization across variations in appearances (e.g., textures, colors), object placements, and camera viewpoints. Additionally, we present GigaBrain-0-Small, an optimized lightweight variant designed to run efficiently on devices such as the NVIDIA Jetson AGX Orin.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Team, GigaBrain and Ye, Angen and Wang, Boyuan and Ni, Chaojun and Huang, Guan and Zhao, Guosheng and Li, Haoyun and Li, Jie and Zhu, Jiagang and Feng, Lv and Li, Peng and Deng, Qiuping and Ouyang, Runqi and Qin, Wenkang and Chen, Xinze and Wang, Xiaofeng and Wang, Yang and Li, Yifan and Li, Yilong and Ding, Yiran and Xu, Yuan and Ye, Yun and Zhou, Yukun and Dong, Zhehao and Wang, Zhenan and Liu, Zhichao and Zhu, Zheng},
	month = dec,
	year = {2025},
	note = {arXiv:2510.19430 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\WHL9AEIJ\\Team 等 - 2025 - GigaBrain-0 A World Model-Powered Vision-Language-Action Model.pdf:application/pdf},
}

@misc{cen_rynnvla-002_2025,
	title = {{RynnVLA}-002: {A} {Unified} {Vision}-{Language}-{Action} and {World} {Model}},
	shorttitle = {{RynnVLA}-002},
	url = {http://arxiv.org/abs/2511.17502},
	doi = {10.48550/arXiv.2511.17502},
	abstract = {We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4\% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50\%.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Cen, Jun and Huang, Siteng and Yuan, Yuqian and Li, Kehan and Yuan, Hangjie and Yu, Chaohui and Jiang, Yuming and Guo, Jiayan and Li, Xin and Luo, Hao and Wang, Fan and Zhao, Deli and Chen, Hao},
	month = nov,
	year = {2025},
	note = {arXiv:2511.17502 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\VD6CRSHV\\Cen 等 - 2025 - RynnVLA-002 A Unified Vision-Language-Action and World Model.pdf:application/pdf},
}

@misc{hung_nora-15_2025,
	title = {{NORA}-1.5: {A} {Vision}-{Language}-{Action} {Model} {Trained} using {World} {Model}- and {Action}-based {Preference} {Rewards}},
	shorttitle = {{NORA}-1.5},
	url = {http://arxiv.org/abs/2511.14659},
	doi = {10.48550/arXiv.2511.14659},
	abstract = {Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Hung, Chia-Yu and Majumder, Navonil and Deng, Haoyuan and Renhang, Liu and Ang, Yankang and Zadeh, Amir and Li, Chuan and Herremans, Dorien and Wang, Ziwei and Poria, Soujanya},
	month = nov,
	year = {2025},
	note = {arXiv:2511.14659 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\MITRP4PC\\Hung 等 - 2025 - NORA-1.5 A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewa.pdf:application/pdf},
}

@misc{wang_vla-adapter_2025,
	title = {{VLA}-{Adapter}: {An} {Effective} {Paradigm} for {Tiny}-{Scale} {Vision}-{Language}-{Action} {Model}},
	shorttitle = {{VLA}-{Adapter}},
	url = {http://arxiv.org/abs/2509.09372},
	doi = {10.48550/arXiv.2509.09372},
	abstract = {Vision-Language-Action (VLA) models typically bridge the gap between perceptual and action spaces by pre-training a large-scale Vision-Language Model (VLM) on robotic data. While this approach greatly enhances performance, it also incurs significant training costs. In this paper, we investigate how to effectively bridge vision-language (VL) representations to action (A). We introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA models on large-scale VLMs and extensive pre-training. To this end, we first systematically analyze the effectiveness of various VL conditions and present key findings on which conditions are essential for bridging perception and action spaces. Based on these insights, we propose a lightweight Policy module with Bridge Attention, which autonomously injects the optimal condition into the action space. In this way, our method achieves high performance using only a 0.5B-parameter backbone, without any robotic data pre-training. Extensive experiments on both simulated and real-world robotic benchmarks demonstrate that VLA-Adapter not only achieves state-of-the-art level performance, but also offers the fast inference speed reported to date. Furthermore, thanks to the proposed advanced bridging paradigm, VLA-Adapter enables the training of a powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly lowering the barrier to deploying the VLA model. Project page: https://vla-adapter.github.io/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Wang, Yihao and Ding, Pengxiang and Li, Lingxiao and Cui, Can and Ge, Zirui and Tong, Xinyang and Song, Wenxuan and Zhao, Han and Zhao, Wei and Hou, Pengxu and Huang, Siteng and Tang, Yifan and Wang, Wenhui and Zhang, Ru and Liu, Jianyi and Wang, Donglin},
	month = sep,
	year = {2025},
	note = {arXiv:2509.09372 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\UZBZXNAE\\Wang 等 - 2025 - VLA-Adapter An Effective Paradigm for Tiny-Scale Vision-Language-Action Model.pdf:application/pdf},
}

@misc{ye_vla-r1_2025,
	title = {{VLA}-{R1}: {Enhancing} {Reasoning} in {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{VLA}-{R1}},
	url = {http://arxiv.org/abs/2510.01623},
	doi = {10.48550/arXiv.2510.01623},
	abstract = {Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: https://github.com/GigaAI-research/VLA-R1. Website: https://gigaai-research.github.io/VLA-R1.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Ye, Angen and Zhang, Zeyu and Wang, Boyuan and Wang, Xiaofeng and Zhang, Dapeng and Zhu, Zheng},
	month = oct,
	year = {2025},
	note = {arXiv:2510.01623 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\7RBLPAJL\\Ye 等 - 2025 - VLA-R1 Enhancing Reasoning in Vision-Language-Action Models.pdf:application/pdf},
}

@misc{dong_vita-vla_2025,
	title = {{VITA}-{VLA}: {Efficiently} {Teaching} {Vision}-{Language} {Models} to {Act} via {Action} {Expert} {Distillation}},
	shorttitle = {{VITA}-{VLA}},
	url = {http://arxiv.org/abs/2510.09607},
	doi = {10.48550/arXiv.2510.09607},
	abstract = {Vision-Language Action (VLA) models significantly advance robotic manipulation by leveraging the strong perception capabilities of pretrained vision-language models (VLMs). By integrating action modules into these pretrained models, VLA methods exhibit improved generalization. However, training them from scratch is costly. In this work, we propose a simple yet effective distillation-based framework that equips VLMs with action-execution capability by transferring knowledge from pretrained small action models. Our architecture retains the original VLM structure, adding only an action token and a state encoder to incorporate physical inputs. To distill action knowledge, we adopt a two-stage training strategy. First, we perform lightweight alignment by mapping VLM hidden states into the action space of the small action model, enabling effective reuse of its pretrained action decoder and avoiding expensive pretraining. Second, we selectively fine-tune the language model, state encoder, and action modules, enabling the system to integrate multimodal inputs with precise action generation. Specifically, the action token provides the VLM with a direct handle for predicting future actions, while the state encoder allows the model to incorporate robot dynamics not captured by vision alone. This design yields substantial efficiency gains over training large VLA models from scratch. Compared with previous state-of-the-art methods, our method achieves 97.3\% average success rate on LIBERO (11.8\% improvement) and 93.5\% on LIBERO-LONG (24.5\% improvement). In real-world experiments across five manipulation tasks, our method consistently outperforms the teacher model, achieving 82.0\% success rate (17\% improvement), which demonstrate that action distillation effectively enables VLMs to generate precise actions while substantially reducing training costs.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Dong, Shaoqi and Fu, Chaoyou and Gao, Haihan and Zhang, Yi-Fan and Yan, Chi and Wu, Chu and Liu, Xiaoyu and Shen, Yunhang and Huo, Jing and Jiang, Deqiang and Cao, Haoyu and Gao, Yang and Sun, Xing and He, Ran and Shan, Caifeng},
	month = oct,
	year = {2025},
	note = {arXiv:2510.09607 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\XRQTXESD\\Dong 等 - 2025 - VITA-VLA Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation.pdf:application/pdf},
}

@article{turgunbaev_perception_2025,
	title = {From {Perception} to {Action} with {Integrated} {VLA} {Systems}},
	volume = {1},
	url = {https://altumnova.com/index.php/tsir/article/view/35},
	number = {6},
	urldate = {2026-02-04},
	journal = {Technical Science Integrated Research},
	author = {Turgunbaev, Rashid},
	year = {2025},
	pages = {11--17},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\JS7FT8PF\\Turgunbaev - 2025 - From Perception to Action with Integrated VLA Systems.pdf:application/pdf},
}

@misc{cen_worldvla_2025,
	title = {{WorldVLA}: {Towards} {Autoregressive} {Action} {World} {Model}},
	shorttitle = {{WorldVLA}},
	url = {http://arxiv.org/abs/2506.21539},
	doi = {10.48550/arXiv.2506.21539},
	abstract = {We present WorldVLA, an autoregressive action world model that unifies action and image understanding and generation. Our WorldVLA intergrates Vision-Language-Action (VLA) model and world model in one single framework. The world model predicts future images by leveraging both action and image understanding, with the purpose of learning the underlying physics of the environment to improve action generation. Meanwhile, the action model generates the subsequent actions based on image observations, aiding in visual understanding and in turn helps visual generation of the world model. We demonstrate that WorldVLA outperforms standalone action and world models, highlighting the mutual enhancement between the world model and the action model. In addition, we find that the performance of the action model deteriorates when generating sequences of actions in an autoregressive manner. This phenomenon can be attributed to the model's limited generalization capability for action prediction, leading to the propagation of errors from earlier actions to subsequent ones. To address this issue, we propose an attention mask strategy that selectively masks prior actions during the generation of the current action, which shows significant performance improvement in the action chunk generation task.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Cen, Jun and Yu, Chaohui and Yuan, Hangjie and Jiang, Yuming and Huang, Siteng and Guo, Jiayan and Li, Xin and Song, Yibing and Luo, Hao and Wang, Fan and Zhao, Deli and Chen, Hao},
	month = jun,
	year = {2025},
	note = {arXiv:2506.21539 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\SYSCZ79Y\\Cen 等 - 2025 - WorldVLA Towards Autoregressive Action World Model.pdf:application/pdf},
}

@misc{seong_vla-r_2025,
	title = {{VLA}-{R}: {Vision}-{Language} {Action} {Retrieval} toward {Open}-{World} {End}-to-{End} {Autonomous} {Driving}},
	shorttitle = {{VLA}-{R}},
	url = {http://arxiv.org/abs/2511.12405},
	doi = {10.48550/arXiv.2511.12405},
	abstract = {Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Seong, Hyunki and Moon, Seongwoo and Ahn, Hojin and Kang, Jehun and Shim, David Hyunchul},
	month = nov,
	year = {2025},
	note = {arXiv:2511.12405 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\S48BDTAS\\Seong 等 - 2025 - VLA-R Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving.pdf:application/pdf},
}

@misc{zhu_wmpo_2025,
	title = {{WMPO}: {World} {Model}-based {Policy} {Optimization} for {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{WMPO}},
	url = {http://arxiv.org/abs/2511.09515},
	doi = {10.48550/arXiv.2511.09515},
	abstract = {Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhu, Fangqi and Yan, Zhengyang and Hong, Zicong and Shou, Quanxin and Ma, Xiao and Guo, Song},
	month = nov,
	year = {2025},
	note = {arXiv:2511.09515 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\EUKT5XTV\\Zhu 等 - 2025 - WMPO World Model-based Policy Optimization for Vision-Language-Action Models.pdf:application/pdf},
}

@misc{zhang_dreamvla_2025,
	title = {{DreamVLA}: {A} {Vision}-{Language}-{Action} {Model} {Dreamed} with {Comprehensive} {World} {Knowledge}},
	shorttitle = {{DreamVLA}},
	url = {http://arxiv.org/abs/2507.04447},
	doi = {10.48550/arXiv.2507.04447},
	abstract = {Recent advances in vision-language-action (VLA) models have shown promise in integrating image generation with action prediction to improve generalization and reasoning in robot manipulation. However, existing methods are limited to challenging image-based forecasting, which suffers from redundant information and lacks comprehensive and critical world knowledge, including dynamic, spatial and semantic information. To address these limitations, we propose DreamVLA, a novel VLA framework that integrates comprehensive world knowledge forecasting to enable inverse dynamics modeling, thereby establishing a perception-prediction-action loop for manipulation tasks. Specifically, DreamVLA introduces a dynamic-region-guided world knowledge prediction, integrated with the spatial and semantic cues, which provide compact yet comprehensive representations for action planning. This design aligns with how humans interact with the world by first forming abstract multimodal reasoning chains before acting. To mitigate interference among the dynamic, spatial and semantic information during training, we adopt a block-wise structured attention mechanism that masks their mutual attention, preventing information leakage and keeping each representation clean and disentangled. Moreover, to model the conditional distribution over future actions, we employ a diffusion-based transformer that disentangles action representations from shared latent features. Extensive experiments on both real-world and simulation environments demonstrate that DreamVLA achieves 76.7\% success rate on real robot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Wenyao and Liu, Hongsi and Qi, Zekun and Wang, Yunnan and Yu, Xinqiang and Zhang, Jiazhao and Dong, Runpei and He, Jiawei and Lu, Fan and Wang, He and Zhang, Zhizheng and Yi, Li and Zeng, Wenjun and Jin, Xin},
	month = aug,
	year = {2025},
	note = {arXiv:2507.04447 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\H3L5M4ZC\\Zhang 等 - 2025 - DreamVLA A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge.pdf:application/pdf},
}

@misc{xiang_parallels_2026,
	title = {Parallels {Between} {VLA} {Model} {Post}-{Training} and {Human} {Motor} {Learning}: {Progress}, {Challenges}, and {Trends}},
	shorttitle = {Parallels {Between} {VLA} {Model} {Post}-{Training} and {Human} {Motor} {Learning}},
	url = {http://arxiv.org/abs/2506.20966},
	doi = {10.48550/arXiv.2506.20966},
	abstract = {Vision-language-action (VLA) models extend vision-language models (VLM) by integrating action generation modules for robotic manipulation. Leveraging the strengths of VLM in vision perception and instruction understanding, VLA models exhibit promising generalization across diverse manipulation tasks. However, applications demanding high precision and accuracy reveal performance gaps without further adaptation. Evidence from multiple domains highlights the critical role of post-training to align foundational models with downstream applications, spurring extensive research on post-training VLA models. VLA model post-training aims to enhance an embodiment's ability to interact with the environment for the specified tasks. This perspective aligns with Newell's constraints-led theory of skill acquisition, which posits that motor behavior arises from interactions among task, environmental, and organismic (embodiment) constraints. Accordingly, this survey structures post-training methods into four categories: (i) enhancing environmental perception, (ii) improving embodiment awareness, (iii) deepening task comprehension, and (iv) multi-component integration. Experimental results on standard benchmarks are synthesized to distill actionable guidelines. Finally, open challenges and emerging trends are outlined, relating insights from human learning to prospective methods for VLA post-training. This work delivers both a comprehensive overview of current VLA model post-training methods from a human motor learning perspective and practical insights for VLA model development. Project website: https://github.com/AoqunJin/Awesome-VLA-Post-Training.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Xiang, Tian-Yu and Jin, Ao-Qun and Zhou, Xiao-Hu and Gui, Mei-Jiang and Xie, Xiao-Liang and Liu, Shi-Qi and Wang, Shuang-Yi and Duan, Sheng-Bin and Xie, Fu-Chao and Wang, Wen-Kai and Wang, Si-Cheng and Li, Ling-Yun and Tu, Tian and Hou, Zeng-Guang},
	month = jan,
	year = {2026},
	note = {arXiv:2506.20966 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\PKADB88M\\Xiang 等 - 2026 - Parallels Between VLA Model Post-Training and Human Motor Learning Progress, Challenges, and Trends.pdf:application/pdf},
}

@misc{gu_manualvla_2025,
	title = {{ManualVLA}: {A} {Unified} {VLA} {Model} for {Chain}-of-{Thought} {Manual} {Generation} and {Robotic} {Manipulation}},
	shorttitle = {{ManualVLA}},
	url = {http://arxiv.org/abs/2512.02013},
	doi = {10.48550/arXiv.2512.02013},
	abstract = {Vision-Language-Action (VLA) models have recently emerged, demonstrating strong generalization in robotic scene understanding and manipulation. However, when confronted with long-horizon tasks that require defined goal states, such as LEGO assembly or object rearrangement, existing VLA models still face challenges in coordinating high-level planning with precise manipulation. Therefore, we aim to endow a VLA model with the capability to infer the "how" process from the "what" outcomes, transforming goal states into executable procedures. In this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution. Unlike prior VLA models that directly map sensory inputs to actions, we first equip ManualVLA with a planning expert that generates intermediate manuals consisting of images, position prompts, and textual instructions. Building upon these multimodal manuals, we design a Manual Chain-of-Thought (ManualCoT) reasoning process that feeds them into the action expert, where each manual step provides explicit control conditions, while its latent representation offers implicit guidance for accurate manipulation. To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training. ManualVLA demonstrates strong real-world performance, achieving an average success rate 32\% higher than the previous hierarchical SOTA baseline on LEGO assembly and object rearrangement tasks.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Gu, Chenyang and Liu, Jiaming and Chen, Hao and Huang, Runzhong and Wuwu, Qingpo and Liu, Zhuoyang and Li, Xiaoqi and Li, Ying and Zhang, Renrui and Jia, Peng and Heng, Pheng-Ann and Zhang, Shanghang},
	month = dec,
	year = {2025},
	note = {arXiv:2512.02013 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\RC6H5377\\Gu 等 - 2025 - ManualVLA A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation.pdf:application/pdf},
}

@inproceedings{zhao_cot-vla_2025,
	title = {Cot-vla: {Visual} chain-of-thought reasoning for vision-language-action models},
	shorttitle = {Cot-vla},
	url = {http://openaccess.thecvf.com/content/CVPR2025/html/Zhao_CoT-VLA_Visual_Chain-of-Thought_Reasoning_for_Vision-Language-Action_Models_CVPR_2025_paper.html},
	urldate = {2026-02-04},
	booktitle = {Proceedings of the {Computer} {Vision} and {Pattern} {Recognition} {Conference}},
	author = {Zhao, Qingqing and Lu, Yao and Kim, Moo Jin and Fu, Zipeng and Zhang, Zhuoyang and Wu, Yecheng and Li, Zhaoshuo and Ma, Qianli and Han, Song and Finn, Chelsea},
	year = {2025},
	pages = {1702--1713},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\988RG9EI\\Zhao 等 - 2025 - Cot-vla Visual chain-of-thought reasoning for vision-language-action models.pdf:application/pdf},
}

@misc{li_simplevla-rl_2025,
	title = {{SimpleVLA}-{RL}: {Scaling} {VLA} {Training} via {Reinforcement} {Learning}},
	shorttitle = {{SimpleVLA}-{RL}},
	url = {http://arxiv.org/abs/2509.09674},
	doi = {10.48550/arXiv.2509.09674},
	abstract = {Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms \$\pi \_0\$ on RoboTwin 1.0{\textbackslash}\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Haozhan and Zuo, Yuxin and Yu, Jiale and Zhang, Yuhao and Yang, Zhaohui and Zhang, Kaiyan and Zhu, Xuekai and Zhang, Yuchen and Chen, Tianxing and Cui, Ganqu and Wang, Dehui and Luo, Dingxiang and Fan, Yuchen and Sun, Youbang and Zeng, Jia and Pang, Jiangmiao and Zhang, Shanghang and Wang, Yu and Mu, Yao and Zhou, Bowen and Ding, Ning},
	month = sep,
	year = {2025},
	note = {arXiv:2509.09674 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\DUS6KB5Q\\Li 等 - 2025 - SimpleVLA-RL Scaling VLA Training via Reinforcement Learning.pdf:application/pdf},
}

@misc{gao_vla-os_2025,
	title = {{VLA}-{OS}: {Structuring} and {Dissecting} {Planning} {Representations} and {Paradigms} in {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{VLA}-{OS}},
	url = {http://arxiv.org/abs/2506.17561},
	doi = {10.48550/arXiv.2506.17561},
	abstract = {Recent studies on Vision-Language-Action (VLA) models have shifted from the end-to-end action-generation paradigm toward a pipeline involving task planning followed by action generation, demonstrating improved performance on various complex, long-horizon manipulation tasks. However, existing approaches vary significantly in terms of network architectures, planning paradigms, representations, and training data sources, making it challenging for researchers to identify the precise sources of performance gains and components to be further improved. To systematically investigate the impacts of different planning paradigms and representations isolating from network architectures and training data, in this paper, we introduce VLA-OS, a unified VLA architecture series capable of various task planning paradigms, and design a comprehensive suite of controlled experiments across diverse object categories (rigid and deformable), visual modalities (2D and 3D), environments (simulation and real-world), and end-effectors (grippers and dexterous hands). Our results demonstrate that: 1) visually grounded planning representations are generally better than language planning representations; 2) the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual learning ability, albeit at the cost of slower training and inference speeds.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Gao, Chongkai and Liu, Zixuan and Chi, Zhenghao and Huang, Junshan and Fei, Xin and Hou, Yiwen and Zhang, Yuxuan and Lin, Yudi and Fang, Zhirui and Jiang, Zeyu and Shao, Lin},
	month = jun,
	year = {2025},
	note = {arXiv:2506.17561 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\EWESQM2H\\Gao 等 - 2025 - VLA-OS Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action.pdf:application/pdf},
}

@misc{zhang_up-vla_2025,
	title = {{UP}-{VLA}: {A} {Unified} {Understanding} and {Prediction} {Model} for {Embodied} {Agent}},
	shorttitle = {{UP}-{VLA}},
	url = {http://arxiv.org/abs/2501.18867},
	doi = {10.48550/arXiv.2501.18867},
	abstract = {Recent advancements in Vision-Language-Action (VLA) models have leveraged pre-trained Vision-Language Models (VLMs) to improve the generalization capabilities. VLMs, typically pre-trained on vision-language understanding tasks, provide rich semantic knowledge and reasoning abilities. However, prior research has shown that VLMs often focus on high-level semantic content and neglect low-level features, limiting their ability to capture detailed spatial information and understand physical dynamics. These aspects, which are crucial for embodied control tasks, remain underexplored in existing pre-training paradigms. In this paper, we investigate the training paradigm for VLAs, and introduce {\textbackslash}textbf\{UP-VLA\}, a {\textbackslash}textbf\{U\}nified VLA model training with both multi-modal {\textbackslash}textbf\{U\}nderstanding and future {\textbackslash}textbf\{P\}rediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33\% improvement on the Calvin ABC-D benchmark compared to the previous state-of-the-art method. Additionally, UP-VLA demonstrates improved success rates in real-world manipulation tasks, particularly those requiring precise spatial information.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Jianke and Guo, Yanjiang and Hu, Yucheng and Chen, Xiaoyu and Zhu, Xiang and Chen, Jianyu},
	month = jun,
	year = {2025},
	note = {arXiv:2501.18867 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\H6A63QK8\\Zhang 等 - 2025 - UP-VLA A Unified Understanding and Prediction Model for Embodied Agent.pdf:application/pdf},
}

@misc{li_vla_2025,
	title = {{VLA} {Models} {Are} {More} {Generalizable} {Than} {You} {Think}: {Revisiting} {Physical} and {Spatial} {Modeling}},
	shorttitle = {{VLA} {Models} {Are} {More} {Generalizable} {Than} {You} {Think}},
	url = {http://arxiv.org/abs/2512.02902},
	doi = {10.48550/arXiv.2512.02902},
	abstract = {Vision-language-action (VLA) models achieve strong in-distribution performance but degrade sharply under novel camera viewpoints and visual perturbations. We show that this brittleness primarily arises from misalignment in Spatial Modeling, rather than Physical Modeling. To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. Our first method, Feature Token Modulation (FTM), applies a global affine transformation to visual tokens and improves Libero viewpoint accuracy from 48.5\% to 87.1\% with only 4K parameters. Building on this, Feature Linear Adaptation (FLA) introduces low-rank updates to the ViT encoder, achieving 90.8\% success with 4.7M parameters -- matching LoRA-scale finetuning at far lower cost. Together, these results reveal substantial untapped robustness in pretrained VLA models and demonstrate that targeted, minimal visual adaptation is sufficient to restore viewpoint generalization.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Weiqi and Zhang, Quande and Zhai, Ruifeng and Lin, Liang and Wang, Guangrun},
	month = dec,
	year = {2025},
	note = {arXiv:2512.02902 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\QEE42NAP\\Li 等 - 2025 - VLA Models Are More Generalizable Than You Think Revisiting Physical and Spatial Modeling.pdf:application/pdf},
}

@misc{won_dual-stream_2025,
	title = {Dual-{Stream} {Diffusion} for {World}-{Model} {Augmented} {Vision}-{Language}-{Action} {Model}},
	url = {http://arxiv.org/abs/2510.27607},
	doi = {10.48550/arXiv.2510.27607},
	abstract = {Recently, augmenting vision-language-action models (VLAs) with world-models has shown promise in robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLAs across diverse tasks. Specifically, we propose a multimodal diffusion transformer architecture that explicitly maintains separate modality streams while enabling cross-modal knowledge sharing. In addition, we propose training techniques such as independent noise perturbations for each modality and a decoupled flow matching loss, which enables the model to learn the joint distribution in a bidirectional manner while avoiding the need for a unified latent space. Furthermore, based on the decoupled training framework, we introduce a sampling method where we sample action and vision tokens asynchronously at different rates, which shows improvement through inference-time scaling. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6\% gains over a standard VLA baseline and implicit world-modeling methods, with our inference-time scaling approach providing an additional 2-5\% gain on success rate. On real-world tasks with the Franka Research 3, DUST outperforms baselines in success rate by 13\%, confirming its effectiveness beyond simulation. Lastly, we demonstrate the effectiveness of DUST in large-scale pretraining with action-free videos from BridgeV2, where DUST leads to significant gain when transferred to the RoboCasa benchmark.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Won, John and Lee, Kyungmin and Jang, Huiwon and Kim, Dongyoung and Shin, Jinwoo},
	month = nov,
	year = {2025},
	note = {arXiv:2510.27607 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\ZZIMFLYL\\Won 等 - 2025 - Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model.pdf:application/pdf},
}

@misc{kachaev_dont_2025,
	title = {Don't {Blind} {Your} {VLA}: {Aligning} {Visual} {Representations} for {OOD} {Generalization}},
	shorttitle = {Don't {Blind} {Your} {VLA}},
	url = {http://arxiv.org/abs/2510.25616},
	doi = {10.48550/arXiv.2510.25616},
	abstract = {The growing success of Vision-Language-Action (VLA) models stems from the promise that pretrained Vision-Language Models (VLMs) can endow agents with transferable world knowledge and vision-language (VL) grounding, laying a foundation for action models with broader generalization. Yet when these VLMs are adapted to the action modality, it remains unclear to what extent their original VL representations and knowledge are preserved. In this work, we conduct a systematic study of representation retention during VLA fine-tuning, showing that naive action fine-tuning leads to degradation of visual representations. To characterize and measure these effects, we probe VLA's hidden representations and analyze attention maps, further, we design a set of targeted tasks and methods that contrast VLA models with their counterpart VLMs, isolating changes in VL capabilities induced by action fine-tuning. We further evaluate a range of strategies for aligning visual representations and introduce a simple yet effective method that mitigates degradation and yields improved generalization to out-of-distribution (OOD) scenarios. Taken together, our analysis clarifies the trade-off between action fine-tuning and the degradation of VL representations and highlights practical approaches to recover inherited VL capabilities. Code is publicly available: https://blind-vla-paper.github.io},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Kachaev, Nikita and Kolosov, Mikhail and Zelezetsky, Daniil and Kovalev, Alexey K. and Panov, Aleksandr I.},
	month = oct,
	year = {2025},
	note = {arXiv:2510.25616 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\ZBXZQW2E\\Kachaev 等 - 2025 - Don't Blind Your VLA Aligning Visual Representations for OOD Generalization.pdf:application/pdf},
}

@misc{chen_unified_2025,
	title = {Unified {Diffusion} {VLA}: {Vision}-{Language}-{Action} {Model} via {Joint} {Discrete} {Denoising} {Diffusion} {Process}},
	shorttitle = {Unified {Diffusion} {VLA}},
	url = {http://arxiv.org/abs/2511.01718},
	doi = {10.48550/arXiv.2511.01718},
	abstract = {Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and act -- reading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through a synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process that integrates multiple modalities into a single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on a unified tokenized space of all modalities and a hybrid attention mechanism. We further propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4\${\textbackslash}times\$ faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at https://irpn-eai.github.io/UD-VLA.github.io/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Chen, Jiayi and Song, Wenxuan and Ding, Pengxiang and Zhou, Ziyang and Zhao, Han and Tang, Feilong and Wang, Donglin and Li, Haoang},
	month = nov,
	year = {2025},
	note = {arXiv:2511.01718 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\9R24JALR\\Chen 等 - 2025 - Unified Diffusion VLA Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process.pdf:application/pdf},
}

@misc{tharwat_latent_2025,
	title = {Latent {Action} {Pretraining} {Through} {World} {Modeling}},
	url = {http://arxiv.org/abs/2509.18428},
	doi = {10.48550/arXiv.2509.18428},
	abstract = {Vision-Language-Action (VLA) models have gained popularity for learning robotic manipulation tasks that follow language instructions. State-of-the-art VLAs, such as OpenVLA and \$\pi \_\{0\}\$, were trained on large-scale, manually labeled action datasets collected through teleoperation. More recent approaches, including LAPA and villa-X, introduce latent action representations that enable unsupervised pretraining on unlabeled datasets by modeling abstract visual changes between frames. Although these methods have shown strong results, their large model sizes make deployment in real-world settings challenging. In this work, we propose LAWM, a model-agnostic framework to pretrain imitation learning models in a self-supervised way, by learning latent action representations from unlabeled video data through world modeling. These videos can be sourced from robot recordings or videos of humans performing actions with everyday objects. Our framework is designed to be effective for transferring across tasks, environments, and embodiments. It outperforms models trained with ground-truth robotics actions and similar pretraining methods on the LIBERO benchmark and real-world setup, while being significantly more efficient and practical for real-world settings.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Tharwat, Bahey and Nasser, Yara and Abouzeid, Ali and Reid, Ian},
	month = sep,
	year = {2025},
	note = {arXiv:2509.18428 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\55CNWB7R\\Tharwat 等 - 2025 - Latent Action Pretraining Through World Modeling.pdf:application/pdf},
}

@misc{li_jarvis-vla_2025,
	title = {{JARVIS}-{VLA}: {Post}-{Training} {Large}-{Scale} {Vision} {Language} {Models} to {Play} {Visual} {Games} with {Keyboards} and {Mouse}},
	shorttitle = {{JARVIS}-{VLA}},
	url = {http://arxiv.org/abs/2503.16365},
	doi = {10.48550/arXiv.2503.16365},
	abstract = {Recently, action-based decision-making in open-world environments has gained significant attention. Visual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in decision-making tasks. However, previous work has primarily focused on action post-training, often neglecting enhancements to the foundational model itself. In response, we introduce a novel approach, Act from Visual Language Post-Training, which refines Visual Language Models (VLMs) through visual and linguistic guidance in a self-supervised manner. This enhancement improves the models' capabilities in world knowledge, visual recognition, and spatial grounding in open-world environments. Following the above post-training paradigms, we obtain the first VLA models in Minecraft that can follow human instructions on over 1k different atomic tasks, including crafting, smelting, cooking, mining, and killing. Our experiments demonstrate that post-training on non-trajectory tasks leads to a significant 40\% improvement over the best agent baseline on a diverse set of atomic tasks. Furthermore, we demonstrate that our approach surpasses traditional imitation learning-based policies in Minecraft, achieving state-of-the-art performance. We have open-sourced the code, models, and datasets to foster further research. The project page can be found in https://craftjarvis.github.io/JarvisVLA.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Muyao and Wang, Zihao and He, Kaichen and Ma, Xiaojian and Liang, Yitao},
	month = sep,
	year = {2025},
	note = {arXiv:2503.16365 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\UXX7ZJMW\\Li 等 - 2025 - JARVIS-VLA Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and.pdf:application/pdf},
}

@misc{zheng_x-vla_2025,
	title = {X-{VLA}: {Soft}-{Prompted} {Transformer} as {Scalable} {Cross}-{Embodiment} {Vision}-{Language}-{Action} {Model}},
	shorttitle = {X-{VLA}},
	url = {http://arxiv.org/abs/2510.10274},
	doi = {10.48550/arXiv.2510.10274},
	abstract = {Successful generalist Vision-Language-Action (VLA) models rely on effective training across diverse robotic platforms with large-scale, cross-embodiment, heterogeneous datasets. To facilitate and leverage the heterogeneity in rich, diverse robotic data sources, we propose a novel Soft Prompt approach with minimally added parameters, by infusing prompt learning concepts into cross-embodiment robot learning and introducing separate sets of learnable embeddings for each distinct data source. These embeddings serve as embodiment-specific prompts, which in unity empower VLA models with effective exploitation of varying cross-embodiment features. Our new X-VLA, a neat flow-matching-based VLA architecture, relies exclusively on soft-prompted standard Transformer encoders, enjoying both scalability and simplicity. Evaluated across 6 simulations as well as 3 real-world robots, our 0.9B instantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep of benchmarks, demonstrating superior results on a wide axes of capabilities, from flexible dexterity to quick adaptation across embodiments, environments, and tasks. Website: https://thu-air-dream.github.io/X-VLA/},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zheng, Jinliang and Li, Jianxiong and Wang, Zhihao and Liu, Dongxiu and Kang, Xirui and Feng, Yuchun and Zheng, Yinan and Zou, Jiayin and Chen, Yilun and Zeng, Jia and Zhang, Ya-Qin and Pang, Jiangmiao and Liu, Jingjing and Wang, Tai and Zhan, Xianyuan},
	month = oct,
	year = {2025},
	note = {arXiv:2510.10274 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\WDCQM6UH\\Zheng 等 - 2025 - X-VLA Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model.pdf:application/pdf},
}

@misc{lin_hif-vla_2025,
	title = {{HiF}-{VLA}: {Hindsight}, {Insight} and {Foresight} through {Motion} {Representation} for {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{HiF}-{VLA}},
	url = {http://arxiv.org/abs/2512.09928},
	doi = {10.48550/arXiv.2512.09928},
	abstract = {Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Lin, Minghui and Ding, Pengxiang and Wang, Shu and Zhuang, Zifeng and Liu, Yang and Tong, Xinyang and Song, Wenxuan and Lyu, Shangke and Huang, Siteng and Wang, Donglin},
	month = dec,
	year = {2025},
	note = {arXiv:2512.09928 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\5I8JWSL6\\Lin 等 - 2025 - HiF-VLA Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action M.pdf:application/pdf},
}

@misc{fan_long-vla_2025,
	title = {Long-{VLA}: {Unleashing} {Long}-{Horizon} {Capability} of {Vision} {Language} {Action} {Model} for {Robot} {Manipulation}},
	shorttitle = {Long-{VLA}},
	url = {http://arxiv.org/abs/2508.19958},
	doi = {10.48550/arXiv.2508.19958},
	abstract = {Vision-Language-Action (VLA) models have become a cornerstone in robotic policy learning, leveraging large-scale multimodal data for robust and scalable control. However, existing VLA frameworks primarily address short-horizon tasks, and their effectiveness on long-horizon, multi-step robotic manipulation remains limited due to challenges in skill chaining and subtask dependencies. In this work, we introduce Long-VLA, the first end-to-end VLA model specifically designed for long-horizon robotic tasks. Our approach features a novel phase-aware input masking strategy that adaptively segments each subtask into moving and interaction phases, enabling the model to focus on phase-relevant sensory cues and enhancing subtask compatibility. This unified strategy preserves the scalability and data efficiency of VLA training, and our architecture-agnostic module can be seamlessly integrated into existing VLA models. We further propose the L-CALVIN benchmark to systematically evaluate long-horizon manipulation. Extensive experiments on both simulated and real-world tasks demonstrate that Long-VLA significantly outperforms prior state-of-the-art methods, establishing a new baseline for long-horizon robotic control.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Fan, Yiguo and Ding, Pengxiang and Bai, Shuanghao and Tong, Xinyang and Zhu, Yuyang and Lu, Hongchao and Dai, Fengqi and Zhao, Wei and Liu, Yang and Huang, Siteng and Fan, Zhaoxin and Chen, Badong and Wang, Donglin},
	month = aug,
	year = {2025},
	note = {arXiv:2508.19958 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\VSTVYXFZ\\Fan 等 - 2025 - Long-VLA Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation.pdf:application/pdf},
}

@misc{goyal_vla-0_2025,
	title = {{VLA}-0: {Building} {State}-of-the-{Art} {VLAs} with {Zero} {Modification}},
	shorttitle = {{VLA}-0},
	url = {http://arxiv.org/abs/2510.13054},
	doi = {10.48550/arXiv.2510.13054},
	abstract = {Vision-Language-Action models (VLAs) hold immense promise for enabling generalist robot manipulation. However, the best way to build them remains an open question. Current approaches often add complexity, such as modifying the existing vocabulary of a Vision-Language Model (VLM) with action tokens or introducing special action heads. Curiously, the simplest strategy of representing actions directly as text has remained largely unexplored. This work introduces VLA-0 to investigate this idea. We find that VLA-0 is not only effective; it is surprisingly powerful. With the right design, VLA-0 outperforms more involved models. On LIBERO, a popular benchmark for evaluating VLAs, VLA-0 outperforms all existing methods trained on the same robotic data, including \$\pi \_0.5\$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without large-scale robotics-specific training, it outperforms methods trained on large-scale robotic data, like \$\pi \_0.5\$-KI, \$\pi \_0\$, GR00T-N1 and MolmoAct. These findings also translate to the real world, where VLA-0 outperforms SmolVLA, a VLA model pre-trained on large-scale real data. This paper summarizes our unexpected findings and spells out the specific techniques required to unlock the high performance of this simple yet potent VLA design. Visual results, code, and trained models are provided here: https://vla0.github.io/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Goyal, Ankit and Hadfield, Hugo and Yang, Xuning and Blukis, Valts and Ramos, Fabio},
	month = oct,
	year = {2025},
	note = {arXiv:2510.13054 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\PRDR42YF\\Goyal 等 - 2025 - VLA-0 Building State-of-the-Art VLAs with Zero Modification.pdf:application/pdf},
}

@misc{zhai_vision-language-action-critic_2025,
	title = {A {Vision}-{Language}-{Action}-{Critic} {Model} for {Robotic} {Real}-{World} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2509.15937},
	doi = {10.48550/arXiv.2509.15937},
	abstract = {Robotic real-world reinforcement learning (RL) with vision-language-action (VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient exploration. We introduce VLAC, a general process reward model built upon InternVL and trained on large scale heterogeneous datasets. Given pairwise observations and a language goal, it outputs dense progress delta and done signal, eliminating task-specific reward engineering, and supports one-shot in-context transfer to unseen tasks and environments. VLAC is trained on vision-language datasets to strengthen perception, dialogic and reasoning capabilities, together with robot and human trajectories data that ground action generation and progress estimation, and additionally strengthened to reject irrelevant prompts as well as detect regression or stagnation by constructing large numbers of negative and semantically mismatched samples. With prompt control, a single VLAC model alternately generating reward and action tokens, unifying critic and policy. Deployed inside an asynchronous real-world RL loop, we layer a graded human-in-the-loop protocol (offline demonstration replay, return and explore, human guided explore) that accelerates exploration and stabilizes early learning. Across four distinct real-world manipulation tasks, VLAC lifts success rates from about 30{\textbackslash}\% to about 90{\textbackslash}\% within 200 real-world interaction episodes; incorporating human-in-the-loop interventions yields a further 50\% improvement in sample efficiency and achieves up to 100\% final success.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhai, Shaopeng and Zhang, Qi and Zhang, Tianyi and Huang, Fuxian and Zhang, Haoran and Zhou, Ming and Zhang, Shengzhe and Liu, Litao and Lin, Sixu and Pang, Jiangmiao},
	month = sep,
	year = {2025},
	note = {arXiv:2509.15937 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\TF534Y7E\\Zhai 等 - 2025 - A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning.pdf:application/pdf},
}

@misc{bai_evolve-vla_2025,
	title = {{EVOLVE}-{VLA}: {Test}-{Time} {Training} from {Environment} {Feedback} for {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{EVOLVE}-{VLA}},
	url = {http://arxiv.org/abs/2512.14666},
	doi = {10.48550/arXiv.2512.14666},
	abstract = {Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6{\textbackslash}\% on long-horizon tasks, +22.0{\textbackslash}\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8{\textbackslash}\% success on unseen tasks without task-specific demonstrations training (vs. 0{\textbackslash}\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Bai, Zechen and Gao, Chen and Shou, Mike Zheng},
	month = dec,
	year = {2025},
	note = {arXiv:2512.14666 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\7IM9UJPN\\Bai 等 - 2025 - EVOLVE-VLA Test-Time Training from Environment Feedback for Vision-Language-Action Models.pdf:application/pdf},
}

@misc{hu_vision-language-action_2026,
	title = {Vision-{Language}-{Action} {Models} for {Autonomous} {Driving}: {Past}, {Present}, and {Future}},
	shorttitle = {Vision-{Language}-{Action} {Models} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2512.16760},
	doi = {10.48550/arXiv.2512.16760},
	abstract = {Autonomous driving has long relied on modular "Perception-Decision-Action" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Hu, Tianshuai and Liu, Xiaolu and Wang, Song and Zhu, Yiyao and Liang, Ao and Kong, Lingdong and Zhao, Guoyang and Gong, Zeying and Cen, Jun and Huang, Zhiyu and Hao, Xiaoshuai and Li, Linfeng and Song, Hang and Li, Xiangtai and Ma, Jun and Shen, Shaojie and Zhu, Jianke and Tao, Dacheng and Liu, Ziwei and Liang, Junwei},
	month = jan,
	year = {2026},
	note = {arXiv:2512.16760 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\PM8WID4F\\Hu 等 - 2026 - Vision-Language-Action Models for Autonomous Driving Past, Present, and Future.pdf:application/pdf},
}

@misc{chen_planning_2025,
	title = {Planning with {Reasoning} using {Vision} {Language} {World} {Model}},
	url = {http://arxiv.org/abs/2509.02722},
	doi = {10.48550/arXiv.2509.02722},
	abstract = {Effective planning requires strong world models, but high-level world models that can understand and reason about actions with semantic and temporal abstraction remain largely underdeveloped. We introduce the Vision Language World Model (VLWM), a foundation model trained for language-based world modeling on natural videos. Given visual observations, the VLWM first infers the overall goal achievements then predicts a trajectory composed of interleaved actions and world state changes. Those targets are extracted by iterative LLM Self-Refine conditioned on compressed future observations represented by Tree of Captions. The VLWM learns both an action policy and a dynamics model, which respectively facilitates reactive system-1 plan decoding and reflective system-2 planning via cost minimization. The cost evaluates the semantic distance between the hypothetical future states given by VLWM roll-outs and the expected goal state, and is measured by a critic model that we trained in a self-supervised manner. The VLWM achieves state-of-the-art Visual Planning for Assistance (VPA) performance on both benchmark evaluations and our proposed PlannerArena human evaluations, where system-2 improves the Elo score by +27\% upon system-1. The VLWM models also outperforms strong VLM baselines on RoboVQA and WorldPrediction benchmark.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Chen, Delong and Moutakanni, Theo and Chung, Willy and Bang, Yejin and Ji, Ziwei and Bolourchi, Allen and Fung, Pascale},
	month = sep,
	year = {2025},
	note = {arXiv:2509.02722 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\HTH5NV2W\\Chen 等 - 2025 - Planning with Reasoning using Vision Language World Model.pdf:application/pdf},
}

@inproceedings{li_coa-vla_2025,
	title = {Coa-vla: {Improving} vision-language-action models via visual-text chain-of-affordance},
	shorttitle = {Coa-vla},
	url = {https://openaccess.thecvf.com/content/ICCV2025/html/Li_CoA-VLA_Improving_Vision-Language-Action_Models_via_Visual-Text_Chain-of-Affordance_ICCV_2025_paper.html},
	urldate = {2026-02-04},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Li, Jinming and Zhu, Yichen and Tang, Zhibin and Wen, Junjie and Zhu, Minjie and Liu, Xiaoyu and Li, Chengmeng and Cheng, Ran and Peng, Yaxin and Peng, Yan},
	year = {2025},
	pages = {9759--9769},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\YY2MA9ZQ\\Li 等 - 2025 - Coa-vla Improving vision-language-action models via visual-text chain-of-affordance.pdf:application/pdf},
}

@inproceedings{li_3ds-vla_2025,
	title = {3ds-vla: {A} 3d spatial-aware vision language action model for robust multi-task manipulation},
	shorttitle = {3ds-vla},
	url = {https://openreview.net/forum?id=dT45OMevL5},
	urldate = {2026-02-04},
	booktitle = {9th {Annual} {Conference} on {Robot} {Learning}},
	author = {Li, Xiaoqi and Heng, Liang and Liu, Jiaming and Shen, Yan and Gu, Chenyang and Liu, Zhuoyang and Chen, Hao and Han, Nuowei and Zhang, Renrui and Tang, Hao},
	year = {2025},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\G2TRXYSZ\\Li 等 - 2025 - 3ds-vla A 3d spatial-aware vision language action model for robust multi-task manipulation.pdf:application/pdf},
}

@misc{li_qdepth-vla_2025,
	title = {{QDepth}-{VLA}: {Quantized} {Depth} {Prediction} as {Auxiliary} {Supervision} for {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{QDepth}-{VLA}},
	url = {http://arxiv.org/abs/2510.14836},
	doi = {10.48550/arXiv.2510.14836},
	abstract = {Spatial perception and reasoning are crucial for Vision-Language-Action (VLA) models to accomplish fine-grained manipulation tasks. However, existing approaches often lack the ability to understand and reason over the essential 3D structures necessary for precise control. To address this limitation, we propose QDepth-VLA, a general framework that augments VLA models with an auxiliary depth prediction task. A dedicated depth expert is designed to predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling the model to learn depth-aware representations that capture critical geometric cues. Experimental results on the simulation benchmarks and real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning and competitive performance on manipulation tasks.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Yixuan and Chen, Yuhui and Zhou, Mingcai and Li, Haoran and Zhang, Zhengtao and Zhao, Dongbin},
	month = dec,
	year = {2025},
	note = {arXiv:2510.14836 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\4J9MASWY\\Li 等 - 2025 - QDepth-VLA Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models.pdf:application/pdf},
}

@misc{pugacheva_bring_2025,
	title = {Bring the {Apple}, {Not} the {Sofa}: {Impact} of {Irrelevant} {Context} in {Embodied} {AI} {Commands} on {VLA} {Models}},
	shorttitle = {Bring the {Apple}, {Not} the {Sofa}},
	url = {http://arxiv.org/abs/2510.07067},
	doi = {10.48550/arXiv.2510.07067},
	abstract = {Vision Language Action (VLA) models are widely used in Embodied AI, enabling robots to interpret and execute language instructions. However, their robustness to natural language variability in real-world scenarios has not been thoroughly investigated. In this work, we present a novel systematic study of the robustness of state-of-the-art VLA models under linguistic perturbations. Specifically, we evaluate model performance under two types of instruction noise: (1) human-generated paraphrasing and (2) the addition of irrelevant context. We further categorize irrelevant contexts into two groups according to their length and their semantic and lexical proximity to robot commands. In this study, we observe consistent performance degradation as context size expands. We also demonstrate that the model can exhibit relative robustness to random context, with a performance drop within 10\%, while semantically and lexically similar context of the same length can trigger a quality decline of around 50\%. Human paraphrases of instructions lead to a drop of nearly 20\%. To mitigate this, we propose an LLM-based filtering framework that extracts core commands from noisy inputs. Incorporating our filtering step allows models to recover up to 98.5\% of their original performance under noisy conditions.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Pugacheva, Daria and Moskalenko, Andrey and Shepelev, Denis and Kuznetsov, Andrey and Shakhuro, Vlad and Tutubalina, Elena},
	month = oct,
	year = {2025},
	note = {arXiv:2510.07067 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\TFBKEQ4Y\\Pugacheva 等 - 2025 - Bring the Apple, Not the Sofa Impact of Irrelevant Context in Embodied AI Commands on VLA Models.pdf:application/pdf},
}

@misc{fan_interleave-vla_2025,
	title = {Interleave-{VLA}: {Enhancing} {Robot} {Manipulation} with {Interleaved} {Image}-{Text} {Instructions}},
	shorttitle = {Interleave-{VLA}},
	url = {http://arxiv.org/abs/2505.02152},
	doi = {10.48550/arXiv.2505.02152},
	abstract = {The rise of foundation models paves the way for generalist robot policies in the physical world. Existing methods relying on text-only instructions often struggle to generalize to unseen scenarios. We argue that interleaved image-text inputs offer richer and less biased context and enable robots to better handle unseen tasks with more versatile human-robot interaction. Building on this insight, Interleave-VLA, the first robot learning paradigm capable of comprehending interleaved image-text instructions and directly generating continuous action sequences in the physical world, is introduced. It offers a natural, flexible, and model-agnostic paradigm that extends state-of-the-art vision-language-action (VLA) models with minimal modifications while achieving strong zero-shot generalization. Interleave-VLA also includes an automatic pipeline that converts text instructions from Open X-Embodiment into interleaved image-text instructions, resulting in a large-scale real-world interleaved embodied dataset with 210k episodes. Comprehensive evaluation in simulation and the real world shows that Interleave-VLA offers two major benefits: (1) improves out-of-domain generalization to unseen objects by 2x compared to text input baselines, (2) supports flexible task interfaces and diverse instructions in a zero-shot manner, such as hand-drawn sketches. We attribute Interleave-VLA's strong zero-shot capability to the use of instruction images, which effectively mitigate hallucinations, and the inclusion of heterogeneous multimodal datasets, enriched with Internet-sourced images, offering potential for scalability. More information is available at https://interleave-vla.github.io/Interleave-VLA-Anonymous/},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Fan, Cunxin and Jia, Xiaosong and Sun, Yihang and Wang, Yixiao and Wei, Jianglan and Gong, Ziyang and Zhao, Xiangyu and Tomizuka, Masayoshi and Yang, Xue and Yan, Junchi and Ding, Mingyu},
	month = oct,
	year = {2025},
	note = {arXiv:2505.02152 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\A2N4RCVK\\Fan 等 - 2025 - Interleave-VLA Enhancing Robot Manipulation with Interleaved Image-Text Instructions.pdf:application/pdf},
}

@misc{hannus_ia-vla_2025,
	title = {{IA}-{VLA}: {Input} {Augmentation} for {Vision}-{Language}-{Action} models in settings with semantically complex tasks},
	shorttitle = {{IA}-{VLA}},
	url = {http://arxiv.org/abs/2509.24768},
	doi = {10.48550/arXiv.2509.24768},
	abstract = {Vision-language-action models (VLAs) have become an increasingly popular approach for addressing robot manipulation problems in recent years. However, such models need to output actions at a rate suitable for robot control, which limits the size of the language model they can be based on, and consequently, their language understanding capabilities. Manipulation tasks may require complex language instructions, such as identifying target objects by their relative positions, to specify human intention. Therefore, we introduce IA-VLA, a framework that utilizes the extensive language understanding of a large vision language model as a pre-processing stage to generate improved context to augment the input of a VLA. We evaluate the framework on a set of semantically complex tasks which have been underexplored in VLA literature, namely tasks involving visual duplicates, i.e., visually indistinguishable objects. A dataset of three types of scenes with duplicate objects is used to compare a baseline VLA against two augmented variants. The experiments show that the VLA benefits from the augmentation scheme, especially when faced with language instructions that require the VLA to extrapolate from concepts it has seen in the demonstrations. For the code, dataset, and videos, see https://sites.google.com/view/ia-vla.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Hannus, Eric and Malin, Miika and Le, Tran Nguyen and Kyrki, Ville},
	month = sep,
	year = {2025},
	note = {arXiv:2509.24768 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\LUS9XN73\\Hannus 等 - 2025 - IA-VLA Input Augmentation for Vision-Language-Action models in settings with semantically complex t.pdf:application/pdf},
}

@misc{jabbour_dont_2025,
	title = {Don't {Run} with {Scissors}: {Pruning} {Breaks} {VLA} {Models} but {They} {Can} {Be} {Recovered}},
	shorttitle = {Don't {Run} with {Scissors}},
	url = {http://arxiv.org/abs/2510.08464},
	doi = {10.48550/arXiv.2510.08464},
	abstract = {Vision-Language-Action (VLA) models have advanced robotic capabilities but remain challenging to deploy on resource-limited hardware. Pruning has enabled efficient compression of large language models (LLMs), yet it is largely understudied in robotics. Surprisingly, we observe that pruning VLA models leads to drastic degradation and increased safety violations. We introduce GLUESTICK, a post-pruning recovery method that restores much of the original model's functionality while retaining sparsity benefits. Our method performs a one-time interpolation between the dense and pruned models in weight-space to compute a corrective term. This correction is used during inference by each pruned layer to recover lost capabilities with minimal overhead. GLUESTICK requires no additional training, is agnostic to the pruning algorithm, and introduces a single hyperparameter that controls the tradeoff between efficiency and accuracy. Across diverse VLA architectures and tasks in manipulation and navigation, GLUESTICK achieves competitive memory efficiency while substantially recovering success rates and reducing safety violations. Additional material can be found at: https://gluestick-vla.github.io/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Jabbour, Jason and Kim, Dong-Ki and Smith, Max and Patrikar, Jay and Ghosal, Radhika and Wang, Youhui and Agha, Ali and Reddi, Vijay Janapa and Omidshafiei, Shayegan},
	month = oct,
	year = {2025},
	note = {arXiv:2510.08464 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\5YTN85M8\\Jabbour 等 - 2025 - Don't Run with Scissors Pruning Breaks VLA Models but They Can Be Recovered.pdf:application/pdf},
}

@misc{qu_spatialvla_2025,
	title = {{SpatialVLA}: {Exploring} {Spatial} {Representations} for {Visual}-{Language}-{Action} {Model}},
	shorttitle = {{SpatialVLA}},
	url = {http://arxiv.org/abs/2501.15830},
	doi = {10.48550/arXiv.2501.15830},
	abstract = {In this paper, we claim that spatial understanding is the keypoint in robot manipulation, and propose SpatialVLA to explore effective spatial representations for the robot foundation model. Specifically, we introduce Ego3D Position Encoding to inject 3D information into the input observations of the visual-language-action model, and propose Adaptive Action Grids to represent spatial robot movement actions with adaptive discretized action grids, facilitating learning generalizable and transferrable spatial action knowledge for cross-robot control. SpatialVLA is first pre-trained on top of a vision-language model with 1.1 Million real-world robot episodes, to learn a generalist manipulation policy across multiple robot environments and tasks. After pre-training, SpatialVLA is directly applied to perform numerous tasks in a zero-shot manner. The superior results in both simulation and real-world robots demonstrate its advantage of inferring complex robot motion trajectories and its strong in-domain multi-task generalization ability. We further show the proposed Adaptive Action Grids offer a new and effective way to fine-tune the pre-trained SpatialVLA model for new simulation and real-world setups, where the pre-learned action grids are re-discretized to capture robot-specific spatial action movements of new setups. The superior results from extensive evaluations demonstrate the exceptional in-distribution generalization and out-of-distribution adaptation capability, highlighting the crucial benefit of the proposed spatial-aware representations for generalist robot policy learning. All the details and codes will be open-sourced.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Qu, Delin and Song, Haoming and Chen, Qizhi and Yao, Yuanqi and Ye, Xinyi and Ding, Yan and Wang, Zhigang and Gu, JiaYuan and Zhao, Bin and Wang, Dong and Li, Xuelong},
	month = may,
	year = {2025},
	note = {arXiv:2501.15830 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\YDZV3IE2\\Qu 等 - 2025 - SpatialVLA Exploring Spatial Representations for Visual-Language-Action Model.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\XER3IG8A\\2501.html:text/html},
}

@misc{neary_improving_2025,
	title = {Improving {Pre}-{Trained} {Vision}-{Language}-{Action} {Policies} with {Model}-{Based} {Search}},
	url = {http://arxiv.org/abs/2508.12211},
	doi = {10.48550/arXiv.2508.12211},
	abstract = {Pre-trained vision-language-action (VLA) models offer a promising foundation for generalist robot policies, but often produce brittle behaviors or unsafe failures when deployed zero-shot in out-of-distribution scenarios. We present Vision-Language-Action Planning \& Search (VLAPS) -- a novel framework and accompanying algorithms that embed model-based search into the inference procedure of pre-trained VLA policies to improve their performance on robotic tasks. Specifically, our method biases a modified Monte Carlo Tree Search (MCTS) algorithm -- run using a model of the target environment -- using action priors defined by the VLA policy. By using VLA-derived abstractions and priors in model-based search, VLAPS efficiently explores language-conditioned robotics tasks whose search spaces would otherwise be intractably large. Conversely, by integrating model-based search with the VLA policy's inference procedure, VLAPS yields behaviors that are more performant than those obtained by directly following the VLA policy's action predictions. VLAPS offers a principled framework to: i) control test-time compute in VLA models, ii) leverage a priori knowledge of the robotic environment, and iii) integrate established planning and reinforcement learning techniques into the VLA inference process. Across all experiments, VLAPS significantly outperforms VLA-only baselines on language-specified tasks that would otherwise be intractable for uninformed search algorithms, increasing success rates by as much as 67 percentage points.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Neary, Cyrus and Younis, Omar G. and Kuramshin, Artur and Aslan, Ozgur and Berseth, Glen},
	month = nov,
	year = {2025},
	note = {arXiv:2508.12211 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\3RZ6IA9D\\Neary 等 - 2025 - Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search.pdf:application/pdf},
}

@misc{bi_vla-touch_2025,
	title = {{VLA}-{Touch}: {Enhancing} {Vision}-{Language}-{Action} {Models} with {Dual}-{Level} {Tactile} {Feedback}},
	shorttitle = {{VLA}-{Touch}},
	url = {http://arxiv.org/abs/2507.17294},
	doi = {10.48550/arXiv.2507.17294},
	abstract = {Tactile feedback is generally recognized to be crucial for effective interaction with the physical world. However, state-of-the-art Vision-Language-Action (VLA) models lack the ability to interpret and use tactile signals, limiting their effectiveness in contact-rich tasks. Incorporating tactile feedback into these systems is challenging due to the absence of large multi-modal datasets. We present VLA-Touch, an approach that enhances generalist robot policies with tactile sensing {\textbackslash}emph\{without fine-tuning\} the base VLA. Our method introduces two key innovations: (1) a pipeline that leverages a pretrained tactile-language model that provides semantic tactile feedback for high-level task planning, and (2) a diffusion-based controller that refines VLA-generated actions with tactile signals for contact-rich manipulation. Through real-world experiments, we demonstrate that our dual-level integration of tactile feedback improves task planning efficiency while enhancing execution precision. Code is open-sourced at {\textbackslash}href\{https://github.com/jxbi1010/VLA-Touch\}\{this URL\}.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Bi, Jianxin and Ma, Kevin Yuchen and Hao, Ce and Shou, Mike Zheng and Soh, Harold},
	month = jul,
	year = {2025},
	note = {arXiv:2507.17294 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\3KW994MU\\Bi 等 - 2025 - VLA-Touch Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback.pdf:application/pdf},
}

@misc{wang_vq-vla_2025,
	title = {{VQ}-{VLA}: {Improving} {Vision}-{Language}-{Action} {Models} via {Scaling} {Vector}-{Quantized} {Action} {Tokenizers}},
	shorttitle = {{VQ}-{VLA}},
	url = {http://arxiv.org/abs/2507.01016},
	doi = {10.48550/arXiv.2507.01016},
	abstract = {In this paper, we introduce an innovative vector quantization based action tokenizer built upon the largest-scale action trajectory dataset to date, leveraging over 100 times more data than previous approaches. This extensive dataset enables our tokenizer to capture rich spatiotemporal dynamics, resulting in a model that not only accelerates inference but also generates smoother and more coherent action outputs. Once trained, the tokenizer can be seamlessly adapted to a wide range of downstream tasks in a zero-shot manner, from short-horizon reactive behaviors to long-horizon planning. A key finding of our work is that the domain gap between synthetic and real action trajectories is marginal, allowing us to effectively utilize a vast amount of synthetic data during training without compromising real-world performance. To validate our approach, we conducted extensive experiments in both simulated environments and on real robotic platforms. The results demonstrate that as the volume of synthetic trajectory data increases, the performance of our tokenizer on downstream tasks improves significantly-most notably, achieving up to a 30\% higher success rate on two real-world tasks in long-horizon scenarios. These findings highlight the potential of our action tokenizer as a robust and scalable solution for real-time embodied intelligence systems, paving the way for more efficient and reliable robotic control in diverse application domains.Project website: https://xiaoxiao0406.github.io/vqvla.github.io},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Wang, Yating and Zhu, Haoyi and Liu, Mingyu and Yang, Jiange and Fang, Hao-Shu and He, Tong},
	month = jul,
	year = {2025},
	note = {arXiv:2507.01016 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\G9BZL6ZA\\Wang 等 - 2025 - VQ-VLA Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers.pdf:application/pdf},
}

@misc{feng_spatial-aware_2025,
	title = {Spatial-{Aware} {VLA} {Pretraining} through {Visual}-{Physical} {Alignment} from {Human} {Videos}},
	url = {http://arxiv.org/abs/2512.13080},
	doi = {10.48550/arXiv.2512.13080},
	abstract = {Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Feng, Yicheng and Zhang, Wanpeng and Wang, Ye and Luo, Hao and Yuan, Haoqi and Zheng, Sipeng and Lu, Zongqing},
	month = dec,
	year = {2025},
	note = {arXiv:2512.13080 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\VW5DQXM5\\Feng 等 - 2025 - Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos.pdf:application/pdf},
}

@misc{guo_vdrive_2025,
	title = {{VDRive}: {Leveraging} {Reinforced} {VLA} and {Diffusion} {Policy} for {End}-to-end {Autonomous} {Driving}},
	shorttitle = {{VDRive}},
	url = {http://arxiv.org/abs/2510.15446},
	doi = {10.48550/arXiv.2510.15446},
	abstract = {In autonomous driving, dynamic environment and corner cases pose significant challenges to the robustness of ego vehicle's state understanding and decision making. We introduce VDRive, a novel pipeline for end-to-end autonomous driving that explicitly models state-action mapping to address these challenges, enabling interpretable and robust decision making. By leveraging the advancement of the state understanding of the Vision Language Action Model (VLA) with generative diffusion policy-based action head, our VDRive guides the driving contextually and geometrically. Contextually, VLA predicts future observations through token generation pre-training, where the observations are represented as discrete codes by a Conditional Vector Quantized Variational Autoencoder (CVQ-VAE). Geometrically, we perform reinforcement learning fine-tuning of the VLA to predict future trajectories and actions based on current driving conditions. VLA supplies the current state tokens and predicted state tokens for the action policy head to generate hierarchical actions and trajectories. During policy training, a learned critic evaluates the actions generated by the policy and provides gradient-based feedback, forming an actor-critic framework that enables a reinforcement-based policy learning pipeline. Experiments show that our VDRive achieves state-of-the-art performance in the Bench2Drive closed-loop benchmark and nuScenes open-loop planning.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Guo, Ziang and Zhang, Zufeng},
	month = oct,
	year = {2025},
	note = {arXiv:2510.15446 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\RGSKNW74\\Guo和Zhang - 2025 - VDRive Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving.pdf:application/pdf},
}

@misc{wen_llada-vla_2025,
	title = {{LLaDA}-{VLA}: {Vision} {Language} {Diffusion} {Action} {Models}},
	shorttitle = {{LLaDA}-{VLA}},
	url = {http://arxiv.org/abs/2509.06932},
	doi = {10.48550/arXiv.2509.06932},
	abstract = {The rapid progress of auto-regressive vision-language models (VLMs) has inspired growing interest in vision-language-action models (VLA) for robotic manipulation. Recently, masked diffusion models, a paradigm distinct from autoregressive models, have begun to demonstrate competitive performance in text generation and multimodal applications, leading to the development of a series of diffusion-based VLMs (d-VLMs). However, leveraging such models for robot policy learning remains largely unexplored. In this work, we present LLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon pretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to robotic domain, we introduce two key designs: (1) a localized special-token classification strategy that replaces full-vocabulary classification with special action token classification, reducing adaptation difficulty; (2) a hierarchical action-structured decoding strategy that decodes action sequences hierarchically considering the dependencies within and across actions. Extensive experiments demonstrate that LLaDA-VLA significantly outperforms state-of-the-art VLAs on both simulation and real-world robots.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Wen, Yuqing and Li, Hebei and Gu, Kefan and Zhao, Yucheng and Wang, Tiancai and Sun, Xiaoyan},
	month = sep,
	year = {2025},
	note = {arXiv:2509.06932 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\L6G6DVCE\\Wen 等 - 2025 - LLaDA-VLA Vision Language Diffusion Action Models.pdf:application/pdf},
}

@misc{zhang_reasoning-vla_2025,
	title = {Reasoning-{VLA}: {A} {Fast} and {General} {Vision}-{Language}-{Action} {Reasoning} {Model} for {Autonomous} {Driving}},
	shorttitle = {Reasoning-{VLA}},
	url = {http://arxiv.org/abs/2511.19912},
	doi = {10.48550/arXiv.2511.19912},
	abstract = {Vision-Language-Action (VLA) models have recently shown strong decision-making capabilities in autonomous driving. However, existing VLAs often struggle with achieving efficient inference and generalizing to novel autonomous vehicle configurations and driving scenarios. In this paper, we propose Reasoning-VLA, a general and fast action-generation VLA framework. The proposed model employs a set of learnable action queries, initialized via Gaussian sampling from ground-truth trajectories within the training corpus. These learnable queries interact with reasoning-enhanced vision-language features to generate continuous action trajectories in parallel. To promote robust generalization, we consolidate eight publicly available autonomous driving datasets into a standardized, Chain-of-Thought reasoning-based, and easy-to-use data format for model training. Leveraging both supervised learning and reinforcement learning fine-tuning, extensive empirical evaluations across multiple benchmarks demonstrate that Reasoning-VLA achieves state-of-the-art performance, superior generalization capability, and the excellent inference speed reported to date.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Dapeng and Yuan, Zhenlong and Chen, Zhangquan and Liao, Chih-Ting and Chen, Yinda and Shen, Fei and Zhou, Qingguo and Chua, Tat-Seng},
	month = nov,
	year = {2025},
	note = {arXiv:2511.19912 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\CULINBLP\\Zhang 等 - 2025 - Reasoning-VLA A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving.pdf:application/pdf},
}

@misc{ye_dream-vl_2026,
	title = {Dream-{VL} \& {Dream}-{VLA}: {Open} {Vision}-{Language} and {Vision}-{Language}-{Action} {Models} with {Diffusion} {Language} {Model} {Backbone}},
	shorttitle = {Dream-{VL} \& {Dream}-{VLA}},
	url = {http://arxiv.org/abs/2512.22615},
	doi = {10.48550/arXiv.2512.22615},
	abstract = {While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2\% average success rate on LIBERO, 71.4\% overall average on SimplerEnv-Bridge, and 60.5\% overall average on SimplerEnv-Fractal, surpassing leading models such as \$\pi \_0\$ and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Ye, Jiacheng and Gong, Shansan and Gao, Jiahui and Fan, Junming and Wu, Shuang and Bi, Wei and Bai, Haoli and Shang, Lifeng and Kong, Lingpeng},
	month = jan,
	year = {2026},
	note = {arXiv:2512.22615 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\3PJYQTC9\\Ye 等 - 2026 - Dream-VL & Dream-VLA Open Vision-Language and Vision-Language-Action Models with Diffusion Language.pdf:application/pdf},
}

@misc{shao_large_2025,
	title = {Large {VLM}-based {Vision}-{Language}-{Action} {Models} for {Robotic} {Manipulation}: {A} {Survey}},
	shorttitle = {Large {VLM}-based {Vision}-{Language}-{Action} {Models} for {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2508.13073},
	doi = {10.48550/arXiv.2508.13073},
	abstract = {Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Shao, Rui and Li, Wei and Zhang, Lingsen and Zhang, Renshan and Liu, Zhiyang and Chen, Ran and Nie, Liqiang},
	month = sep,
	year = {2025},
	note = {arXiv:2508.13073 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\RN8BNQVW\\Shao 等 - 2025 - Large VLM-based Vision-Language-Action Models for Robotic Manipulation A Survey.pdf:application/pdf},
}

@misc{li_map-vla_2025,
	title = {{MAP}-{VLA}: {Memory}-{Augmented} {Prompting} for {Vision}-{Language}-{Action} {Model} in {Robotic} {Manipulation}},
	shorttitle = {{MAP}-{VLA}},
	url = {http://arxiv.org/abs/2511.09516},
	doi = {10.48550/arXiv.2511.09516},
	abstract = {Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0\% absolute performance gains in the simulation benchmark and 25.0\% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Runhao and Guo, Wenkai and Wu, Zhenyu and Wang, Changyuan and Deng, Haoyuan and Weng, Zhenyu and Tan, Yap-Peng and Wang, Ziwei},
	month = nov,
	year = {2025},
	note = {arXiv:2511.09516 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\7UBB8VYN\\Li 等 - 2025 - MAP-VLA Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation.pdf:application/pdf},
}

@misc{liu_trackvla_2025,
	title = {{TrackVLA}++: {Unleashing} {Reasoning} and {Memory} {Capabilities} in {VLA} {Models} for {Embodied} {Visual} {Tracking}},
	shorttitle = {{TrackVLA}++},
	url = {http://arxiv.org/abs/2510.07134},
	doi = {10.48550/arXiv.2510.07134},
	abstract = {Embodied Visual Tracking (EVT) is a fundamental ability that underpins practical applications, such as companion robots, guidance robots and service assistants, where continuously following moving targets is essential. Recent advances have enabled language-guided tracking in complex and unstructured scenes. However, existing approaches lack explicit spatial reasoning and effective temporal memory, causing failures under severe occlusions or in the presence of similar-looking distractors. To address these challenges, we present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances embodied visual tracking with two key modules, a spatial reasoning mechanism and a Target Identification Memory (TIM). The reasoning module introduces a Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative position and encodes it as a compact polar-coordinate token for action prediction. Guided by these spatial priors, the TIM employs a gated update strategy to preserve long-horizon target memory, ensuring spatiotemporal consistency and mitigating target loss during extended occlusions. Extensive experiments show that TrackVLA++ achieves state-of-the-art performance on public benchmarks across both egocentric and multi-camera settings. On the challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong zero-shot generalization, enabling robust real-world tracking in dynamic and occluded scenarios.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Liu, Jiahang and Qi, Yunpeng and Zhang, Jiazhao and Li, Minghan and Wang, Shaoan and Wu, Kui and Ye, Hanjing and Zhang, Hong and Chen, Zhibo and Zhong, Fangwei and Zhang, Zhizheng and Wang, He},
	month = oct,
	year = {2025},
	note = {arXiv:2510.07134 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\J4WUFPNJ\\Liu 等 - 2025 - TrackVLA++ Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking.pdf:application/pdf},
}

@misc{zhang_ta-vla_2025,
	title = {{TA}-{VLA}: {Elucidating} the {Design} {Space} of {Torque}-aware {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{TA}-{VLA}},
	url = {http://arxiv.org/abs/2509.07962},
	doi = {10.48550/arXiv.2509.07962},
	abstract = {Many robotic manipulation tasks require sensing and responding to force signals such as torque to assess whether the task has been successfully completed and to enable closed-loop control. However, current Vision-Language-Action (VLA) models lack the ability to integrate such subtle physical feedback. In this work, we explore Torque-aware VLA models, aiming to bridge this gap by systematically studying the design space for incorporating torque signals into existing VLA architectures. We identify and evaluate several strategies, leading to three key findings. First, introducing torque adapters into the decoder consistently outperforms inserting them into the encoder.Third, inspired by joint prediction and planning paradigms in autonomous driving, we propose predicting torque as an auxiliary output, which further improves performance. This strategy encourages the model to build a physically grounded internal representation of interaction dynamics. Extensive quantitative and qualitative experiments across contact-rich manipulation benchmarks validate our findings.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Zongzheng and Xu, Haobo and Yang, Zhuo and Yue, Chenghao and Lin, Zehao and Gao, Huan-ang and Wang, Ziwei and Zhao, Hao},
	month = sep,
	year = {2025},
	note = {arXiv:2509.07962 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\KDZVZBWX\\Zhang 等 - 2025 - TA-VLA Elucidating the Design Space of Torque-aware Vision-Language-Action Models.pdf:application/pdf},
}

@misc{liu_what_2026,
	title = {What {Can} {RL} {Bring} to {VLA} {Generalization}? {An} {Empirical} {Study}},
	shorttitle = {What {Can} {RL} {Bring} to {VLA} {Generalization}?},
	url = {http://arxiv.org/abs/2505.19789},
	doi = {10.48550/arXiv.2505.19789},
	abstract = {Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. However, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. To address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at https://rlvla.github.io},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Liu, Jijia and Gao, Feng and Wei, Bingwen and Chen, Xinlei and Liao, Qingmin and Wu, Yi and Yu, Chao and Wang, Yu},
	month = jan,
	year = {2026},
	note = {arXiv:2505.19789 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\Z4I9C3QT\\Liu 等 - 2026 - What Can RL Bring to VLA Generalization An Empirical Study.pdf:application/pdf},
}

@misc{sapkota_vision-language-action_2026,
	title = {Vision-{Language}-{Action} ({VLA}) {Models}: {Concepts}, {Progress}, {Applications} and {Challenges}},
	shorttitle = {Vision-{Language}-{Action} ({VLA}) {Models}},
	url = {http://arxiv.org/abs/2505.04769},
	doi = {10.48550/arXiv.2505.04769},
	abstract = {Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as autonomous vehicles, medical and industrial robotics, precision agriculture, humanoid robotics, and augmented reality. We analyzed challenges and propose solutions including agentic adaptation and cross-embodiment planning. Furthermore, we outline a forward-looking roadmap where VLA models, VLMs, and agentic AI converge to strengthen socially aligned, adaptive, and general-purpose embodied agents. This work, is expected to serve as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. The project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Vision-Language-Action-Models-Concepts-Progress-Applications-and-Challenges. [Index Terms: Vision Language Action, VLA, Vision Language Models, VLMs, Action Tokenization, NLP]},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Sapkota, Ranjan and Cao, Yang and Roumeliotis, Konstantinos I. and Karkee, Manoj},
	month = jan,
	year = {2026},
	note = {arXiv:2505.04769 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\JS8SCTB5\\Sapkota 等 - 2026 - Vision-Language-Action (VLA) Models Concepts, Progress, Applications and Challenges.pdf:application/pdf},
}

@misc{jin_dual-actor_2025,
	title = {Dual-{Actor} {Fine}-{Tuning} of {VLA} {Models}: {A} {Talk}-and-{Tweak} {Human}-in-the-{Loop} {Approach}},
	shorttitle = {Dual-{Actor} {Fine}-{Tuning} of {VLA} {Models}},
	url = {http://arxiv.org/abs/2509.13774},
	doi = {10.48550/arXiv.2509.13774},
	abstract = {Vision-language-action (VLA) models demonstrate strong generalization in robotic manipulation but face challenges in complex, real-world tasks. While supervised fine-tuning with demonstrations is constrained by data quality, reinforcement learning (RL) offers a promising alternative. We propose a human-in-the-loop dual-actor fine-tuning framework grounded in RL. The framework integrates a primary actor for robust multi-task performance with a refinement actor for latent-space adaptation. Beyond standard physical interventions, we introduce a lightweight talk-and-tweak scheme that converts human corrections into semantically grounded language commands, thereby generating a new dataset for policy learning. In real-world multi-task experiments, our approach achieves 100\% success across three tasks within 101 minutes of online fine-tuning. For long-horizon tasks, it sustains a 50\% success rate over 12 consecutive operations. Furthermore, the framework scales effectively to multi-robot training, achieving up to a 2 times improvement in efficiency when using dual robots. The experiment videos are available at https://sites.google.com/view/hil-daft/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Jin, Piaopiao and Wang, Qi and Sun, Guokang and Cai, Ziwen and He, Pinjia and You, Yangwei},
	month = sep,
	year = {2025},
	note = {arXiv:2509.13774 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\YYFR28SE\\Jin 等 - 2025 - Dual-Actor Fine-Tuning of VLA Models A Talk-and-Tweak Human-in-the-Loop Approach.pdf:application/pdf},
}

@misc{zhao_vla2_2025,
	title = {{VLA}{\textasciicircum}2: {Empowering} {Vision}-{Language}-{Action} {Models} with an {Agentic} {Framework} for {Unseen} {Concept} {Manipulation}},
	shorttitle = {{VLA}{\textasciicircum}2},
	url = {http://arxiv.org/abs/2510.14902},
	doi = {10.48550/arXiv.2510.14902},
	abstract = {Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA{\textasciicircum}2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA{\textasciicircum}2 achieves a 44.2\% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2\% in all customized environments without any performance degradation on in-domain tasks. Project website: https://vla-2.github.io.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhao, Han and Zhang, Jiaxuan and Song, Wenxuan and Ding, Pengxiang and Wang, Donglin},
	month = oct,
	year = {2025},
	note = {arXiv:2510.14902 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\NL5WZ6AP\\Zhao 等 - 2025 - VLA^2 Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipul.pdf:application/pdf},
}

@misc{chen_conrft_2025,
	title = {{ConRFT}: {A} {Reinforced} {Fine}-tuning {Method} for {VLA} {Models} via {Consistency} {Policy}},
	shorttitle = {{ConRFT}},
	url = {http://arxiv.org/abs/2502.05450},
	doi = {10.48550/arXiv.2502.05450},
	abstract = {Vision-Language-Action (VLA) models have shown substantial potential in real-world robotic manipulation. However, fine-tuning these models through supervised learning struggles to achieve robust performance due to limited, inconsistent demonstrations, especially in contact-rich environments. In this paper, we propose a reinforced fine-tuning approach for VLA models, named ConRFT, which consists of offline and online fine-tuning with a unified consistency-based training objective, to address these challenges. In the offline stage, our method integrates behavior cloning and Q-learning to effectively extract policy from a small set of demonstrations and stabilize value estimating. In the online stage, the VLA model is further fine-tuned via consistency policy, with human interventions to ensure safe exploration and high sample efficiency. We evaluate our approach on eight diverse real-world manipulation tasks. It achieves an average success rate of 96.3\% within 45-90 minutes of online fine-tuning, outperforming prior supervised methods with a 144\% improvement in success rate and 1.9x shorter episode length. This work highlights the potential of integrating reinforcement learning to enhance the performance of VLA models for real-world robotic applications. Videos and code are available at our project website https://cccedric.github.io/conrft/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Chen, Yuhui and Tian, Shuai and Liu, Shugao and Zhou, Yingting and Li, Haoran and Zhao, Dongbin},
	month = apr,
	year = {2025},
	note = {arXiv:2502.05450 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\CRRH2B7V\\Chen 等 - 2025 - ConRFT A Reinforced Fine-tuning Method for VLA Models via Consistency Policy.pdf:application/pdf},
}

@misc{huang_graphcot-vla_2025,
	title = {{GraphCoT}-{VLA}: {A} {3D} {Spatial}-{Aware} {Reasoning} {Vision}-{Language}-{Action} {Model} for {Robotic} {Manipulation} with {Ambiguous} {Instructions}},
	shorttitle = {{GraphCoT}-{VLA}},
	url = {http://arxiv.org/abs/2508.07650},
	doi = {10.48550/arXiv.2508.07650},
	abstract = {Vision-language-action models have emerged as a crucial paradigm in robotic manipulation. However, existing VLA models exhibit notable limitations in handling ambiguous language instructions and unknown environmental states. Furthermore, their perception is largely constrained to static two-dimensional observations, lacking the capability to model three-dimensional interactions between the robot and its environment. To address these challenges, this paper proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's ability to interpret ambiguous instructions and improve task planning, we design a structured Chain-of-Thought reasoning module that integrates high-level task understanding and planning, failed task feedback, and low-level imaginative reasoning about future object positions and robot actions. Additionally, we construct a real-time updatable 3D Pose-Object graph, which captures the spatial configuration of robot joints and the topological relationships between objects in 3D space, enabling the model to better understand and manipulate their interactions. We further integrates a dropout hybrid reasoning strategy to achieve efficient control outputs. Experimental results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA significantly outperforms existing methods in terms of task success rate and response speed, exhibiting strong generalization and robustness in open environments and under uncertain instructions.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Huang, Helong and Cen, Min and Tan, Kai and Quan, Xingyue and Huang, Guowei and Zhang, Hong},
	month = aug,
	year = {2025},
	note = {arXiv:2508.07650 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\4A68J7Y9\\Huang 等 - 2025 - GraphCoT-VLA A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation wit.pdf:application/pdf},
}

@misc{zang_rlinf-vla_2025,
	title = {{RLinf}-{VLA}: {A} {Unified} and {Efficient} {Framework} for {VLA}+{RL} {Training}},
	shorttitle = {{RLinf}-{VLA}},
	url = {http://arxiv.org/abs/2510.06710},
	doi = {10.48550/arXiv.2510.06710},
	abstract = {Recent progress in vision and language foundation models has significantly advanced multimodal understanding, reasoning, and generation, inspiring a surge of interest in extending such capabilities to embodied settings through vision-language-action (VLA) models. Yet, most VLA models are still trained with supervised fine-tuning (SFT), which struggles to generalize under distribution shifts due to error accumulation. Reinforcement learning (RL) offers a promising alternative by directly optimizing task performance through interaction, but existing attempts remain fragmented and lack a unified platform for fair and systematic comparison across model architectures and algorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and efficient framework for scalable RL training of VLA models. The system adopts a highly flexible resource allocation design that addresses the challenge of integrating rendering, training, and inference in RL+VLA training. In particular, for GPU-parallelized simulators, RLinf-VLA implements a novel hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup in training. Through a unified interface, RLinf-VLA seamlessly supports diverse VLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g., PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a unified model achieves 98.11{\textbackslash}\% across 130 LIBERO tasks and 97.66{\textbackslash}\% across 25 ManiSkill tasks. Beyond empirical performance, our study distills a set of best practices for applying RL to VLA training and sheds light on emerging patterns in this integration. Furthermore, we present preliminary deployment on a real-world Franka robot, where RL-trained policies exhibit stronger generalization than those trained with SFT. We envision RLinf-VLA as a foundation to accelerate and standardize research on embodied intelligence.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zang, Hongzhi and Wei, Mingjie and Xu, Si and Wu, Yongji and Guo, Zhen and Wang, Yuanqing and Lin, Hao and Shi, Liangzhi and Xie, Yuqing and Xu, Zhexuan and Liu, Zhihao and Chen, Kang and Tang, Wenhao and Zhang, Quanlu and Zhang, Weinan and Yu, Chao and Wang, Yu},
	month = oct,
	year = {2025},
	note = {arXiv:2510.06710 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\Y2CBVQW8\\Zang 等 - 2025 - RLinf-VLA A Unified and Efficient Framework for VLA+RL Training.pdf:application/pdf},
}

@misc{tan_latent_2025,
	title = {Latent {Chain}-of-{Thought} {World} {Modeling} for {End}-to-{End} {Driving}},
	url = {http://arxiv.org/abs/2512.10226},
	doi = {10.48550/arXiv.2512.10226},
	abstract = {Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Tan, Shuhan and Chitta, Kashyap and Chen, Yuxiao and Tian, Ran and You, Yurong and Wang, Yan and Luo, Wenjie and Cao, Yulong and Krahenbuhl, Philipp and Pavone, Marco and Ivanovic, Boris},
	month = dec,
	year = {2025},
	note = {arXiv:2512.10226 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\7DILGN6V\\Tan 等 - 2025 - Latent Chain-of-Thought World Modeling for End-to-End Driving.pdf:application/pdf},
}

@misc{wang_specprune-vla_2025,
	title = {{SpecPrune}-{VLA}: {Accelerating} {Vision}-{Language}-{Action} {Models} via {Action}-{Aware} {Self}-{Speculative} {Pruning}},
	shorttitle = {{SpecPrune}-{VLA}},
	url = {http://arxiv.org/abs/2509.05614},
	doi = {10.48550/arXiv.2509.05614},
	abstract = {Pruning accelerates compute-bound models by reducing computation. Recently applied to Vision-Language-Action (VLA) models, existing methods prune tokens using only local info from current action, ignoring global context from prior actions, causing {\textgreater}20\% success rate drop and limited speedup. We observe high similarity across consecutive actions and propose leveraging both local (current) and global (past) info for smarter token selection. We introduce SpecPrune-VLA, a training-free method with two-level pruning and heuristic control: (1) Static pruning at action level: uses global history and local context to reduce visual tokens per action; (2) Dynamic pruning at layer level: prunes tokens per layer based on layer-specific importance; (3) Lightweight action-aware controller: classifies actions as coarse/fine-grained (by speed), adjusting pruning aggressiveness since fine-grained actions are pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs. OpenVLA-OFT, with negligible success rate loss.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Wang, Hanzhen and Xu, Jiaming and Pan, Jiayi and Zhou, Yongkang and Dai, Guohao},
	month = sep,
	year = {2025},
	note = {arXiv:2509.05614 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\Q5DT4EZR\\Wang 等 - 2025 - SpecPrune-VLA Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning.pdf:application/pdf},
}

@misc{lu_vla-rl_2025,
	title = {{VLA}-{RL}: {Towards} {Masterful} and {General} {Robotic} {Manipulation} with {Scalable} {Reinforcement} {Learning}},
	shorttitle = {{VLA}-{RL}},
	url = {http://arxiv.org/abs/2505.18719},
	doi = {10.48550/arXiv.2505.18719},
	abstract = {Recent high-capacity vision-language-action (VLA) models have demonstrated impressive performance on a range of robotic manipulation tasks by imitating human demonstrations. However, exploiting offline data with limited visited states will cause execution failure in out-of-distribution scenarios. Intuitively, an exploration-based method that improves on online collected data at test time could address this limitation. We present VLA-RL, an algorithmic and systematic framework that leverages online reinforcement learning (RL) to improve pretrained auto-regressive VLAs in downstream tasks. Within a unified perspective, we first introduce a trajectory-level RL formulation for auto-regressive VLA training, which models general robotic manipulation trajectory as multi-modal multi-turn conversation. To address the challenge of sparse rewards, we fine-tune a pretrained vision-language model as a robotic process reward model, which is trained on pseudo reward labels annotated on automatically extracted task segments. To scale up, we identify several implementation findings that improve the stability and efficiency including curriculum selection strategy, GPU-balanced vectorized environments, batch decoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest finetuned baseline by 4.5\% on 40 challenging robotic manipulation tasks in LIBERO, and even matches the performance of advanced commercial models such as \$\pi \_0\$-FAST. Notably, we observe that VLA-RL benefits from increased test-time optimization, indicating an early spark of inference scaling laws in robotics.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Lu, Guanxing and Guo, Wenkai and Zhang, Chubin and Zhou, Yuheng and Jiang, Haonan and Gao, Zifeng and Tang, Yansong and Wang, Ziwei},
	month = may,
	year = {2025},
	note = {arXiv:2505.18719 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\JI74BMT8\\Lu 等 - 2025 - VLA-RL Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning.pdf:application/pdf},
}

@misc{din_vision_2025,
	title = {Vision {Language} {Action} {Models} in {Robotic} {Manipulation}: {A} {Systematic} {Review}},
	shorttitle = {Vision {Language} {Action} {Models} in {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2507.10672},
	doi = {10.48550/arXiv.2507.10672},
	abstract = {Vision Language Action (VLA) models represent a transformative shift in robotics, with the aim of unifying visual perception, natural language understanding, and embodied control within a single learning framework. This review presents a comprehensive and forward-looking synthesis of the VLA paradigm, with a particular emphasis on robotic manipulation and instruction-driven autonomy. We comprehensively analyze 102 VLA models, 26 foundational datasets, and 12 simulation platforms that collectively shape the development and evaluation of VLAs models. These models are categorized into key architectural paradigms, each reflecting distinct strategies for integrating vision, language, and control in robotic systems. Foundational datasets are evaluated using a novel criterion based on task complexity, variety of modalities, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning. We introduce a two-dimensional characterization framework that organizes these datasets based on semantic richness and multimodal alignment, showing underexplored regions in the current data landscape. Simulation environments are evaluated for their effectiveness in generating large-scale data, as well as their ability to facilitate transfer from simulation to real-world settings and the variety of supported tasks. Using both academic and industrial contributions, we recognize ongoing challenges and outline strategic directions such as scalable pretraining protocols, modular architectural design, and robust multimodal alignment strategies. This review serves as both a technical reference and a conceptual roadmap for advancing embodiment and robotic control, providing insights that span from dataset generation to real world deployment of generalist robotic agents.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Din, Muhayy Ud and Akram, Waseem and Saoud, Lyes Saad and Rosell, Jan and Hussain, Irfan},
	month = jul,
	year = {2025},
	note = {arXiv:2507.10672 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\6IQ7M85S\\Din 等 - 2025 - Vision Language Action Models in Robotic Manipulation A Systematic Review.pdf:application/pdf},
}

@inproceedings{wang_spec-vla_2025,
	title = {Spec-vla: speculative decoding for vision-language-action models with relaxed acceptance},
	shorttitle = {Spec-vla},
	url = {https://aclanthology.org/2025.emnlp-main.1367/},
	urldate = {2026-02-04},
	booktitle = {Proceedings of the 2025 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Wang, Songsheng and Yu, Rucheng and Yuan, Zhihang and Yu, Chao and Gao, Feng and Wang, Yu and Wong, Derek F.},
	year = {2025},
	pages = {26916--26928},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\Z93FKEIZ\\Wang 等 - 2025 - Spec-vla speculative decoding for vision-language-action models with relaxed acceptance.pdf:application/pdf},
}

@misc{hao_driveaction_2025,
	title = {{DriveAction}: {A} {Benchmark} for {Exploring} {Human}-like {Driving} {Decisions} in {VLA} {Models}},
	shorttitle = {{DriveAction}},
	url = {http://arxiv.org/abs/2506.05667},
	doi = {10.48550/arXiv.2506.05667},
	abstract = {Vision-Language-Action (VLA) models have advanced autonomous driving, but existing benchmarks still lack scenario diversity, reliable action-level annotation, and evaluation protocols aligned with human preferences. To address these limitations, we introduce DriveAction, the first action-driven benchmark specifically designed for VLA models, comprising 16,185 QA pairs generated from 2,610 driving scenarios. DriveAction leverages real-world driving data proactively collected by drivers of autonomous vehicles to ensure broad and representative scenario coverage, offers high-level discrete action labels collected directly from drivers' actual driving operations, and implements an action-rooted tree-structured evaluation framework that explicitly links vision, language, and action tasks, supporting both comprehensive and task-specific assessment. Our experiments demonstrate that state-of-the-art vision-language models (VLMs) require both vision and language guidance for accurate action prediction: on average, accuracy drops by 3.3\% without vision input, by 4.1\% without language input, and by 8.0\% without either. Our evaluation supports precise identification of model bottlenecks with robust and consistent results, thus providing new insights and a rigorous foundation for advancing human-like decisions in autonomous driving.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Hao, Yuhan and Li, Zhengning and Sun, Lei and Wang, Weilong and Yi, Naixin and Song, Sheng and Qin, Caihong and Zhou, Mofan and Zhan, Yifei and Lang, Xianpeng},
	month = sep,
	year = {2025},
	note = {arXiv:2506.05667 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\QKQYCSQH\\Hao 等 - 2025 - DriveAction A Benchmark for Exploring Human-like Driving Decisions in VLA Models.pdf:application/pdf},
}

@misc{sun_geovla_2025,
	title = {{GeoVLA}: {Empowering} {3D} {Representations} in {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{GeoVLA}},
	url = {http://arxiv.org/abs/2508.09071},
	doi = {10.48550/arXiv.2508.09071},
	abstract = {Vision-Language-Action (VLA) models have emerged as a promising approach for enabling robots to follow language instructions and predict corresponding actions. However, current VLA models mainly rely on 2D visual inputs, neglecting the rich geometric information in the 3D physical world, which limits their spatial awareness and adaptability. In this paper, we present GeoVLA, a novel VLA framework that effectively integrates 3D information to advance robotic manipulation. It uses a vision-language model (VLM) to process images and language instructions,extracting fused vision-language embeddings. In parallel, it converts depth maps into point clouds and employs a customized point encoder, called Point Embedding Network, to generate 3D geometric embeddings independently. These produced embeddings are then concatenated and processed by our proposed spatial-aware action expert, called 3D-enhanced Action Expert, which combines information from different sensor modalities to produce precise action sequences. Through extensive experiments in both simulation and real-world environments, GeoVLA demonstrates superior performance and robustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2 simulation benchmarks and shows remarkable robustness in real-world tasks requiring height adaptability, scale awareness and viewpoint invariance.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Sun, Lin and Xie, Bin and Liu, Yingfei and Shi, Hao and Wang, Tiancai and Cao, Jiale},
	month = aug,
	year = {2025},
	note = {arXiv:2508.09071 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\AYU3S9Q7\\Sun 等 - 2025 - GeoVLA Empowering 3D Representations in Vision-Language-Action Models.pdf:application/pdf},
}

@misc{zhang_mole-vla_2025,
	title = {{MoLe}-{VLA}: {Dynamic} {Layer}-skipping {Vision} {Language} {Action} {Model} via {Mixture}-of-{Layers} for {Efficient} {Robot} {Manipulation}},
	shorttitle = {{MoLe}-{VLA}},
	url = {http://arxiv.org/abs/2503.20384},
	doi = {10.48550/arXiv.2503.20384},
	abstract = {Multimodal Large Language Models (MLLMs) excel in understanding complex language and visual data, enabling generalist robotic systems to interpret instructions and perform embodied tasks. Nevertheless, their real-world deployment is hindered by substantial computational and storage demands. Recent insights into the homogeneous patterns in the LLM layer have inspired sparsification techniques to address these challenges, such as early exit and token pruning. However, these methods often neglect the critical role of the final layers that encode the semantic information most relevant to downstream robotic tasks. Aligning with the recent breakthrough of the Shallow Brain Hypothesis (SBH) in neuroscience and the mixture of experts in model sparsification, we conceptualize each LLM layer as an expert and propose a Mixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe) architecture for dynamic LLM layer activation. We introduce a Spatial-Temporal Aware Router (STAR) for MoLe to selectively activate only parts of the layers based on the robot's current state, mimicking the brain's distinct signal pathways specialized for cognition and causal reasoning. Additionally, to compensate for the cognitive ability of LLMs lost in MoLe, we devise a Cognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the understanding of task demands and improves the generation of task-relevant action sequences by leveraging cognitive features. Extensive experiments conducted in both RLBench simulation and real-world environments demonstrate the superiority of MoLe-VLA in both efficiency and performance. Specifically, MoLe-VLA achieves an 8\% improvement in the mean success rate across ten tasks while reducing computational costs by up to x5.6 compared to standard LLMs.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Rongyu and Dong, Menghang and Zhang, Yuan and Heng, Liang and Chi, Xiaowei and Dai, Gaole and Du, Li and Du, Yuan and Zhang, Shanghang},
	month = apr,
	year = {2025},
	note = {arXiv:2503.20384 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\9LW299S2\\Zhang 等 - 2025 - MoLe-VLA Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Ro.pdf:application/pdf},
}

@misc{xiong_hypervla_2025,
	title = {{HyperVLA}: {Efficient} {Inference} in {Vision}-{Language}-{Action} {Models} via {Hypernetworks}},
	shorttitle = {{HyperVLA}},
	url = {http://arxiv.org/abs/2510.04898},
	doi = {10.48550/arXiv.2510.04898},
	abstract = {Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by \$90{\textbackslash}times\$, and accelerates inference speed by \$120{\textbackslash}times\$. Code is publicly available at https://github.com/MasterXiong/HyperVLA},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Xiong, Zheng and Li, Kang and Wang, Zilin and Jackson, Matthew and Foerster, Jakob and Whiteson, Shimon},
	month = oct,
	year = {2025},
	note = {arXiv:2510.04898 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\RS6Z7TX2\\Xiong 等 - 2025 - HyperVLA Efficient Inference in Vision-Language-Action Models via Hypernetworks.pdf:application/pdf},
}

@misc{li_towards_2025,
	title = {Towards {Deploying} {VLA} without {Fine}-{Tuning}: {Plug}-and-{Play} {Inference}-{Time} {VLA} {Policy} {Steering} via {Embodied} {Evolutionary} {Diffusion}},
	shorttitle = {Towards {Deploying} {VLA} without {Fine}-{Tuning}},
	url = {http://arxiv.org/abs/2511.14178},
	doi = {10.48550/arXiv.2511.14178},
	abstract = {Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: https://rip4kobe.github.io/vla-pilot/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Zhuo and Liu, Junjia and Dong, Zhipeng and Teng, Tao and Rouxel, Quentin and Caldwell, Darwin and Chen, Fei},
	month = nov,
	year = {2025},
	note = {arXiv:2511.14178 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\SDWAMQ95\\Li 等 - 2025 - Towards Deploying VLA without Fine-Tuning Plug-and-Play Inference-Time VLA Policy Steering via Embo.pdf:application/pdf},
}

@misc{guo_improving_2025,
	title = {Improving {Vision}-{Language}-{Action} {Model} with {Online} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2501.16664},
	doi = {10.48550/arXiv.2501.16664},
	abstract = {Recent studies have successfully integrated large vision-language models (VLMs) into low-level robotic control by supervised fine-tuning (SFT) with expert robotic datasets, resulting in what we term vision-language-action (VLA) models. Although the VLA models are powerful, how to improve these large models during interaction with environments remains an open question. In this paper, we explore how to further improve these VLA models via Reinforcement Learning (RL), a commonly used fine-tuning technique for large models. However, we find that directly applying online RL to large VLA models presents significant challenges, including training instability that severely impacts the performance of large models, and computing burdens that exceed the capabilities of most local machines. To address these challenges, we propose iRe-VLA framework, which iterates between Reinforcement Learning and Supervised Learning to effectively improve VLA models, leveraging the exploratory benefits of RL while maintaining the stability of supervised learning. Experiments in two simulated benchmarks and a real-world manipulation suite validate the effectiveness of our method.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Guo, Yanjiang and Zhang, Jianke and Chen, Xiaoyu and Ji, Xiang and Wang, Yen-Jen and Hu, Yucheng and Chen, Jianyu},
	month = jan,
	year = {2025},
	note = {arXiv:2501.16664 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\6HMJERDE\\Guo 等 - 2025 - Improving Vision-Language-Action Model with Online Reinforcement Learning.pdf:application/pdf},
}

@misc{fei_libero-plus_2025,
	title = {{LIBERO}-{Plus}: {In}-depth {Robustness} {Analysis} of {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{LIBERO}-{Plus}},
	url = {http://arxiv.org/abs/2510.13626},
	doi = {10.48550/arXiv.2510.13626},
	abstract = {Visual-Language-Action (VLA) models report impressive success rates on robotic manipulation benchmarks, yet these results may mask fundamental weaknesses in robustness. We perform a systematic vulnerability analysis by introducing controlled perturbations across seven dimensions: objects layout, camera viewpoints, robot initial states, language instructions, light conditions, background textures and sensor noise. We comprehensively analyzed multiple state-of-the-art models and revealed consistent brittleness beneath apparent competence. Our analysis exposes critical weaknesses: models exhibit extreme sensitivity to perturbation factors, including camera viewpoints and robot initial states, with performance dropping from 95\% to below 30\% under modest perturbations. Surprisingly, models are largely insensitive to language variations, with further experiments revealing that models tend to ignore language instructions completely. Our findings challenge the assumption that high benchmark scores equate to true competency and highlight the need for evaluation practices that assess reliability under realistic variation.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Fei, Senyu and Wang, Siyin and Shi, Junhao and Dai, Zihao and Cai, Jikun and Qian, Pengfang and Ji, Li and He, Xinzhe and Zhang, Shiduo and Fei, Zhaoye and Fu, Jinlan and Gong, Jingjing and Qiu, Xipeng},
	month = dec,
	year = {2025},
	note = {arXiv:2510.13626 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\UZILIN3I\\Fei 等 - 2025 - LIBERO-Plus In-depth Robustness Analysis of Vision-Language-Action Models.pdf:application/pdf},
}

@misc{li_mimicdreamer_2025,
	title = {{MimicDreamer}: {Aligning} {Human} and {Robot} {Demonstrations} for {Scalable} {VLA} {Training}},
	shorttitle = {{MimicDreamer}},
	url = {http://arxiv.org/abs/2509.22199},
	doi = {10.48550/arXiv.2509.22199},
	abstract = {Vision Language Action (VLA) models derive their generalization capability from diverse training data, yet collecting embodied robot interaction data remains prohibitively expensive. In contrast, human demonstration videos are far more scalable and cost-efficient to collect, and recent studies confirm their effectiveness in training VLA models. However, a significant domain gap persists between human videos and robot-executed videos, including unstable camera viewpoints, visual discrepancies between human hands and robotic arms, and differences in motion dynamics. To bridge this gap, we propose MimicDreamer, a framework that turns fast, low-cost human demonstrations into robot-usable supervision by jointly aligning vision, viewpoint, and actions to directly support policy training. For visual alignment, we propose H2R Aligner, a video diffusion model that generates high-fidelity robot demonstration videos by transferring motion from human manipulation footage. For viewpoint stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos via homography and inpaints occlusions and distortions caused by warping. For action alignment, we map human hand trajectories to the robot frame and apply a constrained inverse kinematics solver to produce feasible, low-jitter joint commands with accurate pose tracking. Empirically, VLA models trained purely on our synthesized human-to-robot videos achieve few-shot execution on real robots. Moreover, scaling training with human data significantly boosts performance compared to models trained solely on real robot data; our approach improves the average success rate by 14.7{\textbackslash}\% across six representative manipulation tasks.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Haoyun and Zhang, Ivan and Ouyang, Runqi and Wang, Xiaofeng and Zhu, Zheng and Yang, Zhiqin and Zhang, Zhentao and Wang, Boyuan and Ni, Chaojun and Qin, Wenkang and Chen, Xinze and Ye, Yun and Huang, Guan and Song, Zhenbo and Wang, Xingang},
	month = sep,
	year = {2025},
	note = {arXiv:2509.22199 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\7X7JKWJE\\Li 等 - 2025 - MimicDreamer Aligning Human and Robot Demonstrations for Scalable VLA Training.pdf:application/pdf},
}

@misc{xiang_vla_2025,
	title = {{VLA} {Model}-{Expert} {Collaboration} for {Bi}-directional {Manipulation} {Learning}},
	url = {http://arxiv.org/abs/2503.04163},
	doi = {10.48550/arXiv.2503.04163},
	abstract = {The emergence of vision-language-action (VLA) models has given rise to foundation models for robot manipulation. Although these models have achieved significant improvements, their generalization in multi-task manipulation remains limited. This study proposes a VLA model-expert collaboration framework that leverages a limited number of expert actions to enhance VLA model performance. This approach reduces expert workload relative to manual operation while simultaneously improving the reliability and generalization of VLA models. Furthermore, manipulation data collected during collaboration can further refine the VLA model, while human participants concurrently enhance their skills. This bi-directional learning loop boosts the overall performance of the collaboration system. Experimental results across various VLA models demonstrate the effectiveness of the proposed system in collaborative manipulation and learning, as evidenced by improved success rates across tasks. Additionally, validation using a brain-computer interface (BCI) indicates that the collaboration system enhances the efficiency of low-speed action systems by involving VLA model during manipulation. These promising results pave the way for advancing human-robot interaction in the era of foundation models for robotics. (Project website: https://aoqunjin.github.io/Expert-VLA/)},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Xiang, Tian-Yu and Jin, Ao-Qun and Zhou, Xiao-Hu and Gui, Mei-Jiang and Xie, Xiao-Liang and Liu, Shi-Qi and Wang, Shuang-Yi and Duang, Sheng-Bin and Wang, Si-Cheng and Lei, Zheng and Hou, Zeng-Guang},
	month = mar,
	year = {2025},
	note = {arXiv:2503.04163 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\XJVF8PFL\\Xiang 等 - 2025 - VLA Model-Expert Collaboration for Bi-directional Manipulation Learning.pdf:application/pdf},
}

@misc{peng_counterfactual_2025,
	title = {Counterfactual {VLA}: {Self}-{Reflective} {Vision}-{Language}-{Action} {Model} with {Adaptive} {Reasoning}},
	shorttitle = {Counterfactual {VLA}},
	url = {http://arxiv.org/abs/2512.24426},
	doi = {10.48550/arXiv.2512.24426},
	abstract = {Recent reasoning-augmented Vision-Language-Action (VLA) models have improved the interpretability of end-to-end autonomous driving by generating intermediate reasoning traces. Yet these models primarily describe what they perceive and intend to do, rarely questioning whether their planned actions are safe or appropriate. This work introduces Counterfactual VLA (CF-VLA), a self-reflective VLA framework that enables the model to reason about and revise its planned actions before execution. CF-VLA first generates time-segmented meta-actions that summarize driving intent, and then performs counterfactual reasoning conditioned on both the meta-actions and the visual context. This step simulates potential outcomes, identifies unsafe behaviors, and outputs corrected meta-actions that guide the final trajectory generation. To efficiently obtain such self-reflective capabilities, we propose a rollout-filter-label pipeline that mines high-value scenes from a base (non-counterfactual) VLA's rollouts and labels counterfactual reasoning traces for subsequent training rounds. Experiments on large-scale driving datasets show that CF-VLA improves trajectory accuracy by up to 17.6\%, enhances safety metrics by 20.5\%, and exhibits adaptive thinking: it only enables counterfactual reasoning in challenging scenarios. By transforming reasoning traces from one-shot descriptions to causal self-correction signals, CF-VLA takes a step toward self-reflective autonomous driving agents that learn to think before they act.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Peng, Zhenghao "Mark" and Ding, Wenhao and You, Yurong and Chen, Yuxiao and Luo, Wenjie and Tian, Thomas and Cao, Yulong and Sharma, Apoorva and Xu, Danfei and Ivanovic, Boris and Li, Boyi and Zhou, Bolei and Wang, Yan and Pavone, Marco},
	month = dec,
	year = {2025},
	note = {arXiv:2512.24426 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\IWR8QPAK\\Peng 等 - 2025 - Counterfactual VLA Self-Reflective Vision-Language-Action Model with Adaptive Reasoning.pdf:application/pdf},
}

@misc{wen_dvla_2025,
	title = {{dVLA}: {Diffusion} {Vision}-{Language}-{Action} {Model} with {Multimodal} {Chain}-of-{Thought}},
	shorttitle = {{dVLA}},
	url = {http://arxiv.org/abs/2509.25681},
	doi = {10.48550/arXiv.2509.25681},
	abstract = {Vision-Language-Action (VLA) models are emerging as a next-generation paradigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages a multimodal chain-of-thought to unify visual perception, language reasoning, and robotic control in a single system. dVLA jointly optimizes perception, language understanding, and action under a single diffusion objective, enabling stronger cross-modal reasoning and better generalization to novel instructions and objects. For practical deployment, we mitigate inference latency by incorporating two acceleration strategies, a prefix attention mask and KV caching, yielding up to around times speedup at test-time inference. We evaluate dVLA in both simulation and the real world: on the LIBERO benchmark, it achieves state-of-the-art performance with a 96.4\% average success rate, consistently surpassing both discrete and continuous action policies; on a real Franka robot, it succeeds across a diverse task suite, including a challenging bin-picking task that requires multi-step planning, demonstrating robust real-world performance. Together, these results underscore the promise of unified diffusion frameworks for practical, high-performance VLA robotics.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Wen, Junjie and Zhu, Minjie and Liu, Jiaming and Liu, Zhiyuan and Yang, Yicun and Zhang, Linfeng and Zhang, Shanghang and Zhu, Yichen and Xu, Yi},
	month = sep,
	year = {2025},
	note = {arXiv:2509.25681 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\XNWEG3BH\\Wen 等 - 2025 - dVLA Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought.pdf:application/pdf},
}

@article{patratskiy_spatial_2025,
	title = {Spatial {Traces}: {Enhancing} {VLA} {Models} with {Spatial}-{Temporal} {Understanding}},
	volume = {34},
	issn = {1060-992X, 1934-7898},
	shorttitle = {Spatial {Traces}},
	url = {https://link.springer.com/10.3103/S1060992X25601654},
	doi = {10.3103/S1060992X25601654},
	language = {en},
	number = {S1},
	urldate = {2026-02-04},
	journal = {Optical Memory and Neural Networks},
	author = {Patratskiy, M. A. and Kovalev, A. K. and Panov, A. I.},
	month = dec,
	year = {2025},
	pages = {S72--S82},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\CVZZNU42\\Patratskiy 等 - 2025 - Spatial Traces Enhancing VLA Models with Spatial-Temporal Understanding.pdf:application/pdf},
}

@misc{team_gigaworld-0_2025,
	title = {{GigaWorld}-0: {World} {Models} as {Data} {Engine} to {Empower} {Embodied} {AI}},
	shorttitle = {{GigaWorld}-0},
	url = {http://arxiv.org/abs/2511.19861},
	doi = {10.48550/arXiv.2511.19861},
	abstract = {World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Team, GigaWorld and Ye, Angen and Wang, Boyuan and Ni, Chaojun and Huang, Guan and Zhao, Guosheng and Li, Haoyun and Zhu, Jiagang and Li, Kerui and Xu, Mengyuan and Deng, Qiuping and Wang, Siting and Qin, Wenkang and Chen, Xinze and Wang, Xiaofeng and Wang, Yankai and Cao, Yu and Chang, Yifan and Xu, Yuan and Ye, Yun and Wang, Yang and Zhou, Yukun and Zhang, Zhengyuan and Dong, Zhehao and Zhu, Zheng},
	month = nov,
	year = {2025},
	note = {arXiv:2511.19861 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\DB2F5GJY\\Team 等 - 2025 - GigaWorld-0 World Models as Data Engine to Empower Embodied AI.pdf:application/pdf},
}

@misc{huang_tactile-vla_2025,
	title = {Tactile-{VLA}: {Unlocking} {Vision}-{Language}-{Action} {Model}'s {Physical} {Knowledge} for {Tactile} {Generalization}},
	shorttitle = {Tactile-{VLA}},
	url = {http://arxiv.org/abs/2507.09160},
	doi = {10.48550/arXiv.2507.09160},
	abstract = {Vision-Language-Action (VLA) models have shown remarkable achievements, driven by the rich implicit knowledge of their vision-language components. However, achieving generalist robotic agents demands precise grounding into physical interactions, especially in contact-rich scenarios where fine-grained force control is essential. We advance VLAs' implicit knowledge beyond identifying what to do, towards guiding how to physically interact with real world. This paper introduces Tactile-VLA, a novel framework that deeply fuses vision, language, action, and tactile sensing. This framework incorporates a hybrid position-force controller to translate the model's intentions into precise physical actions and a reasoning module that allows the robot to adapt its strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's effectiveness and generalizability in three key aspects: (1) enabling tactile-aware instruction following, (2) utilizing tactile-relevant commonsense, and (3) facilitating adaptive tactile-involved reasoning. A key finding is that the VLM's prior knowledge already contains semantic understanding of physical interaction; by connecting it to the robot's tactile sensors with only a few demonstrations, we can activate this prior knowledge to achieve zero-shot generalization in contact-rich tasks.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Huang, Jialei and Wang, Shuo and Lin, Fanqi and Hu, Yihang and Wen, Chuan and Gao, Yang},
	month = jul,
	year = {2025},
	note = {arXiv:2507.09160 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\4Q6TUK8H\\Huang 等 - 2025 - Tactile-VLA Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization.pdf:application/pdf},
}

@misc{sendai_leave_2025,
	title = {Leave {No} {Observation} {Behind}: {Real}-time {Correction} for {VLA} {Action} {Chunks}},
	shorttitle = {Leave {No} {Observation} {Behind}},
	url = {http://arxiv.org/abs/2509.23224},
	doi = {10.48550/arXiv.2509.23224},
	abstract = {To improve efficiency and temporal coherence, Vision-Language-Action (VLA) models often predict action chunks; however, this action chunking harms reactivity under inference delay and long horizons. We introduce Asynchronous Action Chunk Correction (A2C2), which is a lightweight real-time chunk correction head that runs every control step and adds a time-aware correction to any off-the-shelf VLA's action chunk. The module combines the latest observation, the predicted action from VLA (base action), a positional feature that encodes the index of the base action within the chunk, and some features from the base policy, then outputs a per-step correction. This preserves the base model's competence while restoring closed-loop responsiveness. The approach requires no retraining of the base policy and is orthogonal to asynchronous execution schemes such as Real Time Chunking (RTC). On the dynamic Kinetix task suite (12 tasks) and LIBERO Spatial, our method yields consistent success rate improvements across increasing delays and execution horizons (+23\% point and +7\% point respectively, compared to RTC), and also improves robustness for long horizons even with zero injected delay. Since the correction head is small and fast, there is minimal overhead compared to the inference of large VLA models. These results indicate that A2C2 is an effective, plug-in mechanism for deploying high-capacity chunking policies in real-time control.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Sendai, Kohei and Alvarez, Maxime and Matsushima, Tatsuya and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = sep,
	year = {2025},
	note = {arXiv:2509.23224 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Systems and Control},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\3RJHZHHV\\Sendai 等 - 2025 - Leave No Observation Behind Real-time Correction for VLA Action Chunks.pdf:application/pdf},
}

@misc{zhang_4d-vla_2025,
	title = {{4D}-{VLA}: {Spatiotemporal} {Vision}-{Language}-{Action} {Pretraining} with {Cross}-{Scene} {Calibration}},
	shorttitle = {{4D}-{VLA}},
	url = {http://arxiv.org/abs/2506.22242},
	doi = {10.48550/arXiv.2506.22242},
	abstract = {Leveraging diverse robotic data for pretraining remains a critical challenge. Existing methods typically model the dataset's action distribution using simple observations as inputs. However, these inputs are often incomplete, resulting in a dispersed conditional action distribution-an issue we refer to as coordinate system chaos and state chaos. This inconsistency significantly hampers pretraining efficiency. To address this, we propose 4D-VLA, a novel approach that effectively integrates 4D information into the input to mitigate these sources of chaos. Our model introduces depth and temporal information into visual features with sequential RGB-D inputs, aligning the coordinate systems of the robot and the scene. This alignment endows the model with strong spatiotemporal reasoning capabilities while minimizing training overhead. Additionally, we introduce memory bank sampling, a frame sampling strategy designed to extract informative frames from historical images, further improving effectiveness and efficiency. Experimental results demonstrate that our pretraining method and architectural components substantially enhance model performance. In both simulated and real-world experiments, our model achieves a significant increase in success rate over OpenVLA. To further assess spatial perception and generalization to novel views, we introduce MV-Bench, a multi-view simulation benchmark. Our model consistently outperforms existing methods, demonstrating stronger spatial understanding and adaptability.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Jiahui and Chen, Yurui and Xu, Yueming and Huang, Ze and Zhou, Yanpeng and Yuan, Yu-Jie and Cai, Xinyue and Huang, Guowei and Quan, Xingyue and Xu, Hang and Zhang, Li},
	month = nov,
	year = {2025},
	note = {arXiv:2506.22242 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\8UX6JDVH\\Zhang 等 - 2025 - 4D-VLA Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration.pdf:application/pdf},
}

@misc{fu_mergevla_2025,
	title = {{MergeVLA}: {Cross}-{Skill} {Model} {Merging} {Toward} a {Generalist} {Vision}-{Language}-{Action} {Agent}},
	shorttitle = {{MergeVLA}},
	url = {http://arxiv.org/abs/2511.18810},
	doi = {10.48550/arXiv.2511.18810},
	abstract = {Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Fu, Yuxia and Zhang, Zhizhen and Zhang, Yuqi and Wang, Zijian and Huang, Zi and Luo, Yadan},
	month = nov,
	year = {2025},
	note = {arXiv:2511.18810 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\SSNVPQIM\\Fu 等 - 2025 - MergeVLA Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent.pdf:application/pdf},
}

@misc{chi_impromptu_2025,
	title = {Impromptu {VLA}: {Open} {Weights} and {Open} {Data} for {Driving} {Vision}-{Language}-{Action} {Models}},
	shorttitle = {Impromptu {VLA}},
	url = {http://arxiv.org/abs/2505.23757},
	doi = {10.48550/arXiv.2505.23757},
	abstract = {Vision-Language-Action (VLA) models for autonomous driving show promise but falter in unstructured corner case scenarios, largely due to a scarcity of targeted benchmarks. To address this, we introduce Impromptu VLA. Our core contribution is the Impromptu VLA Dataset: over 80,000 meticulously curated video clips, distilled from over 2M source clips sourced from 8 open-source large-scale datasets. This dataset is built upon our novel taxonomy of four challenging unstructured categories and features rich, planning-oriented question-answering annotations and action trajectories. Crucially, experiments demonstrate that VLAs trained with our dataset achieve substantial performance gains on established benchmarks--improving closed-loop NeuroNCAP scores and collision rates, and reaching near state-of-the-art L2 accuracy in open-loop nuScenes trajectory prediction. Furthermore, our Q\&A suite serves as an effective diagnostic, revealing clear VLM improvements in perception, prediction, and planning. Our code, data and models are available at https://github.com/ahydchh/Impromptu-VLA.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Chi, Haohan and Gao, Huan-ang and Liu, Ziming and Liu, Jianing and Liu, Chenyu and Li, Jinwei and Yang, Kaisen and Yu, Yangcheng and Wang, Zeda and Li, Wenyi and Wang, Leichen and Hu, Xingtao and Sun, Hao and Zhao, Hang and Zhao, Hao},
	month = may,
	year = {2025},
	note = {arXiv:2505.23757 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\4LLZDTC7\\Chi 等 - 2025 - Impromptu VLA Open Weights and Open Data for Driving Vision-Language-Action Models.pdf:application/pdf},
}

@misc{song_reconvla_2025,
	title = {{ReconVLA}: {Reconstructive} {Vision}-{Language}-{Action} {Model} as {Effective} {Robot} {Perceiver}},
	shorttitle = {{ReconVLA}},
	url = {http://arxiv.org/abs/2508.10333},
	doi = {10.48550/arXiv.2508.10333},
	abstract = {Recent advances in Vision-Language-Action (VLA) models have enabled robotic agents to integrate multimodal understanding with action execution. However, our empirical analysis reveals that current VLAs struggle to allocate visual attention to target regions. Instead, visual attention is always dispersed. To guide the visual attention grounding on the correct target, we propose ReconVLA, a reconstructive VLA model with an implicit grounding paradigm. Conditioned on the model's visual outputs, a diffusion transformer aims to reconstruct the gaze region of the image, which corresponds to the target manipulated objects. This process prompts the VLA model to learn fine-grained representations and accurately allocate visual attention, thus effectively leveraging task-specific visual information and conducting precise manipulation. Moreover, we curate a large-scale pretraining dataset comprising over 100k trajectories and 2 million data samples from open-source robotic datasets, further boosting the model's generalization in visual reconstruction. Extensive experiments in simulation and the real world demonstrate the superiority of our implicit grounding method, showcasing its capabilities of precise manipulation and generalization. Our project page is https://zionchow.github.io/ReconVLA/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Song, Wenxuan and Zhou, Ziyang and Zhao, Han and Chen, Jiayi and Ding, Pengxiang and Yan, Haodong and Huang, Yuxin and Tang, Feilong and Wang, Donglin and Li, Haoang},
	month = aug,
	year = {2025},
	note = {arXiv:2508.10333 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\LQKEPCBJ\\Song 等 - 2025 - ReconVLA Reconstructive Vision-Language-Action Model as Effective Robot Perceiver.pdf:application/pdf},
}

@misc{driess_knowledge_2025,
	title = {Knowledge {Insulating} {Vision}-{Language}-{Action} {Models}: {Train} {Fast}, {Run} {Fast}, {Generalize} {Better}},
	shorttitle = {Knowledge {Insulating} {Vision}-{Language}-{Action} {Models}},
	url = {http://arxiv.org/abs/2505.23705},
	doi = {10.48550/arXiv.2505.23705},
	abstract = {Vision-language-action (VLA) models provide a powerful approach to training control policies for physical systems, such as robots, by combining end-to-end learning with transfer of semantic knowledge from web-scale vision-language model (VLM) training. However, the constraints of real-time control are often at odds with the design of VLMs: the most powerful VLMs have tens or hundreds of billions of parameters, presenting an obstacle to real-time inference, and operate on discrete tokens rather than the continuous-valued outputs that are required for controlling robots. To address this challenge, recent VLA models have used specialized modules for efficient continuous control, such as action experts or continuous output heads, which typically require adding new untrained parameters to the pretrained VLM backbone. While these modules improve real-time and control capabilities, it remains an open question whether they preserve or degrade the semantic knowledge contained in the pretrained VLM, and what effect they have on the VLA training dynamics. In this paper, we study this question in the context of VLAs that include a continuous diffusion or flow matching action expert, showing that naively including such experts significantly harms both training speed and knowledge transfer. We provide an extensive analysis of various design choices, their impact on performance and knowledge transfer, and propose a technique for insulating the VLM backbone during VLA training that mitigates this issue. Videos are available at https://pi.website/research/knowledge\_insulation.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Driess, Danny and Springenberg, Jost Tobias and Ichter, Brian and Yu, Lili and Li-Bell, Adrian and Pertsch, Karl and Ren, Allen Z. and Walke, Homer and Vuong, Quan and Shi, Lucy Xiaoyang and Levine, Sergey},
	month = may,
	year = {2025},
	note = {arXiv:2505.23705 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\83H6X92B\\Driess 等 - 2025 - Knowledge Insulating Vision-Language-Action Models Train Fast, Run Fast, Generalize Better.pdf:application/pdf},
}

@misc{neau_grasp-vla_2025,
	title = {{GraSP}-{VLA}: {Graph}-based {Symbolic} {Action} {Representation} for {Long}-{Horizon} {Planning} with {VLA} {Policies}},
	shorttitle = {{GraSP}-{VLA}},
	url = {http://arxiv.org/abs/2511.04357},
	doi = {10.48550/arXiv.2511.04357},
	abstract = {Deploying autonomous robots that can learn new skills from demonstrations is an important challenge of modern robotics. Existing solutions often apply end-to-end imitation learning with Vision-Language Action (VLA) models or symbolic approaches with Action Model Learning (AML). On the one hand, current VLA models are limited by the lack of high-level symbolic planning, which hinders their abilities in long-horizon tasks. On the other hand, symbolic approaches in AML lack generalization and scalability perspectives. In this paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that uses a Continuous Scene Graph representation to generate a symbolic representation of human demonstrations. This representation is used to generate new planning domains during inference and serves as an orchestrator for low-level VLA policies, scaling up the number of actions that can be reproduced in a row. Our results show that GraSP-VLA is effective for modeling symbolic representations on the task of automatic planning domain generation from observations. In addition, results on real-world experiments show the potential of our Continuous Scene Graph representation to orchestrate low-level VLA policies in long-horizon tasks.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Neau, Maëlic and Falomir, Zoe and Santos, Paulo E. and Bosser, Anne-Gwenn and Buche, Cédric},
	month = nov,
	year = {2025},
	note = {arXiv:2511.04357 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\RT7QQ79H\\Neau 等 - 2025 - GraSP-VLA Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies.pdf:application/pdf},
}

@misc{song_ceed-vla_2025,
	title = {{CEED}-{VLA}: {Consistency} {Vision}-{Language}-{Action} {Model} with {Early}-{Exit} {Decoding}},
	shorttitle = {{CEED}-{VLA}},
	url = {http://arxiv.org/abs/2506.13725},
	doi = {10.48550/arXiv.2506.13725},
	abstract = {In recent years, Vision-Language-Action (VLA) models have become a vital research direction in robotics due to their impressive multimodal understanding and generalization capabilities. Despite the progress, their practical deployment is severely constrained by inference speed bottlenecks, particularly in high-frequency and dexterous manipulation tasks. While recent studies have explored Jacobi decoding as a more efficient alternative to traditional autoregressive decoding, its practical benefits are marginal due to the lengthy iterations. To address it, we introduce consistency distillation training to predict multiple correct action tokens in each iteration, thereby achieving acceleration. Besides, we design mixed-label supervision to mitigate the error accumulation during distillation. Although distillation brings acceptable speedup, we identify that certain inefficient iterations remain a critical bottleneck. To tackle this, we propose an early-exit decoding strategy that moderately relaxes convergence conditions, which further improves average inference efficiency. Experimental results show that the proposed method achieves more than 4 times inference acceleration across different baselines while maintaining high task success rates in both simulated and real-world robot tasks. These experiments validate that our approach provides an efficient and general paradigm for accelerating multimodal decision-making in robotics. Our project page is available at https://irpn-eai.github.io/CEED-VLA/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Song, Wenxuan and Chen, Jiayi and Ding, Pengxiang and Huang, Yuxin and Zhao, Han and Wang, Donglin and Li, Haoang},
	month = jun,
	year = {2025},
	note = {arXiv:2506.13725 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\2WSJ4ALN\\Song 等 - 2025 - CEED-VLA Consistency Vision-Language-Action Model with Early-Exit Decoding.pdf:application/pdf},
}

@misc{qian_wristworld_2025,
	title = {{WristWorld}: {Generating} {Wrist}-{Views} via {4D} {World} {Models} for {Robotic} {Manipulation}},
	shorttitle = {{WristWorld}},
	url = {http://arxiv.org/abs/2510.07313},
	doi = {10.48550/arXiv.2510.07313},
	abstract = {Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81\% and closing 42.4\% of the anchor-wrist view gap.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Qian, Zezhong and Chi, Xiaowei and Li, Yuming and Wang, Shizun and Qin, Zhiyuan and Ju, Xiaozhu and Han, Sirui and Zhang, Shanghang},
	month = oct,
	year = {2025},
	note = {arXiv:2510.07313 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\I665K7IR\\Qian 等 - 2025 - WristWorld Generating Wrist-Views via 4D World Models for Robotic Manipulation.pdf:application/pdf},
}

@misc{yu_forcevla_2025,
	title = {{ForceVLA}: {Enhancing} {VLA} {Models} with a {Force}-aware {MoE} for {Contact}-rich {Manipulation}},
	shorttitle = {{ForceVLA}},
	url = {http://arxiv.org/abs/2505.22159},
	doi = {10.48550/arXiv.2505.22159},
	abstract = {Vision-Language-Action (VLA) models have advanced general-purpose robotic manipulation by leveraging pretrained visual and linguistic representations. However, they struggle with contact-rich tasks that require fine-grained control involving force, especially under visual occlusion or dynamic uncertainty. To address these limitations, we propose ForceVLA, a novel end-to-end manipulation framework that treats external force sensing as a first-class modality within VLA systems. ForceVLA introduces FVLMoE, a force-aware Mixture-of-Experts fusion module that dynamically integrates pretrained visual-language embeddings with real-time 6-axis force feedback during action decoding. This enables context-aware routing across modality-specific experts, enhancing the robot's ability to adapt to subtle contact dynamics. We also introduce {\textbackslash}textbf\{ForceVLA-Data\}, a new dataset comprising synchronized vision, proprioception, and force-torque signals across five contact-rich manipulation tasks. ForceVLA improves average task success by 23.2\% over strong pi\_0-based baselines, achieving up to 80\% success in tasks such as plug insertion. Our approach highlights the importance of multimodal integration for dexterous manipulation and sets a new benchmark for physically intelligent robotic control. Code and data will be released at https://sites.google.com/view/forcevla2025.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Yu, Jiawen and Liu, Hairuo and Yu, Qiaojun and Ren, Jieji and Hao, Ce and Ding, Haitong and Huang, Guangyu and Huang, Guofan and Song, Yan and Cai, Panpan and Lu, Cewu and Zhang, Wenqiang},
	month = sep,
	year = {2025},
	note = {arXiv:2505.22159 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\AFK7MCSW\\Yu 等 - 2025 - ForceVLA Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation.pdf:application/pdf},
}

@misc{xu_stare-vla_2025,
	title = {{STARE}-{VLA}: {Progressive} {Stage}-{Aware} {Reinforcement} for {Fine}-{Tuning} {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{STARE}-{VLA}},
	url = {http://arxiv.org/abs/2512.05107},
	doi = {10.48550/arXiv.2512.05107},
	abstract = {Recent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -{\textgreater} Preference -{\textgreater} Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Xu, Feng and Zhai, Guangyao and Kong, Xin and Fu, Tingzhong and Gordon, Daniel F. N. and An, Xueli and Busam, Benjamin},
	month = dec,
	year = {2025},
	note = {arXiv:2512.05107 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\YBEBBWJ4\\Xu 等 - 2025 - STARE-VLA Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models.pdf:application/pdf},
}

@misc{singh_og-vla_2025,
	title = {{OG}-{VLA}: {Orthographic} {Image} {Generation} for {3D}-{Aware} {Vision}-{Language} {Action} {Model}},
	shorttitle = {{OG}-{VLA}},
	url = {http://arxiv.org/abs/2506.01196},
	doi = {10.48550/arXiv.2506.01196},
	abstract = {We introduce OG-VLA, a novel architecture and learning framework that combines the generalization strengths of Vision Language Action models (VLAs) with the robustness of 3D-aware policies. We address the challenge of mapping natural language instructions and one or more RGBD observations to quasi-static robot actions. 3D-aware robot policies achieve state-of-the-art performance on precise robot manipulation tasks, but struggle with generalization to unseen instructions, scenes, and objects. On the other hand, VLAs excel at generalizing across instructions and scenes, but can be sensitive to camera and robot pose variations. We leverage prior knowledge embedded in language and vision foundation models to improve generalization of 3D-aware keyframe policies. OG-VLA unprojects input observations from diverse views into a point cloud which is then rendered from canonical orthographic views, ensuring input view invariance and consistency between input and output spaces. These canonical views are processed with a vision backbone, a Large Language Model (LLM), and an image diffusion model to generate images that encode the next position and orientation of the end-effector on the input scene. Evaluations on the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization to unseen environments, with over 40\% relative improvements while maintaining robust performance in seen settings. We also show real-world adaption in 3 to 5 demonstrations along with strong generalization. Videos and resources at https://og-vla.github.io/},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Singh, Ishika and Goyal, Ankit and Birchfield, Stan and Fox, Dieter and Garg, Animesh and Blukis, Valts},
	month = nov,
	year = {2025},
	note = {arXiv:2506.01196 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\QA8FRL25\\Singh 等 - 2025 - OG-VLA Orthographic Image Generation for 3D-Aware Vision-Language Action Model.pdf:application/pdf},
}

@misc{zhang_balancing_2025,
	title = {Balancing {Signal} and {Variance}: {Adaptive} {Offline} {RL} {Post}-{Training} for {VLA} {Flow} {Models}},
	shorttitle = {Balancing {Signal} and {Variance}},
	url = {http://arxiv.org/abs/2509.04063},
	doi = {10.48550/arXiv.2509.04063},
	abstract = {Vision-Language-Action (VLA) models based on flow matching have shown excellent performance in general-purpose robotic manipulation tasks. However, the action accuracy of these models on complex downstream tasks is unsatisfactory. One important reason is that these models rely solely on the post-training paradigm of imitation learning, which makes it difficult to have a deeper understanding of the distribution properties of data quality, which is exactly what Reinforcement Learning (RL) excels at. In this paper, we theoretically propose an offline RL post-training objective for VLA flow models and induce an efficient and feasible offline RL fine-tuning algorithm -- Adaptive Reinforced Flow Matching (ARFM). By introducing an adaptively adjusted scaling factor in the VLA flow model loss, we construct a principled bias-variance trade-off objective function to optimally control the impact of RL signal on flow loss. ARFM adaptively balances RL advantage preservation and flow loss gradient variance control, resulting in a more stable and efficient fine-tuning process. Extensive simulation and real-world experimental results show that ARFM exhibits excellent generalization, robustness, few-shot learning, and continuous learning performance.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Hongyin and Zhang, Shiyuan and Jin, Junxi and Zeng, Qixin and Qiao, Yifan and Lu, Hongchao and Wang, Donglin},
	month = sep,
	year = {2025},
	note = {arXiv:2509.04063 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\D6832X28\\Zhang 等 - 2025 - Balancing Signal and Variance Adaptive Offline RL Post-Training for VLA Flow Models.pdf:application/pdf},
}

@misc{zhu_objectvla_2025,
	title = {{ObjectVLA}: {End}-to-{End} {Open}-{World} {Object} {Manipulation} {Without} {Demonstration}},
	shorttitle = {{ObjectVLA}},
	url = {http://arxiv.org/abs/2502.19250},
	doi = {10.48550/arXiv.2502.19250},
	abstract = {Imitation learning has proven to be highly effective in teaching robots dexterous manipulation skills. However, it typically relies on large amounts of human demonstration data, which limits its scalability and applicability in dynamic, real-world environments. One key challenge in this context is object generalization, where a robot trained to perform a task with one object, such as "hand over the apple," struggles to transfer its skills to a semantically similar but visually different object, such as "hand over the peach." This gap in generalization to new objects beyond those in the same category has yet to be adequately addressed in previous work on end-to-end visuomotor policy learning. In this paper, we present a simple yet effective approach for achieving object generalization through Vision-Language-Action (VLA) models, referred to as {\textbackslash}textbf\{ObjectVLA\}. Our model enables robots to generalize learned skills to novel objects without requiring explicit human demonstrations for each new target object. By leveraging vision-language pair data, our method provides a lightweight and scalable way to inject knowledge about the target object, establishing an implicit link between the object and the desired action. We evaluate ObjectVLA on a real robotic platform, demonstrating its ability to generalize across 100 novel objects with a 64{\textbackslash}\% success rate in selecting objects not seen during training. Furthermore, we propose a more accessible method for enhancing object generalization in VLA models, using a smartphone to capture a few images and fine-tune the pre-trained model. These results highlight the effectiveness of our approach in enabling object-level generalization and reducing the need for extensive human demonstrations, paving the way for more flexible and scalable robotic learning systems.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhu, Minjie and Zhu, Yichen and Li, Jinming and Zhou, Zhongyi and Wen, Junjie and Liu, Xiaoyu and Shen, Chaomin and Peng, Yaxin and Feng, Feifei},
	month = feb,
	year = {2025},
	note = {arXiv:2502.19250 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\ETF3FIA3\\Zhu 等 - 2025 - ObjectVLA End-to-End Open-World Object Manipulation Without Demonstration.pdf:application/pdf},
}

@misc{fang_sqap-vla_2025,
	title = {{SQAP}-{VLA}: {A} {Synergistic} {Quantization}-{Aware} {Pruning} {Framework} for {High}-{Performance} {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{SQAP}-{VLA}},
	url = {http://arxiv.org/abs/2509.09090},
	doi = {10.48550/arXiv.2509.09090},
	abstract = {Vision-Language-Action (VLA) models exhibit unprecedented capabilities for embodied intelligence. However, their extensive computational and memory costs hinder their practical deployment. Existing VLA compression and acceleration approaches conduct quantization or token pruning in an ad-hoc manner but fail to enable both for a holistic efficiency improvement due to an observed incompatibility. This work introduces SQAP-VLA, the first structured, training-free VLA inference acceleration framework that simultaneously enables state-of-the-art quantization and token pruning. We overcome the incompatibility by co-designing the quantization and token pruning pipeline, where we propose new quantization-aware token pruning criteria that work on an aggressively quantized model while improving the quantizer design to enhance pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields significant gains in computational efficiency and inference speed while successfully preserving core model performance, achieving a \${\textbackslash}times\$1.93 speedup and up to a 4.5{\textbackslash}\% average success rate enhancement compared to the original model.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Fang, Hengyu and Liu, Yijiang and Du, Yuan and Du, Li and Yang, Huanrui},
	month = sep,
	year = {2025},
	note = {arXiv:2509.09090 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\74M2Z8QN\\Fang 等 - 2025 - SQAP-VLA A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Ac.pdf:application/pdf},
}

@misc{zhang_align-then-steer_2025,
	title = {Align-{Then}-{stEer}: {Adapting} the {Vision}-{Language} {Action} {Models} through {Unified} {Latent} {Guidance}},
	shorttitle = {Align-{Then}-{stEer}},
	url = {http://arxiv.org/abs/2509.02055},
	doi = {10.48550/arXiv.2509.02055},
	abstract = {Vision-Language-Action (VLA) models pre-trained on large, diverse datasets show remarkable potential for general-purpose robotic manipulation. However, a primary bottleneck remains in adapting these models to downstream tasks, especially when the robot's embodiment or the task itself differs from the pre-training data. This discrepancy leads to a significant mismatch in action distributions, demanding extensive data and compute for effective fine-tuning. To address this challenge, we introduce {\textbackslash}textbf\{Align-Then-stEer ({\textbackslash}texttt\{ATE\})\}, a novel, data-efficient, and plug-and-play adaptation framework. {\textbackslash}texttt\{ATE\} first aligns disparate action spaces by constructing a unified latent space, where a variational autoencoder constrained by reverse KL divergence embeds adaptation actions into modes of the pre-training action latent distribution. Subsequently, it steers the diffusion- or flow-based VLA's generation process during fine-tuning via a guidance mechanism that pushes the model's output distribution towards the target domain. We conduct extensive experiments on cross-embodiment and cross-task manipulation in both simulation and real world. Compared to direct fine-tuning of representative VLAs, our method improves the average multi-task success rate by up to {\textbackslash}textbf\{9.8{\textbackslash}\%\} in simulation and achieves a striking {\textbackslash}textbf\{32{\textbackslash}\% success rate gain\} in a real-world cross-embodiment setting. Our work presents a general and lightweight solution that greatly enhances the practicality of deploying VLA models to new robotic platforms and tasks.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Yang and Wang, Chenwei and Lu, Ouyang and Zhao, Yuan and Ge, Yunfei and Sun, Zhenglong and Li, Xiu and Zhang, Chi and Bai, Chenjia and Li, Xuelong},
	month = sep,
	year = {2025},
	note = {arXiv:2509.02055 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\XJPPTYWJ\\Zhang 等 - 2025 - Align-Then-stEer Adapting the Vision-Language Action Models through Unified Latent Guidance.pdf:application/pdf},
}

@misc{li_cogvla_2025,
	title = {{CogVLA}: {Cognition}-{Aligned} {Vision}-{Language}-{Action} {Model} via {Instruction}-{Driven} {Routing} \& {Sparsification}},
	shorttitle = {{CogVLA}},
	url = {http://arxiv.org/abs/2508.21046},
	doi = {10.48550/arXiv.2508.21046},
	abstract = {Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4\% and 70.0\%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/CogVLA.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Wei and Zhang, Renshan and Shao, Rui and He, Jie and Nie, Liqiang},
	month = oct,
	year = {2025},
	note = {arXiv:2508.21046 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\39PKJSBG\\Li 等 - 2025 - CogVLA Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsificati.pdf:application/pdf},
}

@misc{li_reflection-based_2026,
	title = {Reflection-{Based} {Task} {Adaptation} for {Self}-{Improving} {VLA}},
	url = {http://arxiv.org/abs/2510.12710},
	doi = {10.48550/arXiv.2510.12710},
	abstract = {Pre-trained Vision-Language-Action (VLA) models represent a major leap towards general-purpose robots, yet efficiently adapting them to novel, specific tasks in-situ remains a significant hurdle. While reinforcement learning (RL) is a promising avenue for such adaptation, the process often suffers from low efficiency, hindering rapid task mastery. We introduce Reflective Self-Adaptation, a framework for rapid, autonomous task adaptation without human intervention. Our framework establishes a self-improving loop where the agent learns from its own experience to enhance both strategy and execution. The core of our framework is a dual-pathway architecture that addresses the full adaptation lifecycle. First, a Failure-Driven Reflective RL pathway enables rapid learning by using the VLM's causal reasoning to automatically synthesize a targeted, dense reward function from failure analysis. This provides a focused learning signal that significantly accelerates policy exploration. However, optimizing such proxy rewards introduces a potential risk of "reward hacking," where the agent masters the reward function but fails the actual task. To counteract this, our second pathway, Success-Driven Quality-Guided SFT, grounds the policy in holistic success. It identifies and selectively imitates high-quality successful trajectories, ensuring the agent remains aligned with the ultimate task goal. This pathway is strengthened by a conditional curriculum mechanism to aid initial exploration. We conduct experiments in challenging manipulation tasks. The results demonstrate that our framework achieves faster convergence and higher final success rates compared to representative baselines. Our work presents a robust solution for creating self-improving agents that can efficiently and reliably adapt to new environments.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Baicheng and Wu, Dong and Yan, Zike and Liu, Xinchen and Zeng, Zecui and Li, Lusong and Zha, Hongbin},
	month = jan,
	year = {2026},
	note = {arXiv:2510.12710 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\Z6V7VV8M\\Li 等 - 2026 - Reflection-Based Task Adaptation for Self-Improving VLA.pdf:application/pdf},
}

@misc{li_scalable_2025,
	title = {Scalable {Vision}-{Language}-{Action} {Model} {Pretraining} for {Robotic} {Manipulation} with {Real}-{Life} {Human} {Activity} {Videos}},
	url = {http://arxiv.org/abs/2510.21571},
	doi = {10.48550/arXiv.2510.21571},
	abstract = {This paper presents a novel approach for pretraining robotic manipulation Vision-Language-Action (VLA) models using a large corpus of unscripted real-life video recordings of human hand activities. Treating human hand as dexterous robot end-effector, we show that "in-the-wild" egocentric human videos without any annotations can be transformed into data formats fully aligned with existing robotic V-L-A training data in terms of task granularity and labels. This is achieved by the development of a fully-automated holistic human activity analysis approach for arbitrary human hand videos. This approach can generate atomic-level hand activity segments and their language descriptions, each accompanied with framewise 3D hand motion and camera motion. We process a large volume of egocentric videos and create a hand-VLA training dataset containing 1M episodes and 26M frames. This training data covers a wide range of objects and concepts, dexterous manipulation tasks, and environment variations in real life, vastly exceeding the coverage of existing robot data. We design a dexterous hand VLA model architecture and pretrain the model on this dataset. The model exhibits strong zero-shot capabilities on completely unseen real-world observations. Additionally, fine-tuning it on a small amount of real robot action data significantly improves task success rates and generalization to novel objects in real robotic experiments. We also demonstrate the appealing scaling behavior of the model's task performance with respect to pretraining data scale. We believe this work lays a solid foundation for scalable VLA pretraining, advancing robots toward truly generalizable embodied intelligence.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Qixiu and Deng, Yu and Liang, Yaobo and Luo, Lin and Zhou, Lei and Yao, Chengtang and Zeng, Lingqi and Feng, Zhiyuan and Liang, Huizhi and Xu, Sicheng and Zhang, Yizhong and Chen, Xi and Chen, Hao and Sun, Lily and Chen, Dong and Yang, Jiaolong and Guo, Baining},
	month = oct,
	year = {2025},
	note = {arXiv:2510.21571 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\RFQ2FJV9\\Li 等 - 2025 - Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Acti.pdf:application/pdf},
}

@article{dolgopolyi_bridging_2025,
	title = {Bridging {Perception}, {Language}, and {Action}: {A} {Survey} and {Bibliometric} {Analysis} of {VLM} \& {VLA} {Systems}},
	shorttitle = {Bridging {Perception}, {Language}, and {Action}},
	url = {https://www.researchsquare.com/article/rs-7935378/latest},
	urldate = {2026-02-04},
	author = {Dolgopolyi, Roman and Tsevas, Anastasios},
	year = {2025},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\9KBUKATV\\Dolgopolyi和Tsevas - 2025 - Bridging Perception, Language, and Action A Survey and Bibliometric Analysis of VLM & VLA Systems.pdf:application/pdf},
}

@misc{jang_contextvla_2025,
	title = {{ContextVLA}: {Vision}-{Language}-{Action} {Model} with {Amortized} {Multi}-{Frame} {Context}},
	shorttitle = {{ContextVLA}},
	url = {http://arxiv.org/abs/2510.04246},
	doi = {10.48550/arXiv.2510.04246},
	abstract = {Leveraging temporal context is crucial for success in partially observable robotic tasks. However, prior work in behavior cloning has demonstrated inconsistent performance gains when using multi-frame observations. In this paper, we introduce ContextVLA, a policy model that robustly improves robotic task performance by effectively leveraging multi-frame observations. Our approach is motivated by the key observation that Vision-Language-Action models (VLA), i.e., policy models built upon a Vision-Language Model (VLM), more effectively utilize multi-frame observations for action generation. This suggests that VLMs' inherent temporal understanding capability enables them to extract more meaningful context from multi-frame observations. However, the high dimensionality of video inputs introduces significant computational overhead, making VLA training and inference inefficient. To address this, ContextVLA compresses past observations into a single context token, allowing the policy to efficiently leverage temporal context for action generation. Our experiments show that ContextVLA consistently improves over single-frame VLAs and achieves the benefits of full multi-frame training but with reduced training and inference times.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Jang, Huiwon and Yu, Sihyun and Kwon, Heeseung and Jeon, Hojin and Seo, Younggyo and Shin, Jinwoo},
	month = oct,
	year = {2025},
	note = {arXiv:2510.04246 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\NAWJSTCM\\Jang 等 - 2025 - ContextVLA Vision-Language-Action Model with Amortized Multi-Frame Context.pdf:application/pdf},
}

@misc{park_acg_2025,
	title = {{ACG}: {Action} {Coherence} {Guidance} for {Flow}-based {VLA} models},
	shorttitle = {{ACG}},
	url = {http://arxiv.org/abs/2510.22201},
	doi = {10.48550/arXiv.2510.22201},
	abstract = {Diffusion and flow matching models have emerged as powerful robot policies, enabling Vision-Language-Action (VLA) models to generalize across diverse scenes and instructions. Yet, when trained via imitation learning, their high generative capacity makes them sensitive to noise in human demonstrations: jerks, pauses, and jitter which reduce action coherence. Reduced action coherence causes instability and trajectory drift during deployment, failures that are catastrophic in fine-grained manipulation where precision is crucial. In this paper, we present Action Coherence Guidance (ACG) for VLA models, a training-free test-time guidance algorithm that improves action coherence and thereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and real-world SO-101 tasks, ACG consistently improves action coherence and boosts success rates across diverse manipulation tasks. Code and project page are available at https://github.com/DAVIAN-Robotics/ACG and https://DAVIAN-Robotics.github.io/ACG , respectively.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Park, Minho and Kim, Kinam and Hyung, Junha and Jang, Hyojin and Jin, Hoiyeong and Yun, Jooyeol and Lee, Hojoon and Choo, Jaegul},
	month = oct,
	year = {2025},
	note = {arXiv:2510.22201 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\YLLHFGGK\\Park 等 - 2025 - ACG Action Coherence Guidance for Flow-based VLA models.pdf:application/pdf},
}

@misc{yang_beyond_2025,
	title = {Beyond {Human} {Demonstrations}: {Diffusion}-{Based} {Reinforcement} {Learning} to {Generate} {Data} for {VLA} {Training}},
	shorttitle = {Beyond {Human} {Demonstrations}},
	url = {http://arxiv.org/abs/2509.19752},
	doi = {10.48550/arXiv.2509.19752},
	abstract = {Vision-language-action (VLA) models have shown strong generalization across tasks and embodiments; however, their reliance on large-scale human demonstrations limits their scalability owing to the cost and effort of manual data collection. Reinforcement learning (RL) offers a potential alternative to generate demonstrations autonomously, yet conventional RL algorithms often struggle on long-horizon manipulation tasks with sparse rewards. In this paper, we propose a modified diffusion policy optimization algorithm to generate high-quality and low-variance trajectories, which contributes to a diffusion RL-powered VLA training pipeline. Our algorithm benefits from not only the high expressiveness of diffusion models to explore complex and diverse behaviors but also the implicit regularization of the iterative denoising process to yield smooth and consistent demonstrations. We evaluate our approach on the LIBERO benchmark, which includes 130 long-horizon manipulation tasks, and show that the generated trajectories are smoother and more consistent than both human demonstrations and those from standard Gaussian RL policies. Further, training a VLA model exclusively on the diffusion RL-generated data achieves an average success rate of 81.9\%, which outperforms the model trained on human data by +5.3\% and that on Gaussian RL-generated data by +12.6\%. The results highlight our diffusion RL as an effective alternative for generating abundant, high-quality, and low-variance demonstrations for VLA models.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Yang, Rushuai and Wei, Hangxing and Zhang, Ran and Feng, Zhiyuan and Chen, Xiaoyu and Li, Tong and Zhang, Chuheng and Zhao, Li and Bian, Jiang and Su, Xiu and Chen, Yi},
	month = sep,
	year = {2025},
	note = {arXiv:2509.19752 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\F56BYYRK\\Yang 等 - 2025 - Beyond Human Demonstrations Diffusion-Based Reinforcement Learning to Generate Data for VLA Trainin.pdf:application/pdf},
}

@misc{wu_what_2026,
	title = {Do {What} {You} {Say}: {Steering} {Vision}-{Language}-{Action} {Models} via {Runtime} {Reasoning}-{Action} {Alignment} {Verification}},
	shorttitle = {Do {What} {You} {Say}},
	url = {http://arxiv.org/abs/2510.16281},
	doi = {10.48550/arXiv.2510.16281},
	abstract = {Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA's intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA's own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA's natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15\% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: https://yilin-wu98.github.io/steering-reasoning-vla/},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Wu, Yilin and Li, Anqi and Hermans, Tucker and Ramos, Fabio and Bajcsy, Andrea and Pérez-D'Arpino, Claudia},
	month = jan,
	year = {2026},
	note = {arXiv:2510.16281 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\H3XPXTGJ\\Wu 等 - 2026 - Do What You Say Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verif.pdf:application/pdf},
}

@misc{hung_nora_2025,
	title = {{NORA}: {A} {Small} {Open}-{Sourced} {Generalist} {Vision} {Language} {Action} {Model} for {Embodied} {Tasks}},
	shorttitle = {{NORA}},
	url = {http://arxiv.org/abs/2504.19854},
	doi = {10.48550/arXiv.2504.19854},
	abstract = {Existing Visual-Language-Action (VLA) models have shown promising performance in zero-shot scenarios, demonstrating impressive task execution and reasoning capabilities. However, a significant challenge arises from the limitations of visual encoding, which can result in failures during tasks such as object grasping. Moreover, these models typically suffer from high computational overhead due to their large sizes, often exceeding 7B parameters. While these models excel in reasoning and task planning, the substantial computational overhead they incur makes them impractical for real-time robotic environments, where speed and efficiency are paramount. To address the limitations of existing VLA models, we propose NORA, a 3B-parameter model designed to reduce computational overhead while maintaining strong task performance. NORA adopts the Qwen-2.5-VL-3B multimodal model as its backbone, leveraging its superior visual-semantic understanding to enhance visual reasoning and action grounding. Additionally, our {\textbackslash}model\{\} is trained on 970k real-world robot demonstrations and equipped with the FAST+ tokenizer for efficient action sequence generation. Experimental results demonstrate that NORA outperforms existing large-scale VLA models, achieving better task performance with significantly reduced computational overhead, making it a more practical solution for real-time robotic autonomy.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Hung, Chia-Yu and Sun, Qi and Hong, Pengfei and Zadeh, Amir and Li, Chuan and Tan, U.-Xuan and Majumder, Navonil and Poria, Soujanya},
	month = apr,
	year = {2025},
	note = {arXiv:2504.19854 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\WBNEFS73\\Hung 等 - 2025 - NORA A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks.pdf:application/pdf},
}

@misc{tarasov_nina_2025,
	title = {{NinA}: {Normalizing} {Flows} in {Action}. {Training} {VLA} {Models} with {Normalizing} {Flows}},
	shorttitle = {{NinA}},
	url = {http://arxiv.org/abs/2508.16845},
	doi = {10.48550/arXiv.2508.16845},
	abstract = {Recent advances in Vision-Language-Action (VLA) models have established a two-component architecture, where a pre-trained Vision-Language Model (VLM) encodes visual observations and task descriptions, and an action decoder maps these representations to continuous actions. Diffusion models have been widely adopted as action decoders due to their ability to model complex, multimodal action distributions. However, they require multiple iterative denoising steps at inference time or downstream techniques to speed up sampling, limiting their practicality in real-world settings where high-frequency control is crucial. In this work, we present NinA (Normalizing Flows in Action), a fast and expressive alternative to diffusion-based decoders for VLAs. NinA replaces the diffusion action decoder with a Normalizing Flow (NF) that enables one-shot sampling through an invertible transformation, significantly reducing inference time. We integrate NinA into the FLOWER VLA architecture and fine-tune on the LIBERO benchmark. Our experiments show that NinA matches the performance of its diffusion-based counterpart under the same training regime, while achieving substantially faster inference. These results suggest that NinA offers a promising path toward efficient, high-frequency VLA control without compromising performance.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Tarasov, Denis and Nikulin, Alexander and Zisman, Ilya and Klepach, Albina and Lyubaykin, Nikita and Polubarov, Andrei and Derevyagin, Alexander and Kurenkov, Vladislav},
	month = oct,
	year = {2025},
	note = {arXiv:2508.16845 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\ICRR38PT\\Tarasov 等 - 2025 - NinA Normalizing Flows in Action. Training VLA Models with Normalizing Flows.pdf:application/pdf},
}

@misc{yang_fpc-vla_2025,
	title = {{FPC}-{VLA}: {A} {Vision}-{Language}-{Action} {Framework} with a {Supervisor} for {Failure} {Prediction} and {Correction}},
	shorttitle = {{FPC}-{VLA}},
	url = {http://arxiv.org/abs/2509.04018},
	doi = {10.48550/arXiv.2509.04018},
	abstract = {Robotic manipulation is a fundamental component of automation. However, traditional perception-planning pipelines often fall short in open-ended tasks due to limited flexibility, while the architecture of a single end-to-end Vision-Language-Action (VLA) offers promising capabilities but lacks crucial mechanisms for anticipating and recovering from failure. To address these challenges, we propose FPC-VLA, a dual-model framework that integrates VLA with a supervisor for failure prediction and correction. The supervisor evaluates action viability through vision-language queries and generates corrective strategies when risks arise, trained efficiently without manual labeling. A dual-stream fusion module further refines actions by leveraging past predictions. Evaluation results on multiple simulation platforms (SIMPLER and LIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA outperforms state-of-the-art models in both zero-shot and fine-tuned settings. Successful real-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong generalization and practical utility for building more reliable autonomous systems.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Yang, Yifan and Duan, Zhixiang and Xie, Tianshi and Cao, Fuyu and Shen, Pinxi and Song, Peili and Jin, Piaopiao and Sun, Guokang and Xu, Shaoqing and You, Yangwei and Liu, Jingtai},
	month = dec,
	year = {2025},
	note = {arXiv:2509.04018 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\V7FIRTEV\\Yang 等 - 2025 - FPC-VLA A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction.pdf:application/pdf},
}

@misc{jiang_wholebodyvla_2025,
	title = {{WholeBodyVLA}: {Towards} {Unified} {Latent} {VLA} for {Whole}-{Body} {Loco}-{Manipulation} {Control}},
	shorttitle = {{WholeBodyVLA}},
	url = {http://arxiv.org/abs/2512.11047},
	doi = {10.48550/arXiv.2512.11047},
	abstract = {Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To execute the desired locomotion commands more precisely, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3\%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Jiang, Haoran and Chen, Jin and Bu, Qingwen and Chen, Li and Shi, Modi and Zhang, Yanjie and Li, Delong and Suo, Chuanzhe and Wang, Chuang and Peng, Zhihui and Li, Hongyang},
	month = dec,
	year = {2025},
	note = {arXiv:2512.11047 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\SCC4DF6R\\Jiang 等 - 2025 - WholeBodyVLA Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control.pdf:application/pdf},
}

@misc{hu_sample-efficient_2025,
	title = {Sample-{Efficient} {Robot} {Skill} {Learning} for {Construction} {Tasks}: {Benchmarking} {Hierarchical} {Reinforcement} {Learning} and {Vision}-{Language}-{Action} {VLA} {Model}},
	shorttitle = {Sample-{Efficient} {Robot} {Skill} {Learning} for {Construction} {Tasks}},
	url = {http://arxiv.org/abs/2512.14031},
	doi = {10.48550/arXiv.2512.14031},
	abstract = {This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60\% and 100\% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Hu, Zhaofeng and Yu, Hongrui and Chandramouli, Vaidhyanathan and Liang, Ci-Jyun},
	month = dec,
	year = {2025},
	note = {arXiv:2512.14031 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\AYEQY8DM\\Hu 等 - 2025 - Sample-Efficient Robot Skill Learning for Construction Tasks Benchmarking Hierarchical Reinforcemen.pdf:application/pdf},
}

@misc{yu_survey_2026,
	title = {A {Survey} on {Efficient} {Vision}-{Language}-{Action} {Models}},
	url = {http://arxiv.org/abs/2510.24795},
	doi = {10.48550/arXiv.2510.24795},
	abstract = {Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. Despite their remarkable performance, foundational VLAs are hindered by the prohibitive computational and data demands inherent to their large-scale architectures. While a surge of recent research has focused on enhancing VLA efficiency, the field lacks a unified framework to consolidate these disparate advancements. To bridge this gap, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire model-training-data pipeline. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Yu, Zhaoshu and Wang, Bo and Zeng, Pengpeng and Zhang, Haonan and Zhang, Ji and Wang, Zheng and Gao, Lianli and Song, Jingkuan and Sebe, Nicu and Shen, Heng Tao},
	month = feb,
	year = {2026},
	note = {arXiv:2510.24795 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\MPT2ZZV9\\Yu 等 - 2026 - A Survey on Efficient Vision-Language-Action Models.pdf:application/pdf},
}

@misc{haon_mechanistic_2025,
	title = {Mechanistic interpretability for steering vision-language-action models},
	url = {http://arxiv.org/abs/2509.00328},
	doi = {10.48550/arXiv.2509.00328},
	abstract = {Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is a central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce a general-purpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that interpretable components of embodied VLAs can be systematically harnessed for control - establishing a new paradigm for transparent and steerable foundation models in robotics.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Häon, Bear and Stocking, Kaylene and Chuang, Ian and Tomlin, Claire},
	month = aug,
	year = {2025},
	note = {arXiv:2509.00328 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\B8IRZB3U\\Häon 等 - 2025 - Mechanistic interpretability for steering vision-language-action models.pdf:application/pdf},
}

@misc{argus_cvla_2025,
	title = {{cVLA}: {Towards} {Efficient} {Camera}-{Space} {VLAs}},
	shorttitle = {{cVLA}},
	url = {http://arxiv.org/abs/2507.02190},
	doi = {10.48550/arXiv.2507.02190},
	abstract = {Vision-Language-Action (VLA) models offer a compelling framework for tackling complex robotic manipulation tasks, but they are often expensive to train. In this paper, we propose a novel VLA approach that leverages the competitive performance of Vision Language Models (VLMs) on 2D images to directly infer robot end-effector poses in image frame coordinates. Unlike prior VLA models that output low-level controls, our model predicts trajectory waypoints, making it both more efficient to train and robot embodiment agnostic. Despite its lightweight design, our next-token prediction architecture effectively learns meaningful and executable robot trajectories. We further explore the underutilized potential of incorporating depth images, inference-time techniques such as decoding strategies, and demonstration-conditioned action generation. Our model is trained on a simulated dataset and exhibits strong sim-to-real transfer capabilities. We evaluate our approach using a combination of simulated and real data, demonstrating its effectiveness on a real robotic system.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Argus, Max and Bratulic, Jelena and Masnavi, Houman and Velikanov, Maxim and Heppert, Nick and Valada, Abhinav and Brox, Thomas},
	month = dec,
	year = {2025},
	note = {arXiv:2507.02190 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\LRDVSE9Y\\Argus 等 - 2025 - cVLA Towards Efficient Camera-Space VLAs.pdf:application/pdf},
}

@misc{yang_efficientvla_2025,
	title = {{EfficientVLA}: {Training}-{Free} {Acceleration} and {Compression} for {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{EfficientVLA}},
	url = {http://arxiv.org/abs/2506.10100},
	doi = {10.48550/arXiv.2506.10100},
	abstract = {Vision-Language-Action (VLA) models, particularly diffusion-based architectures, demonstrate transformative potential for embodied intelligence but are severely hampered by high computational and memory demands stemming from extensive inherent and inference-time redundancies. While existing acceleration efforts often target isolated inefficiencies, such piecemeal solutions typically fail to holistically address the varied computational and memory bottlenecks across the entire VLA pipeline, thereby limiting practical deployability. We introduce EfficientVLA, a structured and training-free inference acceleration framework that systematically eliminates these barriers by cohesively exploiting multifaceted redundancies. EfficientVLA synergistically integrates three targeted strategies: (1) pruning of functionally inconsequential layers from the language module, guided by an analysis of inter-layer redundancies; (2) optimizing the visual processing pathway through a task-aware strategy that selects a compact, diverse set of visual tokens, balancing task-criticality with informational coverage; and (3) alleviating temporal computational redundancy within the iterative diffusion-based action head by strategically caching and reusing key intermediate features. We apply our method to a standard VLA model CogACT, yielding a 1.93X inference speedup and reduces FLOPs to 28.9\%, with only a 0.6\% success rate drop in the SIMPLER benchmark.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Yang, Yantai and Wang, Yuhao and Wen, Zichen and Zhongwei, Luo and Zou, Chang and Zhang, Zhipeng and Wen, Chuan and Zhang, Linfeng},
	month = jun,
	year = {2025},
	note = {arXiv:2506.10100 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\EFW6YVX8\\Yang 等 - 2025 - EfficientVLA Training-Free Acceleration and Compression for Vision-Language-Action Models.pdf:application/pdf},
}

@article{hsieh_what_2025,
	title = {Do what? {Teaching} vision-language-action models to reject the impossible},
	volume = {2},
	shorttitle = {Do what?},
	url = {https://aclanthology.org/anthology-files/pdf/findings/2025.findings-emnlp.635.pdf},
	urldate = {2026-02-04},
	journal = {arXiv preprint arXiv:2508.16292},
	author = {Hsieh, Wen-Han and Hsieh, Elvis and Niu, Dantong and Darrell, Trevor and Herzig, Roei and Chan, David M.},
	year = {2025},
	file = {全文:E\:\\Users\\AresZz\\Zotero\\storage\\ECVVKDPQ\\Hsieh 等 - 2025 - Do what Teaching vision-language-action models to reject the impossible.pdf:application/pdf;Hsieh 等 - 2025 - Do what Teaching vision-language-action models to reject the impossible:E\:\\Users\\AresZz\\Zotero\\storage\\ZJUIGLXL\\Hsieh 等 - 2025 - Do what Teaching vision-language-action models to reject the impossible.pdf:application/pdf},
}

@inproceedings{wan_worldagen_2025,
	title = {{WorldAgen}: {Unified} {State}-{Action} {Prediction} with {Test}-{Time} {World} {Model} {Training}},
	shorttitle = {{WorldAgen}},
	url = {https://openreview.net/forum?id=egbFo1gvYp},
	urldate = {2026-02-04},
	booktitle = {{NeurIPS} 2025 {Workshop} on {Bridging} {Language}, {Agent}, and {World} {Models} for {Reasoning} and {Planning}},
	author = {Wan, Chi and Wang, Kangrui and Si, Yuan and Zhang, Pingyue and Huang, Huang and Li, Manling},
	year = {2025},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\57J2RUXR\\Wan 等 - 2025 - WorldAgen Unified State-Action Prediction with Test-Time World Model Training.pdf:application/pdf},
}

@misc{valle_evaluating_2025,
	title = {Evaluating {Uncertainty} and {Quality} of {Visual} {Language} {Action}-enabled {Robots}},
	url = {http://arxiv.org/abs/2507.17049},
	doi = {10.48550/arXiv.2507.17049},
	abstract = {Visual Language Action (VLA) models are a multi-modal class of Artificial Intelligence (AI) systems that integrate visual perception, natural language understanding, and action planning to enable agents to interpret their environment, comprehend instructions, and perform embodied tasks autonomously. Recently, significant progress has been made to advance this field. These kinds of models are typically evaluated through task success rates, which fail to capture the quality of task execution and the mode's confidence in its decisions. In this paper, we propose eight uncertainty metrics and five quality metrics specifically designed for VLA models for robotic manipulation tasks. We assess their effectiveness through a large-scale empirical study involving 908 successful task executions from three state-of-the-art VLA models across four representative robotic manipulation tasks. Human domain experts manually labeled task quality, allowing us to analyze the correlation between our proposed metrics and expert judgments. The results reveal that several metrics show moderate to strong correlation with human assessments, highlighting their utility for evaluating task quality and model confidence. Furthermore, we found that some of the metrics can discriminate between high-, medium-, and low-quality executions from unsuccessful tasks, which can be interesting when test oracles are not available. Our findings challenge the adequacy of current evaluation practices that rely solely on binary success rates and pave the way for improved real-time monitoring and adaptive enhancement of VLA-enabled robotic systems.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Valle, Pablo and Lu, Chengjie and Ali, Shaukat and Arrieta, Aitor},
	month = jul,
	year = {2025},
	note = {arXiv:2507.17049 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Software Engineering},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\TUTP37QE\\Valle 等 - 2025 - Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots.pdf:application/pdf},
}

@misc{guan_efficient_2025,
	title = {Efficient {Vision}-{Language}-{Action} {Models} for {Embodied} {Manipulation}: {A} {Systematic} {Survey}},
	shorttitle = {Efficient {Vision}-{Language}-{Action} {Models} for {Embodied} {Manipulation}},
	url = {http://arxiv.org/abs/2510.17111},
	doi = {10.48550/arXiv.2510.17111},
	abstract = {Vision-Language-Action (VLA) models extend vision-language models to embodied control by mapping natural-language instructions and visual observations to robot actions. Despite their capabilities, VLA systems face significant challenges due to their massive computational and memory demands, which conflict with the constraints of edge platforms such as on-board mobile manipulators that require real-time performance. Addressing this tension has become a central focus of recent research. In light of the growing efforts toward more efficient and scalable VLA systems, this survey provides a systematic review of approaches for improving VLA efficiency, with an emphasis on reducing latency, memory footprint, and training and inference costs. We categorize existing solutions into four dimensions: model architecture, perception feature, action generation, and training/inference strategies, summarizing representative techniques within each category. Finally, we discuss future trends and open challenges, highlighting directions for advancing efficient embodied intelligence.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Guan, Weifan and Hu, Qinghao and Li, Aosheng and Cheng, Jian},
	month = oct,
	year = {2025},
	note = {arXiv:2510.17111 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\MZJ4BY8Q\\Guan 等 - 2025 - Efficient Vision-Language-Action Models for Embodied Manipulation A Systematic Survey.pdf:application/pdf},
}

@misc{li_spatial_2025,
	title = {Spatial {Forcing}: {Implicit} {Spatial} {Representation} {Alignment} for {Vision}-language-action {Model}},
	shorttitle = {Spatial {Forcing}},
	url = {http://arxiv.org/abs/2510.12276},
	doi = {10.48550/arXiv.2510.12276},
	abstract = {Vision-language-action (VLA) models have recently shown strong potential in enabling robots to follow language instructions and execute precise actions. However, most VLAs are built upon vision-language models pretrained solely on 2D data, which lack accurate spatial awareness and hinder their ability to operate in the 3D physical world. Existing solutions attempt to incorporate explicit 3D sensor inputs such as depth maps or point clouds, but these approaches face challenges due to sensor noise, hardware heterogeneity, and incomplete depth coverage in existing datasets. Alternative methods that estimate 3D cues from 2D images also suffer from the limited performance of depth estimators. We propose Spatial Forcing (SF), a simple yet effective alignment strategy that implicitly forces VLA models to develop spatial comprehension capabilities without relying on explicit 3D inputs or depth estimators. SF aligns intermediate visual embeddings of VLAs with geometric representations produced by pretrained 3D foundation models. By enforcing alignment at intermediate layers, SF guides VLAs to encode richer spatial representations that enhance action precision. Extensive experiments in simulation and real-world environments demonstrate that SF achieves state-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further accelerates training by up to 3.8x and improves data efficiency across diverse robotic tasks. Project page is at https://spatial-forcing.github.io/},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Fuhao and Song, Wenxuan and Zhao, Han and Wang, Jingbo and Ding, Pengxiang and Wang, Donglin and Zeng, Long and Li, Haoang},
	month = oct,
	year = {2025},
	note = {arXiv:2510.12276 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\BWAK9HPX\\Li 等 - 2025 - Spatial Forcing Implicit Spatial Representation Alignment for Vision-language-action Model.pdf:application/pdf},
}

@misc{shukor_smolvla_2025,
	title = {{SmolVLA}: {A} {Vision}-{Language}-{Action} {Model} for {Affordable} and {Efficient} {Robotics}},
	shorttitle = {{SmolVLA}},
	url = {http://arxiv.org/abs/2506.01844},
	doi = {10.48550/arXiv.2506.01844},
	abstract = {Vision-language models (VLMs) pretrained on large-scale multimodal datasets encode rich visual and linguistic knowledge, making them a strong foundation for robotics. Rather than training robotic policies from scratch, recent approaches adapt VLMs into vision-language-action (VLA) models that enable natural language-driven perception and control. However, existing VLAs are typically massive--often with billions of parameters--leading to high training costs and limited real-world deployability. Moreover, they rely on academic and industrial datasets, overlooking the growing availability of community-collected data from affordable robotic platforms. In this work, we present SmolVLA, a small, efficient, and community-driven VLA that drastically reduces both training and inference costs, while retaining competitive performance. SmolVLA is designed to be trained on a single GPU and deployed on consumer-grade GPUs or even CPUs. To further improve responsiveness, we introduce an asynchronous inference stack decoupling perception and action prediction from action execution, allowing higher control rates with chunked action generation. Despite its compact size, SmolVLA achieves performance comparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both simulated as well as real-world robotic benchmarks and release all code, pretrained models, and training data.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Shukor, Mustafa and Aubakirova, Dana and Capuano, Francesco and Kooijmans, Pepijn and Palma, Steven and Zouitine, Adil and Aractingi, Michel and Pascal, Caroline and Russi, Martino and Marafioti, Andres and Alibert, Simon and Cord, Matthieu and Wolf, Thomas and Cadene, Remi},
	month = jun,
	year = {2025},
	note = {arXiv:2506.01844 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\69WZDVIN\\Shukor 等 - 2025 - SmolVLA A Vision-Language-Action Model for Affordable and Efficient Robotics.pdf:application/pdf},
}

@misc{liang_discrete_2025,
	title = {Discrete {Diffusion} {VLA}: {Bringing} {Discrete} {Diffusion} to {Action} {Decoding} in {Vision}-{Language}-{Action} {Policies}},
	shorttitle = {Discrete {Diffusion} {VLA}},
	url = {http://arxiv.org/abs/2508.20072},
	doi = {10.48550/arXiv.2508.20072},
	abstract = {Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions into robot actions. However, prevailing VLAs either generate actions auto-regressively in a fixed left-to-right order or attach separate MLP or diffusion heads outside the backbone, leading to fragmented information pathways and specialized training requirements that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a unified-transformer policy that models discretized action chunks with discrete diffusion. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary re-masking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pre-trained vision-language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3\% avg. success rates on LIBERO, 71.2\% visual matching on SimplerEnv-Fractal and 54.2\% overall on SimplerEnv-Bridge. We also provide ablation study on vision-language ability retention on LIBERO-OOD (Out-of-Distribution) benchmark, with our method improving over autoregressive, MLP decoder and continuous diffusion baselines. These findings indicate that discrete-diffusion VLA supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets. Our code is available at https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Liang, Zhixuan and Li, Yizhuo and Yang, Tianshuo and Wu, Chengyue and Mao, Sitong and Nian, Tian and Pei, Liuao and Zhou, Shunbo and Yang, Xiaokang and Pang, Jiangmiao and Mu, Yao and Luo, Ping},
	month = dec,
	year = {2025},
	note = {arXiv:2508.20072 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\SHDD6TET\\Liang 等 - 2025 - Discrete Diffusion VLA Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Pol.pdf:application/pdf},
}

@misc{huang_adapower_2025,
	title = {{AdaPower}: {Specializing} {World} {Foundation} {Models} for {Predictive} {Manipulation}},
	shorttitle = {{AdaPower}},
	url = {http://arxiv.org/abs/2512.03538},
	doi = {10.48550/arXiv.2512.03538},
	abstract = {World Foundation Models (WFMs) offer remarkable visual dynamics simulation capabilities, yet their application to precise robotic control remains limited by the gap between generative realism and control-oriented precision. While existing approaches use WFMs as synthetic data generators, they suffer from high computational costs and underutilization of pre-trained VLA policies. We introduce {\textbackslash}textbf\{AdaPower\} ({\textbackslash}textbf\{Ada\}pt and Em{\textbackslash}textbf\{power\}), a lightweight adaptation framework that transforms general-purpose WFMs into specialist world models through two novel components: Temporal-Spatial Test-Time Training (TS-TTT) for inference-time adaptation and Memory Persistence (MP) for long-horizon consistency. Integrated within a Model Predictive Control framework, our adapted world model empowers pre-trained VLAs, achieving over 41{\textbackslash}\% improvement in task success rates on LIBERO benchmarks without policy retraining, while preserving computational efficiency and generalist capabilities.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Huang, Yuhang and Zou, Shilong and Zhang, Jiazhao and Liu, Xinwang and Hu, Ruizhen and Xu, Kai},
	month = dec,
	year = {2025},
	note = {arXiv:2512.03538 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\RKUHK7U6\\Huang 等 - 2025 - AdaPower Specializing World Foundation Models for Predictive Manipulation.pdf:application/pdf},
}

@misc{wang_unified_2025,
	title = {Unified {Vision}-{Language}-{Action} {Model}},
	url = {http://arxiv.org/abs/2506.19850},
	doi = {10.48550/arXiv.2506.19850},
	abstract = {Vision-language-action models (VLAs) have garnered significant attention for their potential in advancing robotic manipulation. However, previous approaches predominantly rely on the general comprehension capabilities of vision-language models (VLMs) to generate action signals, often overlooking the rich temporal and causal structure embedded in visual observations. In this paper, we present UniVLA, a unified and native multimodal VLA model that autoregressively models vision, language, and action signals as discrete token sequences. This formulation enables flexible multimodal tasks learning, particularly from large-scale video data. By incorporating world modeling during post-training, UniVLA captures causal dynamics from videos, facilitating effective transfer to downstream policy learning--especially for long-horizon tasks. Our approach sets new state-of-the-art results across several widely used simulation benchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly surpassing previous methods. For example, UniVLA achieves 95.5\% average success rate on LIBERO benchmark, surpassing pi0-FAST's 85.5\%. We further demonstrate its broad applicability on real-world ALOHA manipulation and autonomous driving.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Wang, Yuqi and Li, Xinghang and Wang, Wenxuan and Zhang, Junbo and Li, Yingyan and Chen, Yuntao and Wang, Xinlong and Zhang, Zhaoxiang},
	month = jun,
	year = {2025},
	note = {arXiv:2506.19850 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\6TF6KJYL\\Wang 等 - 2025 - Unified Vision-Language-Action Model.pdf:application/pdf},
}

@misc{chen_combatvla_2026,
	title = {{CombatVLA}: {An} {Efficient} {Vision}-{Language}-{Action} {Model} for {Combat} {Tasks} in {3D} {Action} {Role}-{Playing} {Games}},
	shorttitle = {{CombatVLA}},
	url = {http://arxiv.org/abs/2503.09527},
	doi = {10.48550/arXiv.2503.09527},
	abstract = {Recent advances in Vision-Language-Action models (VLAs) have expanded the capabilities of embodied intelligence. However, significant challenges remain in real-time decision-making in complex 3D environments, which demand second-level responses, high-resolution perception, and tactical reasoning under dynamic conditions. To advance the field, we introduce CombatVLA, an efficient VLA model optimized for combat tasks in 3D action role-playing games(ARPGs). Specifically, our CombatVLA is a 3B model trained on video-action pairs collected by an action tracker, where the data is formatted as action-of-thought (AoT) sequences. Thereafter, CombatVLA seamlessly integrates into an action execution framework, allowing efficient inference through our truncated AoT strategy. Experimental results demonstrate that CombatVLA not only outperforms all existing models on the combat understanding benchmark but also achieves a 50-fold acceleration in game combat. Moreover, it has a higher task success rate than human players. We will open-source all resources, including the action tracker, dataset, benchmark, model weights, training code, and the implementation of the framework at https://combatvla.github.io/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Chen, Peng and Bu, Pi and Wang, Yingyao and Wang, Xinyi and Wang, Ziming and Guo, Jie and Zhao, Yingxiu and Zhu, Qi and Song, Jun and Yang, Siran and Wang, Jiamang and Zheng, Bo},
	month = jan,
	year = {2026},
	note = {arXiv:2503.09527 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\HNJ4Q3WS\\Chen 等 - 2026 - CombatVLA An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Game.pdf:application/pdf},
}

@misc{chen_villa-x_2025,
	title = {villa-{X}: {Enhancing} {Latent} {Action} {Modeling} in {Vision}-{Language}-{Action} {Models}},
	shorttitle = {villa-{X}},
	url = {http://arxiv.org/abs/2507.23682},
	doi = {10.48550/arXiv.2507.23682},
	abstract = {Vision-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent works have begun to explore the incorporation of latent actions, abstract representations of motion between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Vision-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. We demonstrate that villa-X can generate latent action plans in a zero-shot fashion, even for unseen embodiments and open-vocabulary symbolic understanding. This capability enables villa-X to achieve superior performance across diverse simulation tasks in SIMPLER and on two real-world robotic setups involving both gripper and dexterous hand manipulation. These results establish villa-X as a principled and scalable paradigm for learning generalizable robot manipulation policies. We believe it provides a strong foundation for future research.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Chen, Xiaoyu and Wei, Hangxing and Zhang, Pushi and Zhang, Chuheng and Wang, Kaixin and Guo, Yanjiang and Yang, Rushuai and Wang, Yucen and Xiao, Xinquan and Zhao, Li and Chen, Jianyu and Bian, Jiang},
	month = sep,
	year = {2025},
	note = {arXiv:2507.23682 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\W9GR7R5Y\\Chen 等 - 2025 - villa-X Enhancing Latent Action Modeling in Vision-Language-Action Models.pdf:application/pdf},
}

@misc{jiang_survey_2025,
	title = {A {Survey} on {Vision}-{Language}-{Action} {Models} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2506.24044},
	doi = {10.48550/arXiv.2506.24044},
	abstract = {The rapid progress of multimodal large language models (MLLM) has paved the way for Vision-Language-Action (VLA) paradigms, which integrate visual perception, natural language understanding, and control within a single policy. Researchers in autonomous driving are actively adapting these methods to the vehicle domain. Such models promise autonomous vehicles that can interpret high-level instructions, reason about complex traffic scenes, and make their own decisions. However, the literature remains fragmented and is rapidly expanding. This survey offers the first comprehensive overview of VLA for Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks shared across recent work, (ii) trace the evolution from early explainer to reasoning-centric VLA models, and (iii) compare over 20 representative models according to VLA's progress in the autonomous driving domain. We also consolidate existing datasets and benchmarks, highlighting protocols that jointly measure driving safety, accuracy, and explanation quality. Finally, we detail open challenges - robustness, real-time efficiency, and formal verification - and outline future directions of VLA4AD. This survey provides a concise yet complete reference for advancing interpretable socially aligned autonomous vehicles. Github repo is available at {\textbackslash}href\{https://github.com/JohnsonJiang1996/Awesome-VLA4AD\}\{SicongJiang/Awesome-VLA4AD\}.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Jiang, Sicong and Huang, Zilin and Qian, Kangan and Luo, Ziang and Zhu, Tianze and Zhong, Yang and Tang, Yihong and Kong, Menglin and Wang, Yunlong and Jiao, Siwen and Ye, Hao and Sheng, Zihao and Zhao, Xin and Wen, Tuopu and Fu, Zheng and Chen, Sikai and Jiang, Kun and Yang, Diange and Choi, Seongjin and Sun, Lijun},
	month = jun,
	year = {2025},
	note = {arXiv:2506.24044 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\WW8G3X6X\\Jiang 等 - 2025 - A Survey on Vision-Language-Action Models for Autonomous Driving.pdf:application/pdf},
}

@misc{zhong_flowvla_2025,
	title = {{FlowVLA}: {Visual} {Chain} of {Thought}-based {Motion} {Reasoning} for {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{FlowVLA}},
	url = {http://arxiv.org/abs/2508.18269},
	doi = {10.48550/arXiv.2508.18269},
	abstract = {Many Vision-Language-Action (VLA) models are built upon an internal world model trained via next-frame prediction ``\$v\_t {\textbackslash}rightarrow v\_\{t+1\}\$''. However, this paradigm attempts to predict the future frame's appearance directly, without explicitly reasoning about the underlying dynamics. {\textbackslash}textbf\{This lack of an explicit motion reasoning step\} often leads to physically implausible visual forecasts and inefficient policy learning. To address this limitation, we introduce the {\textbackslash}textbf\{Visual Chain of Thought (Visual CoT)\}, a paradigm that compels the model to first reason about {\textbackslash}textbf\{motion dynamics\} before generating the future frame. We instantiate this paradigm by proposing {\textbackslash}textbf\{FlowVLA\}, an autoregressive Transformer that explicitly materializes this reasoning process as ``\$v\_t {\textbackslash}rightarrow f\_t {\textbackslash}rightarrow v\_\{t+1\}\$'', where \$f\_t\$ is an intermediate optical flow prediction that inherently encodes motion. By forcing the model to first follow the motion plan encoded by \$f\_t\$, this process inherently {\textbackslash}textbf\{aligns the pre-training objective of dynamics prediction with the downstream task of action generation.\} We conduct experiments on challenging robotics manipulation benchmarks, as well as real-robot evaluations. Our FlowVLA not only generates {\textbackslash}textbf\{more coherent and physically plausible visual predictions\}, but also achieves state-of-the-art policy performance with {\textbackslash}textbf\{substantially improved sample efficiency\}, pointing toward a more principled foundation for world modeling in VLAs. Project page: https://irpn-lab.github.io/FlowVLA/},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhong, Zhide and Yan, Haodong and Li, Junfeng and Liu, Xiangchen and Gong, Xin and Zhang, Tianran and Song, Wenxuan and Chen, Jiayi and Zheng, Xinhu and Wang, Hesheng and Li, Haoang},
	month = oct,
	year = {2025},
	note = {arXiv:2508.18269 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\DYKYZU7G\\Zhong 等 - 2025 - FlowVLA Visual Chain of Thought-based Motion Reasoning for Vision-Language-Action Models.pdf:application/pdf},
}

@misc{du_himoe-vla_2025,
	title = {{HiMoE}-{VLA}: {Hierarchical} {Mixture}-of-{Experts} for {Generalist} {Vision}-{Language}-{Action} {Policies}},
	shorttitle = {{HiMoE}-{VLA}},
	url = {http://arxiv.org/abs/2512.05693},
	doi = {10.48550/arXiv.2512.05693},
	abstract = {The development of foundation models for embodied intelligence critically depends on access to large-scale, high-quality robot demonstration data. Recent approaches have sought to address this challenge by training on large collections of heterogeneous robotic datasets. However, unlike vision or language data, robotic demonstrations exhibit substantial heterogeneity across embodiments and action spaces as well as other prominent variations such as senor configurations and action control frequencies. The lack of explicit designs for handling such heterogeneity causes existing methods to struggle with integrating diverse factors, thereby limiting their generalization and leading to degraded performance when transferred to new settings. In this paper, we present HiMoE-VLA, a novel vision-language-action (VLA) framework tailored to effectively handle diverse robotic data with heterogeneity. Specifically, we introduce a Hierarchical Mixture-of-Experts (HiMoE) architecture for the action module which adaptively handles multiple sources of heterogeneity across layers and gradually abstracts them into shared knowledge representations. Through extensive experimentation with simulation benchmarks and real-world robotic platforms, HiMoE-VLA demonstrates a consistent performance boost over existing VLA baselines, achieving higher accuracy and robust generalization across diverse robots and action spaces. The code and models are publicly available at https://github.com/ZhiyingDu/HiMoE-VLA.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Du, Zhiying and Liu, Bei and Liang, Yaobo and Shen, Yichao and Cao, Haidong and Zheng, Xiangyu and Feng, Zhiyuan and Wu, Zuxuan and Yang, Jiaolong and Jiang, Yu-Gang},
	month = dec,
	year = {2025},
	note = {arXiv:2512.05693 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\DTM39SA4\\Du 等 - 2025 - HiMoE-VLA Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies.pdf:application/pdf},
}

@misc{bhat_3d_2025,
	title = {{3D} {CAVLA}: {Leveraging} {Depth} and {3D} {Context} to {Generalize} {Vision} {Language} {Action} {Models} for {Unseen} {Tasks}},
	shorttitle = {{3D} {CAVLA}},
	url = {http://arxiv.org/abs/2505.05800},
	doi = {10.48550/arXiv.2505.05800},
	abstract = {Robotic manipulation in 3D requires learning an \$N\$ degree-of-freedom joint space trajectory of a robot manipulator. Robots must possess semantic and visual perception abilities to transform real-world mappings of their workspace into the low-level control necessary for object manipulation. Recent work has demonstrated the capabilities of fine-tuning large Vision-Language Models (VLMs) to learn the mapping between RGB images, language instructions, and joint space control. These models typically take as input RGB images of the workspace and language instructions, and are trained on large datasets of teleoperated robot demonstrations. In this work, we explore methods to improve the scene context awareness of a popular recent Vision-Language-Action model by integrating chain-of-thought reasoning, depth perception, and task-oriented region of interest detection. Our experiments in the LIBERO simulation environment show that our proposed model, 3D-CAVLA, improves the success rate across various LIBERO task suites, achieving an average success rate of 98.1\${\textbackslash}\%\$. We also evaluate the zero-shot capabilities of our method, demonstrating that 3D scene awareness leads to robust learning and adaptation for completely unseen tasks. 3D-CAVLA achieves an absolute improvement of 8.8\${\textbackslash}\%\$ on unseen tasks. We will open-source our code and the unseen tasks dataset to promote community-driven research here: https://3d-cavla.github.io},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Bhat, Vineet and Lan, Yu-Hsiang and Krishnamurthy, Prashanth and Karri, Ramesh and Khorrami, Farshad},
	month = may,
	year = {2025},
	note = {arXiv:2505.05800 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\LYPNLDKH\\Bhat 等 - 2025 - 3D CAVLA Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tas.pdf:application/pdf},
}

@inproceedings{liu_vla-mark_2025,
	title = {{VLA}-{Mark}: {A} cross modal watermark for large vision-language alignment models},
	shorttitle = {{VLA}-{Mark}},
	url = {https://aclanthology.org/2025.emnlp-main.1342/},
	urldate = {2026-02-04},
	booktitle = {Proceedings of the 2025 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Liu, Shuliang and Qi, Zheng and Xu, Jesse Jiaxi and Yan, Yibo and Zhang, Junyan and Geng, He and Liu, Aiwei and Jiang, Peijie and Liu, Jia and Tam, Yik-Cheung},
	year = {2025},
	pages = {26420--26438},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\8GPBVIDV\\Liu 等 - 2025 - VLA-Mark A cross modal watermark for large vision-language alignment models.pdf:application/pdf},
}

@misc{lykov_cognitivedrone_2025,
	title = {{CognitiveDrone}: {A} {VLA} {Model} and {Evaluation} {Benchmark} for {Real}-{Time} {Cognitive} {Task} {Solving} and {Reasoning} in {UAVs}},
	shorttitle = {{CognitiveDrone}},
	url = {http://arxiv.org/abs/2503.01378},
	doi = {10.48550/arXiv.2503.01378},
	abstract = {This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition, Symbol Understanding, and Reasoning-the model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while a racing-oriented model (RaceVLA) achieves an overall success rate of 31.3\%, the base CognitiveDrone model reaches 59.6\%, and CognitiveDrone-R1 attains a success rate of 77.2\%. These results demonstrate improvements of up to 30\% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of a state-of-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at cognitivedrone.github.io},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Lykov, Artem and Serpiva, Valerii and Khan, Muhammad Haris and Sautenkov, Oleg and Myshlyaev, Artyom and Tadevosyan, Grik and Yaqoot, Yasheerah and Tsetserukou, Dzmitry},
	month = mar,
	year = {2025},
	note = {arXiv:2503.01378 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\U9GJF4GH\\Lykov 等 - 2025 - CognitiveDrone A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reason.pdf:application/pdf},
}

@misc{syed_expres-vla_2025,
	title = {{ExpReS}-{VLA}: {Specializing} {Vision}-{Language}-{Action} {Models} {Through} {Experience} {Replay} and {Retrieval}},
	shorttitle = {{ExpReS}-{VLA}},
	url = {http://arxiv.org/abs/2511.06202},
	doi = {10.48550/arXiv.2511.06202},
	abstract = {Vision-Language-Action models such as OpenVLA show impressive zero-shot generalization across robotic manipulation tasks but often fail to adapt efficiently to new deployment environments. In many real-world applications, consistent high performance on a limited set of tasks is more important than broad generalization. We propose ExpReS-VLA, a method for specializing pre-trained VLA models through experience replay and retrieval while preventing catastrophic forgetting. ExpReS-VLA stores compact feature representations from the frozen vision backbone instead of raw image-action pairs, reducing memory usage by approximately 97 percent. During deployment, relevant past experiences are retrieved using cosine similarity and used to guide adaptation, while prioritized experience replay emphasizes successful trajectories. We also introduce Thresholded Hybrid Contrastive Loss, which enables learning from both successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments with five manipulation tasks, it reaches 98 percent success on both seen and unseen settings, compared to 84.7 and 32 percent for naive fine-tuning. Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU, making the approach practical for real robot deployment.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Syed, Shahram Najam and Ahuja, Yatharth and Jakobsson, Arthur and Ichnowski, Jeff},
	month = nov,
	year = {2025},
	note = {arXiv:2511.06202 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\NYHHJIFY\\Syed 等 - 2025 - ExpReS-VLA Specializing Vision-Language-Action Models Through Experience Replay and Retrieval.pdf:application/pdf},
}

@misc{guo_omnivla_2025,
	title = {{OmniVLA}: {Physically}-{Grounded} {Multimodal} {VLA} with {Unified} {Multi}-{Sensor} {Perception} for {Robotic} {Manipulation}},
	shorttitle = {{OmniVLA}},
	url = {http://arxiv.org/abs/2511.01210},
	doi = {10.48550/arXiv.2511.01210},
	abstract = {Vision-language-action (VLA) models have shown strong generalization for robotic action prediction through large-scale vision-language pretraining. However, most existing models rely solely on RGB cameras, limiting their perception and, consequently, manipulation capabilities. We present OmniVLA, an omni-modality VLA model that integrates novel sensing modalities for physically-grounded spatial intelligence beyond RGB perception. The core of our approach is the sensor-masked image, a unified representation that overlays spatially grounded and physically meaningful masks onto the RGB images, derived from sensors including an infrared camera, a mmWave radar, and a microphone array. This image-native unification keeps sensor input close to RGB statistics to facilitate training, provides a uniform interface across sensor hardware, and enables data-efficient learning with lightweight per-sensor projectors. Built on this, we present a multisensory vision-language-action model architecture and train the model based on an RGB-pretrained VLA backbone. We evaluate OmniVLA on challenging real-world tasks where sensor-modality perception guides the robotic manipulation. OmniVLA achieves an average task success rate of 84\%, significantly outperforms both RGB-only and raw-sensor-input baseline models by 59\% and 28\% respectively, meanwhile showing higher learning efficiency and stronger generalization capability.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Guo, Heyu and Wang, Shanmu and Ma, Ruichun and Jiang, Shiqi and Ghasempour, Yasaman and Abari, Omid and Guo, Baining and Qiu, Lili},
	month = nov,
	year = {2025},
	note = {arXiv:2511.01210 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\IEHJILBL\\Guo 等 - 2025 - OmniVLA Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipul.pdf:application/pdf},
}

@misc{sun_collabvla_2025,
	title = {{CollabVLA}: {Self}-{Reflective} {Vision}-{Language}-{Action} {Model} {Dreaming} {Together} with {Human}},
	shorttitle = {{CollabVLA}},
	url = {http://arxiv.org/abs/2509.14889},
	doi = {10.48550/arXiv.2509.14889},
	abstract = {In this work, we present CollabVLA, a self-reflective vision-language-action framework that transforms a standard visuomotor policy into a collaborative assistant. CollabVLA tackles key limitations of prior VLAs, including domain overfitting, non-interpretable reasoning, and the high latency of auxiliary generative models, by integrating VLM-based reflective reasoning with diffusion-based action generation under a mixture-of-experts design. Through a two-stage training recipe of action grounding and reflection tuning, it supports explicit self-reflection and proactively solicits human guidance when confronted with uncertainty or repeated failure. It cuts normalized Time by {\textasciitilde}2x and Dream counts by {\textasciitilde}4x vs. generative agents, achieving higher success rates, improved interpretability, and balanced low latency compared with existing methods. This work takes a pioneering step toward shifting VLAs from opaque controllers to genuinely assistive agents capable of reasoning, acting, and collaborating with humans.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Sun, Nan and Li, Yongchang and Wang, Chenxu and Li, Huiying and Liu, Huaping},
	month = sep,
	year = {2025},
	note = {arXiv:2509.14889 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\L798VTDT\\Sun 等 - 2025 - CollabVLA Self-Reflective Vision-Language-Action Model Dreaming Together with Human.pdf:application/pdf},
}

@misc{yuan_depthvla_2025,
	title = {{DepthVLA}: {Enhancing} {Vision}-{Language}-{Action} {Models} with {Depth}-{Aware} {Spatial} {Reasoning}},
	shorttitle = {{DepthVLA}},
	url = {http://arxiv.org/abs/2510.13375},
	doi = {10.48550/arXiv.2510.13375},
	abstract = {Vision-Language-Action (VLA) models have recently shown impressive generalization and language-guided manipulation capabilities. However, their performance degrades on tasks requiring precise spatial reasoning due to limited spatial reasoning inherited from Vision-Language Models (VLMs). Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3D space, which reduces training efficiency and is still insufficient for accurate spatial understanding. In this work, we present DepthVLA, a simple yet effective VLA architecture that explicitly incorporates spatial awareness through a pretrained depth prediction module. DepthVLA adopts a mixture-of-transformers design that unifies a VLM, a depth transformer, and an action expert with fully shared attentions, forming an end-to-end model with enhanced spatial reasoning. Extensive evaluations in both real-world and simulated environments show that DepthVLA outperforms state-of-the-art approaches, achieving 78.5\% vs. 65.0\% progress in real-world tasks, 94.9\% vs. 93.6\% in the LIBERO simulator, and 74.8\% vs. 58.8\% in the Simpler simulator. Our code will be made publicly available.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Yuan, Tianyuan and Liu, Yicheng and Lu, Chenhao and Chen, Zhuoguang and Jiang, Tao and Zhao, Hang},
	month = oct,
	year = {2025},
	note = {arXiv:2510.13375 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\EP44U8KL\\Yuan 等 - 2025 - DepthVLA Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning.pdf:application/pdf},
}

@misc{li_controlvla_2025,
	title = {{ControlVLA}: {Few}-shot {Object}-centric {Adaptation} for {Pre}-trained {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{ControlVLA}},
	url = {http://arxiv.org/abs/2506.16211},
	doi = {10.48550/arXiv.2506.16211},
	abstract = {Learning real-world robotic manipulation is challenging, particularly when limited demonstrations are available. Existing methods for few-shot manipulation often rely on simulation-augmented data or pre-built modules like grasping and pose estimation, which struggle with sim-to-real gaps and lack extensibility. While large-scale imitation pre-training shows promise, adapting these general-purpose policies to specific tasks in data-scarce settings remains unexplored. To achieve this, we propose ControlVLA, a novel framework that bridges pre-trained VLA models with object-centric representations via a ControlNet-style architecture for efficient fine-tuning. Specifically, to introduce object-centric conditions without overwriting prior knowledge, ControlVLA zero-initializes a set of projection layers, allowing them to gradually adapt the pre-trained manipulation policies. In real-world experiments across 6 diverse tasks, including pouring cubes and folding clothes, our method achieves a 76.7\% success rate while requiring only 10-20 demonstrations -- a significant improvement over traditional approaches that require more than 100 demonstrations to achieve comparable success. Additional experiments highlight ControlVLA's extensibility to long-horizon tasks and robustness to unseen objects and backgrounds.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Puhao and Wu, Yingying and Xi, Ziheng and Li, Wanlin and Huang, Yuzhe and Zhang, Zhiyuan and Chen, Yinghan and Wang, Jianan and Zhu, Song-Chun and Liu, Tengyu and Huang, Siyuan},
	month = jun,
	year = {2025},
	note = {arXiv:2506.16211 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\BK2LVQYW\\Li 等 - 2025 - ControlVLA Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models.pdf:application/pdf},
}

@misc{liu_mla_2025,
	title = {{MLA}: {A} {Multisensory} {Language}-{Action} {Model} for {Multimodal} {Understanding} and {Forecasting} in {Robotic} {Manipulation}},
	shorttitle = {{MLA}},
	url = {http://arxiv.org/abs/2509.26642},
	doi = {10.48550/arXiv.2509.26642},
	abstract = {Vision-language-action models (VLAs) have shown generalization capabilities in robotic manipulation tasks by inheriting from vision-language models (VLMs) and learning action generation. Most VLA models focus on interpreting vision and language to generate actions, whereas robots must perceive and interact within the spatial-physical world. This gap highlights the need for a comprehensive understanding of robotic-specific multisensory information, which is crucial for achieving complex and contact-rich control. To this end, we introduce a multisensory language-action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory objectives to facilitate physical world modeling. Specifically, to enhance perceptual representations, we propose an encoder-free multimodal alignment scheme that innovatively repurposes the large language model itself as a perception module, directly interpreting multimodal cues by aligning 2D images, 3D point clouds, and tactile tokens through positional correspondence. To further enhance MLA's understanding of physical dynamics, we design a future multisensory generation post-training strategy that enables MLA to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation. For evaluation, the MLA model outperforms the previous state-of-the-art 2D and 3D VLA methods by 12\% and 24\% in complex, contact-rich real-world tasks, respectively, while also demonstrating improved generalization to unseen configurations. Project website: https://sites.google.com/view/open-mla},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Liu, Zhuoyang and Liu, Jiaming and Xu, Jiadong and Han, Nuowei and Gu, Chenyang and Chen, Hao and Zhou, Kaichen and Zhang, Renrui and Hsieh, Kai Chin and Wu, Kun and Che, Zhengping and Tang, Jian and Zhang, Shanghang},
	month = sep,
	year = {2025},
	note = {arXiv:2509.26642 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\83ZAKWKV\\Liu 等 - 2025 - MLA A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Ma.pdf:application/pdf},
}

@misc{song_rationalvla_2025,
	title = {{RationalVLA}: {A} {Rational} {Vision}-{Language}-{Action} {Model} with {Dual} {System}},
	shorttitle = {{RationalVLA}},
	url = {http://arxiv.org/abs/2506.10826},
	doi = {10.48550/arXiv.2506.10826},
	abstract = {A fundamental requirement for real-world robotic deployment is the ability to understand and respond to natural language instructions. Existing language-conditioned manipulation tasks typically assume that instructions are perfectly aligned with the environment. This assumption limits robustness and generalization in realistic scenarios where instructions may be ambiguous, irrelevant, or infeasible. To address this problem, we introduce RAtional MAnipulation (RAMA), a new benchmark that challenges models with both unseen executable instructions and defective ones that should be rejected. In RAMA, we construct a dataset with over 14,000 samples, including diverse defective instructions spanning six dimensions: visual, physical, semantic, motion, safety, and out-of-context. We further propose the Rational Vision-Language-Action model (RationalVLA). It is a dual system for robotic arms that integrates the high-level vision-language model with the low-level manipulation policy by introducing learnable latent space embeddings. This design enables RationalVLA to reason over instructions, reject infeasible commands, and execute manipulation effectively. Experiments demonstrate that RationalVLA outperforms state-of-the-art baselines on RAMA by a 14.5\% higher success rate and 0.94 average task length, while maintaining competitive performance on standard manipulation tasks. Real-world trials further validate its effectiveness and robustness in practical applications. Our project page is https://irpn-eai.github.io/RationalVLA.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Song, Wenxuan and Chen, Jiayi and Li, Wenxue and He, Xu and Zhao, Han and Cui, Can and Su, Pengxiang Ding Shiyan and Tang, Feilong and Cheng, Xuelian and Wang, Donglin and Ge, Zongyuan and Zheng, Xinhu and Liu, Zhe and Wang, Hesheng and Li, Haoang},
	month = jun,
	year = {2025},
	note = {arXiv:2506.10826 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\D73SC2XL\\Song 等 - 2025 - RationalVLA A Rational Vision-Language-Action Model with Dual System.pdf:application/pdf},
}

@misc{kim_fine-tuning_2025,
	title = {Fine-{Tuning} {Vision}-{Language}-{Action} {Models}: {Optimizing} {Speed} and {Success}},
	shorttitle = {Fine-{Tuning} {Vision}-{Language}-{Action} {Models}},
	url = {http://arxiv.org/abs/2502.19645},
	doi = {10.48550/arXiv.2502.19645},
	abstract = {Recent vision-language-action models (VLAs) build upon pretrained vision-language models and leverage diverse robot datasets to demonstrate strong task execution, language following ability, and semantic generalization. Despite these successes, VLAs struggle with novel robot setups and require fine-tuning to achieve good performance, yet how to most effectively fine-tune them is unclear given many possible strategies. In this work, we study key VLA adaptation design choices such as different action decoding schemes, action representations, and learning objectives for fine-tuning, using OpenVLA as our representative base model. Our empirical analysis informs an Optimized Fine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, a continuous action representation, and a simple L1 regression-based learning objective to altogether improve inference efficiency, policy performance, and flexibility in the model's input-output specifications. We propose OpenVLA-OFT, an instantiation of this recipe, which sets a new state of the art on the LIBERO simulation benchmark, significantly boosting OpenVLA's average success rate across four task suites from 76.5\% to 97.1\% while increasing action generation throughput by 26\${\textbackslash}times\$. In real-world evaluations, our fine-tuning recipe enables OpenVLA to successfully execute dexterous, high-frequency control tasks on a bimanual ALOHA robot and outperform other VLAs (\$\pi \_0\$ and RDT-1B) fine-tuned using their default recipes, as well as strong imitation learning policies trained from scratch (Diffusion Policy and ACT) by up to 15\% (absolute) in average success rate. We release code for OFT and pretrained model checkpoints at https://openvla-oft.github.io/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Kim, Moo Jin and Finn, Chelsea and Liang, Percy},
	month = apr,
	year = {2025},
	note = {arXiv:2502.19645 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\XRUY5QEV\\Kim 等 - 2025 - Fine-Tuning Vision-Language-Action Models Optimizing Speed and Success.pdf:application/pdf},
}

@misc{tai_realmirror_2025,
	title = {{RealMirror}: {A} {Comprehensive}, {Open}-{Source} {Vision}-{Language}-{Action} {Platform} for {Embodied} {AI}},
	shorttitle = {{RealMirror}},
	url = {http://arxiv.org/abs/2509.14687},
	doi = {10.48550/arXiv.2509.14687},
	abstract = {The emerging field of Vision-Language-Action (VLA) for humanoid robots faces several fundamental challenges, including the high cost of data acquisition, the lack of a standardized benchmark, and the significant gap between simulation and the real world. To overcome these obstacles, we propose RealMirror, a comprehensive, open-source embodied AI VLA platform. RealMirror builds an efficient, low-cost data collection, model training, and inference system that enables end-to-end VLA research without requiring a real robot. To facilitate model evolution and fair comparison, we also introduce a dedicated VLA benchmark for humanoid robots, featuring multiple scenarios, extensive trajectories, and various VLA models. Furthermore, by integrating generative models and 3D Gaussian Splatting to reconstruct realistic environments and robot models, we successfully demonstrate zero-shot Sim2Real transfer, where models trained exclusively on simulation data can perform tasks on a real robot seamlessly, without any fine-tuning. In conclusion, with the unification of these critical components, RealMirror provides a robust framework that significantly accelerates the development of VLA models for humanoid robots. Project page: https://terminators2025.github.io/RealMirror.github.io},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Tai, Cong and Zheng, Zhaoyu and Long, Haixu and Wu, Hansheng and Xiang, Haodong and Long, Zhengbin and Xiong, Jun and Shi, Rong and Zhang, Shizhuang and Qiu, Gang and Wang, He and Li, Ruifeng and Huang, Jun and Chang, Bin and Feng, Shuai and Shen, Tao},
	month = sep,
	year = {2025},
	note = {arXiv:2509.14687 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\9IPCARSI\\Tai 等 - 2025 - RealMirror A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI.pdf:application/pdf},
}

@misc{tan_interactive_2025,
	title = {Interactive {Post}-{Training} for {Vision}-{Language}-{Action} {Models}},
	url = {http://arxiv.org/abs/2505.17016},
	doi = {10.48550/arXiv.2505.17016},
	abstract = {We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation. RIPT-VLA has the following characteristics. First, it applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2\%, and the 7B OpenVLA-OFT model to an unprecedented 97.5\% success rate. Second, it is computationally efficient and data-efficient: with only one demonstration, RIPT-VLA enables an unworkable SFT model (4\%) to succeed with a 97\% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Tan, Shuhan and Dou, Kairan and Zhao, Yue and Krähenbühl, Philipp},
	month = may,
	year = {2025},
	note = {arXiv:2505.17016 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\65WGGSHF\\Tan 等 - 2025 - Interactive Post-Training for Vision-Language-Action Models.pdf:application/pdf},
}

@misc{xie_latentvla_2026,
	title = {{LatentVLA}: {Efficient} {Vision}-{Language} {Models} for {Autonomous} {Driving} via {Latent} {Action} {Prediction}},
	shorttitle = {{LatentVLA}},
	url = {http://arxiv.org/abs/2601.05611},
	doi = {10.48550/arXiv.2601.05611},
	abstract = {End-to-end autonomous driving models trained on largescale datasets perform well in common scenarios but struggle with rare, long-tail situations due to limited scenario diversity. Recent Vision-Language-Action (VLA) models leverage broad knowledge from pre-trained visionlanguage models to address this limitation, yet face critical challenges: (1) numerical imprecision in trajectory prediction due to discrete tokenization, (2) heavy reliance on language annotations that introduce linguistic bias and annotation burden, and (3) computational inefficiency from multi-step chain-of-thought reasoning hinders real-time deployment. We propose LatentVLA, a novel framework that employs self-supervised latent action prediction to train VLA models without language annotations, eliminating linguistic bias while learning rich driving representations from unlabeled trajectory data. Through knowledge distillation, LatentVLA transfers the generalization capabilities of VLA models to efficient vision-based networks, achieving both robust performance and real-time efficiency. LatentVLA establishes a new state-of-the-art on the NAVSIM benchmark with a PDMS score of 92.4 and demonstrates strong zeroshot generalization on the nuScenes benchmark.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Xie, Chengen and Sun, Bin and Li, Tianyu and Wu, Junjie and Hao, Zhihui and Lang, XianPeng and Li, Hongyang},
	month = jan,
	year = {2026},
	note = {arXiv:2601.05611 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\Y6MTCYWL\\Xie 等 - 2026 - LatentVLA Efficient Vision-Language Models for Autonomous Driving via Latent Action Prediction.pdf:application/pdf},
}

@misc{wei_audio-vla_2025,
	title = {Audio-{VLA}: {Adding} {Contact} {Audio} {Perception} to {Vision}-{Language}-{Action} {Model} for {Robotic} {Manipulation}},
	shorttitle = {Audio-{VLA}},
	url = {http://arxiv.org/abs/2511.09958},
	doi = {10.48550/arXiv.2511.09958},
	abstract = {The Vision-Language-Action models (VLA) have achieved significant advances in robotic manipulation recently. However, vision-only VLA models create fundamental limitations, particularly in perceiving interactive and manipulation dynamic processes. This paper proposes Audio-VLA, a multimodal manipulation policy that leverages contact audio to perceive contact events and dynamic process feedback. Audio-VLA overcomes the vision-only constraints of VLA models. Additionally, this paper introduces the Task Completion Rate (TCR) metric to systematically evaluate dynamic operational processes. Audio-VLA employs pre-trained DINOv2 and SigLIP as visual encoders, AudioCLIP as the audio encoder, and Llama2 as the large language model backbone. We apply LoRA fine-tuning to these pre-trained modules to achieve robust cross-modal understanding of both visual and acoustic inputs. A multimodal projection layer aligns features from different modalities into the same feature space. Moreover RLBench and LIBERO simulation environments are enhanced by adding collision-based audio generation to provide realistic sound feedback during object interactions. Since current robotic manipulation evaluations focus on final outcomes rather than providing systematic assessment of dynamic operational processes, the proposed TCR metric measures how well robots perceive dynamic processes during manipulation, creating a more comprehensive evaluation metric. Extensive experiments on LIBERO, RLBench, and two real-world tasks demonstrate Audio-VLA's superior performance over vision-only comparative methods, while the TCR metric effectively quantifies dynamic process perception capabilities.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Wei, Xiangyi and Zhang, Haotian and Cao, Xinyi and Xie, Siyu and Ge, Weifeng and Li, Yang and Wang, Changbo},
	month = nov,
	year = {2025},
	note = {arXiv:2511.09958 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Sound},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\DLR5I4UQ\\Wei 等 - 2025 - Audio-VLA Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation.pdf:application/pdf},
}

@inproceedings{wu_momanipvla_2025,
	title = {Momanipvla: {Transferring} vision-language-action models for general mobile manipulation},
	shorttitle = {Momanipvla},
	url = {https://openaccess.thecvf.com/content/CVPR2025/html/Wu_MoManipVLA_Transferring_Vision-language-action_Models_for_General_Mobile_Manipulation_CVPR_2025_paper.html},
	urldate = {2026-02-04},
	booktitle = {Proceedings of the {Computer} {Vision} and {Pattern} {Recognition} {Conference}},
	author = {Wu, Zhenyu and Zhou, Yuheng and Xu, Xiuwei and Wang, Ziwei and Yan, Haibin},
	year = {2025},
	pages = {1714--1723},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\XM5YV6QJ\\Wu 等 - 2025 - Momanipvla Transferring vision-language-action models for general mobile manipulation.pdf:application/pdf},
}

@misc{liu_hybridvla_2025,
	title = {{HybridVLA}: {Collaborative} {Diffusion} and {Autoregression} in a {Unified} {Vision}-{Language}-{Action} {Model}},
	shorttitle = {{HybridVLA}},
	url = {http://arxiv.org/abs/2503.10631},
	doi = {10.48550/arXiv.2503.10631},
	abstract = {A fundamental objective of manipulation policy design is to endow robots to comprehend human instructions, reason about scene cues, and execute generalized actions in dynamic environments. Recent autoregressive vision-language-action (VLA) methods inherit common-sense reasoning capabilities from vision-language models (VLMs) for next action-token prediction. However, these methods quantize actions into discrete bins, which disrupts the continuity required for precise control. In contrast, existing diffusion-based VLA methods incorporate an additional diffusion head to predict continuous actions solely conditioned on feature representations extracted by the VLM, without fully leveraging the VLM's pretrained reasoning capabilities through token-level generation. To address these limitations, we introduce HybridVLA, a unified framework that absorbs the continuous nature of diffusion-based actions and the contextual reasoning of autoregression within a single large language model. To mitigate interference between the two generation paradigms, we propose a collaborative training recipe that seamlessly incorporates diffusion denoising into the next-token prediction process. With this recipe, we find these two action prediction methods not only reinforce each other but also exhibit varying strength across different tasks. Therefore, we design a collaborative action ensemble mechanism that adaptively fuses both predictions, leading to more robust control. HybridVLA outperforms previous state-of-the-art VLA methods by 14{\textbackslash}\% and 19{\textbackslash}\% in mean success rate on simulation and real-world tasks, respectively, while demonstrating stable manipulation in unseen configurations.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Liu, Jiaming and Chen, Hao and An, Pengju and Liu, Zhuoyang and Zhang, Renrui and Gu, Chenyang and Li, Xiaoqi and Guo, Ziyu and Chen, Sixiang and Liu, Mengzhen and Hou, Chengkai and Zhao, Mengdi and Zhou, KC alex and Heng, Pheng-Ann and Zhang, Shanghang},
	month = jun,
	year = {2025},
	note = {arXiv:2503.10631 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\87FQK7F3\\Liu 等 - 2025 - HybridVLA Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model.pdf:application/pdf},
}

@misc{liu_evovla_2025,
	title = {{EvoVLA}: {Self}-{Evolving} {Vision}-{Language}-{Action} {Model}},
	shorttitle = {{EvoVLA}},
	url = {http://arxiv.org/abs/2511.16166},
	doi = {10.48550/arXiv.2511.16166},
	abstract = {Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Liu, Zeting and Yang, Zida and Zhang, Zeyu and Tang, Hao},
	month = nov,
	year = {2025},
	note = {arXiv:2511.16166 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\RC397V5X\\Liu 等 - 2025 - EvoVLA Self-Evolving Vision-Language-Action Model.pdf:application/pdf},
}

@misc{zhong_survey_2025,
	title = {A {Survey} on {Vision}-{Language}-{Action} {Models}: {An} {Action} {Tokenization} {Perspective}},
	shorttitle = {A {Survey} on {Vision}-{Language}-{Action} {Models}},
	url = {http://arxiv.org/abs/2507.01925},
	doi = {10.48550/arXiv.2507.01925},
	abstract = {The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of {\textbackslash}textit\{action tokens\} that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhong, Yifan and Bai, Fengshuo and Cai, Shaofei and Huang, Xuchuan and Chen, Zhang and Zhang, Xiaowei and Wang, Yuanfei and Guo, Shaoyang and Guan, Tianrui and Lui, Ka Nam and Qi, Zhiquan and Liang, Yitao and Chen, Yuanpei and Yang, Yaodong},
	month = jul,
	year = {2025},
	note = {arXiv:2507.01925 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\3K6SIG7A\\Zhong 等 - 2025 - A Survey on Vision-Language-Action Models An Action Tokenization Perspective.pdf:application/pdf},
}

@misc{han_percept-wam_2025,
	title = {Percept-{WAM}: {Perception}-{Enhanced} {World}-{Awareness}-{Action} {Model} for {Robust} {End}-to-{End} {Autonomous} {Driving}},
	shorttitle = {Percept-{WAM}},
	url = {http://arxiv.org/abs/2511.19221},
	doi = {10.48550/arXiv.2511.19221},
	abstract = {Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Han, Jianhua and Tian, Meng and Zhu, Jiangtong and He, Fan and Zhang, Huixin and Guo, Sitong and Zhu, Dechang and Tang, Hao and Xu, Pei and Guo, Yuze and Niu, Minzhe and Zhu, Haojie and Dong, Qichao and Yan, Xuechao and Dong, Siyuan and Hou, Lu and Huang, Qingqiu and Jia, Xiaosong and Xu, Hang},
	month = nov,
	year = {2025},
	note = {arXiv:2511.19221 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\INGQQII4\\Han 等 - 2025 - Percept-WAM Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Drivi.pdf:application/pdf},
}

@misc{julg_refined_2025,
	title = {Refined {Policy} {Distillation}: {From} {VLA} {Generalists} to {RL} {Experts}},
	shorttitle = {Refined {Policy} {Distillation}},
	url = {http://arxiv.org/abs/2503.05833},
	doi = {10.48550/arXiv.2503.05833},
	abstract = {Vision-Language-Action Models (VLAs) have demonstrated remarkable generalization capabilities in real-world experiments. However, their success rates are often not on par with expert policies, and they require fine-tuning when the setup changes. In this work, we introduce Refined Policy Distillation (RPD), a novel Reinforcement Learning (RL)-based policy refinement method that bridges this performance gap through a combination of on-policy RL with behavioral cloning. The core idea of RPD is to distill and refine VLAs into compact, high-performing expert policies by guiding the student policy during RL exploration using the actions of a teacher VLA, resulting in increased sample efficiency and faster convergence. We complement our method by fine-tuned versions of Octo and OpenVLA for ManiSkill3 to evaluate RPD in simulation. While this is a key requirement for applying RL, it also yields new insights beyond existing studies on VLA performance in real-world settings. Our experimental results across various manipulation tasks show that RPD enables the RL student to learn expert policies that outperform the VLA teacher in both dense and sparse reward settings, while also achieving faster convergence than the RL baseline. Our approach is even robust to changes in camera perspective and can generalize to task variations that the underlying VLA cannot solve. Our code, dataset, VLA checkpoints, and videos are available at https://refined-policy-distillation.github.io},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Jülg, Tobias and Burgard, Wolfram and Walter, Florian},
	month = aug,
	year = {2025},
	note = {arXiv:2503.05833 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\CUETJXBX\\Jülg 等 - 2025 - Refined Policy Distillation From VLA Generalists to RL Experts.pdf:application/pdf},
}

@misc{jiang_better_2025,
	title = {The {Better} {You} {Learn}, {The} {Smarter} {You} {Prune}: {Towards} {Efficient} {Vision}-language-action {Models} via {Differentiable} {Token} {Pruning}},
	shorttitle = {The {Better} {You} {Learn}, {The} {Smarter} {You} {Prune}},
	url = {http://arxiv.org/abs/2509.12594},
	doi = {10.48550/arXiv.2509.12594},
	abstract = {We present LightVLA, a simple yet effective differentiable token pruning framework for vision-language-action (VLA) models. While VLA models have shown impressive capability in executing real-world robotic tasks, their deployment on resource-constrained platforms is often bottlenecked by the heavy attention-based computation over large sets of visual tokens. LightVLA addresses this challenge through adaptive, performance-driven pruning of visual tokens: It generates dynamic queries to evaluate visual token importance, and adopts Gumbel softmax to enable differentiable token selection. Through fine-tuning, LightVLA learns to preserve the most informative visual tokens while pruning tokens which do not contribute to task execution, thereby improving efficiency and performance simultaneously. Notably, LightVLA requires no heuristic magic numbers and introduces no additional trainable parameters, making it compatible with modern inference frameworks. Experimental results demonstrate that LightVLA outperforms different VLA models and existing token pruning methods across diverse tasks on the LIBERO benchmark, achieving higher success rates with substantially reduced computational overhead. Specifically, LightVLA reduces FLOPs and latency by 59.1\% and 38.2\% respectively, with a 2.6\% improvement in task success rate. Meanwhile, we also investigate the learnable query-based token pruning method LightVLA* with additional trainable parameters, which also achieves satisfactory performance. Our work reveals that as VLA pursues optimal performance, LightVLA spontaneously learns to prune tokens from a performance-driven perspective. To the best of our knowledge, LightVLA is the first work to apply adaptive visual token pruning to VLA tasks with the collateral goals of efficiency and performance, marking a significant step toward more efficient, powerful and practical real-time robotic systems.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Jiang, Titong and Jiang, Xuefeng and Ma, Yuan and Wen, Xin and Li, Bailin and Zhan, Kun and Jia, Peng and Liu, Yahui and Sun, Sheng and Lang, Xianpeng},
	month = sep,
	year = {2025},
	note = {arXiv:2509.12594 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\NGKXPQUC\\Jiang 等 - 2025 - The Better You Learn, The Smarter You Prune Towards Efficient Vision-language-action Models via Dif.pdf:application/pdf},
}

@misc{chen_internvla-m1_2025,
	title = {{InternVLA}-{M1}: {A} {Spatially} {Guided} {Vision}-{Language}-{Action} {Framework} for {Generalist} {Robot} {Policy}},
	shorttitle = {{InternVLA}-{M1}},
	url = {http://arxiv.org/abs/2510.13778},
	doi = {10.48550/arXiv.2510.13778},
	abstract = {We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6\% on SimplerEnv Google Robot, +17\% on WidowX, and +4.3\% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2\% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3\%, and with synthetic co-training, achieved +20.6\% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10\%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Chen, Xinyi and Chen, Yilun and Fu, Yanwei and Gao, Ning and Jia, Jiaya and Jin, Weiyang and Li, Hao and Mu, Yao and Pang, Jiangmiao and Qiao, Yu and Tian, Yang and Wang, Bin and Wang, Bolun and Wang, Fangjing and Wang, Hanqing and Wang, Tai and Wang, Ziqin and Wei, Xueyuan and Wu, Chao and Yang, Shuai and Ye, Jinhui and Yu, Junqiu and Zeng, Jia and Zhang, Jingjing and Zhang, Jinyu and Zhang, Shi and Zheng, Feng and Zhou, Bowen and Zhu, Yangkun},
	month = oct,
	year = {2025},
	note = {arXiv:2510.13778 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\PL6JLFLY\\Chen 等 - 2025 - InternVLA-M1 A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy.pdf:application/pdf},
}

@misc{zhang_safevla_2025,
	title = {{SafeVLA}: {Towards} {Safety} {Alignment} of {Vision}-{Language}-{Action} {Model} via {Constrained} {Learning}},
	shorttitle = {{SafeVLA}},
	url = {http://arxiv.org/abs/2503.03480},
	doi = {10.48550/arXiv.2503.03480},
	abstract = {Vision-language-action models (VLAs) show potential as generalist robot policies. However, these models pose extreme safety challenges during real-world deployment, including the risk of harm to the environment, the robot itself, and humans. How can safety constraints be explicitly integrated into VLAs? We address this by exploring an integrated safety approach (ISA), systematically modeling safety requirements, then actively eliciting diverse unsafe behaviors, effectively constraining VLA policies via safe reinforcement learning, and rigorously assuring their safety through targeted evaluations. Leveraging the constrained Markov decision process (CMDP) paradigm, ISA optimizes VLAs from a min-max perspective against elicited safety risks. Thus, policies aligned through this comprehensive approach achieve the following key features: (I) effective safety-performance trade-offs, reducing the cumulative cost of safety violations by 83.58\% compared to the state-of-the-art method, while also maintaining task success rate (+3.85\%). (II) strong safety assurance, with the ability to mitigate long-tail risks and handle extreme failure scenarios. (III) robust generalization of learned safety behaviors to various out-of-distribution perturbations. The effectiveness is evaluated on long-horizon mobile manipulation tasks. Our data, models and newly proposed benchmark environment are available at https://pku-safevla.github.io.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Borong and Zhang, Yuhao and Ji, Jiaming and Lei, Yingshan and Dai, Josef and Chen, Yuanpei and Yang, Yaodong},
	month = nov,
	year = {2025},
	note = {arXiv:2503.03480 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\55MQSA6C\\Zhang 等 - 2025 - SafeVLA Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning.pdf:application/pdf},
}

@misc{li_survey_2025,
	title = {Survey of {Vision}-{Language}-{Action} {Models} for {Embodied} {Manipulation}},
	url = {http://arxiv.org/abs/2508.15201},
	doi = {10.48550/arXiv.2508.15201},
	abstract = {Embodied intelligence systems, which enhance agent capabilities through continuous environment interactions, have garnered significant attention from both academia and industry. Vision-Language-Action models, inspired by advancements in large foundation models, serve as universal robotic control frameworks that substantially improve agent-environment interaction capabilities in embodied intelligence systems. This expansion has broadened application scenarios for embodied AI robots. This survey comprehensively reviews VLA models for embodied manipulation. Firstly, it chronicles the developmental trajectory of VLA architectures. Subsequently, we conduct a detailed analysis of current research across 5 critical dimensions: VLA model structures, training datasets, pre-training methods, post-training methods, and model evaluation. Finally, we synthesize key challenges in VLA development and real-world deployment, while outlining promising future research directions.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Haoran and Chen, Yuhui and Cui, Wenbo and Liu, Weiheng and Liu, Kai and Zhou, Mingcai and Zhang, Zhengtao and Zhao, Dongbin},
	month = nov,
	year = {2025},
	note = {arXiv:2508.15201 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\5BLBBQ5X\\Li 等 - 2025 - Survey of Vision-Language-Action Models for Embodied Manipulation.pdf:application/pdf},
}

@misc{zhang_inspire_2025,
	title = {{InSpire}: {Vision}-{Language}-{Action} {Models} with {Intrinsic} {Spatial} {Reasoning}},
	shorttitle = {{InSpire}},
	url = {http://arxiv.org/abs/2505.13888},
	doi = {10.48550/arXiv.2505.13888},
	abstract = {Leveraging pretrained Vision-Language Models (VLMs) to map language instruction and visual observations to raw low-level actions, Vision-Language-Action models (VLAs) hold great promise for achieving general-purpose robotic systems. Despite their advancements, existing VLAs tend to spuriously correlate task-irrelevant visual features with actions, limiting their generalization capacity beyond the training data. To tackle this challenge, we propose Intrinsic Spatial Reasoning (InSpire), a simple yet effective approach that mitigates the adverse effects of spurious correlations by boosting the spatial reasoning ability of VLAs. Specifically, InSpire redirects the VLA's attention to task-relevant factors by prepending the question "In which direction is the [object] relative to the robot?" to the language instruction and aligning the answer "right/left/up/down/front/back/grasped" and predicted actions with ground-truth. Notably, InSpire can be used as a plugin to enhance existing autoregressive VLAs, requiring no extra training data or interaction with other large models. Extensive experimental results in both simulation and real-world environments demonstrate the effectiveness and flexibility of our approach.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Ji and Wu, Shihan and Luo, Xu and Wu, Hao and Gao, Lianli and Shen, Heng Tao and Song, Jingkuan},
	month = sep,
	year = {2025},
	note = {arXiv:2505.13888 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\QSGMRHT7\\Zhang 等 - 2025 - InSpire Vision-Language-Action Models with Intrinsic Spatial Reasoning.pdf:application/pdf},
}

@misc{zhang_robustvla_2025,
	title = {{RobustVLA}: {Robustness}-{Aware} {Reinforcement} {Post}-{Training} for {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{RobustVLA}},
	url = {http://arxiv.org/abs/2511.01331},
	doi = {10.48550/arXiv.2511.01331},
	abstract = {Vision-Language-Action (VLA) models have recently emerged as powerful general-purpose policies for robotic manipulation, benefiting from large-scale multi-modal pre-training. However, they often fail to generalize reliably in out-of-distribution deployments, where unavoidable disturbances such as observation noise, sensor errors, or actuation perturbations become prevalent. While recent Reinforcement Learning (RL)-based post-training provides a practical means to adapt pre-trained VLA models, existing methods mainly emphasize reward maximization and overlook robustness to environmental uncertainty. In this work, we introduce RobustVLA, a lightweight online RL post-training method designed to explicitly enhance the resilience of VLA models. Through a systematic robustness analysis, we identify two key regularizations: Jacobian regularization, which mitigates sensitivity to observation noise, and smoothness regularization, which stabilizes policies under action perturbations. Extensive experiments across diverse robotic environments demonstrate that RobustVLA significantly outperforms prior state-of-the-art methods in robustness and reliability. Our results highlight the importance of principled robustness-aware RL post-training as a key step toward improving the reliability and robustness of VLA models.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Hongyin and Zhang, Shuo and Jin, Junxi and Zeng, Qixin and Li, Runze and Wang, Donglin},
	month = dec,
	year = {2025},
	note = {arXiv:2511.01331 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\5KPLUFXK\\Zhang 等 - 2025 - RobustVLA Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models.pdf:application/pdf},
}

@misc{hancock_actions_2025,
	title = {Actions as {Language}: {Fine}-{Tuning} {VLMs} into {VLAs} {Without} {Catastrophic} {Forgetting}},
	shorttitle = {Actions as {Language}},
	url = {http://arxiv.org/abs/2509.22195},
	doi = {10.48550/arXiv.2509.22195},
	abstract = {Fine-tuning vision-language models (VLMs) on robot teleoperation data to create vision-language-action (VLA) models is a promising paradigm for training generalist policies, but it suffers from a fundamental tradeoff: learning to produce actions often diminishes the VLM's foundational reasoning and multimodal understanding, hindering generalization to novel scenarios, instruction following, and semantic understanding. We argue that this catastrophic forgetting is due to a distribution mismatch between the VLM's internet-scale pretraining corpus and the robotics fine-tuning data. Inspired by this observation, we introduce VLM2VLA: a VLA training paradigm that first resolves this mismatch at the data level by representing low-level actions with natural language. This alignment makes it possible to train VLAs solely with Low-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and averting catastrophic forgetting. As a result, the VLM can be fine-tuned on robot teleoperation data without fundamentally altering the underlying architecture and without expensive co-training on internet-scale VLM datasets. Through extensive Visual Question Answering (VQA) studies and over 800 real-world robotics experiments, we demonstrate that VLM2VLA preserves the VLM's core capabilities, enabling zero-shot generalization to novel tasks that require open-world semantic reasoning and multilingual instruction following.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Hancock, Asher J. and Wu, Xindi and Zha, Lihan and Russakovsky, Olga and Majumdar, Anirudha},
	month = sep,
	year = {2025},
	note = {arXiv:2509.22195 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\W76M3SE7\\Hancock 等 - 2025 - Actions as Language Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting.pdf:application/pdf},
}

@misc{yin_deepthinkvla_2025,
	title = {{DeepThinkVLA}: {Enhancing} {Reasoning} {Capability} of {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{DeepThinkVLA}},
	url = {http://arxiv.org/abs/2511.15669},
	doi = {10.48550/arXiv.2511.15669},
	abstract = {Enabling Vision-Language-Action (VLA) models to "think before acting" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0\% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5\%, and the final RL stage provides a crucial 2\% boost to secure top performance.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Yin, Cheng and Lin, Yankai and Xu, Wang and Tam, Sikyuen and Zeng, Xiangrui and Liu, Zhiyuan and Yin, Zhouping},
	month = oct,
	year = {2025},
	note = {arXiv:2511.15669 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\EMJEHS42\\Yin 等 - 2025 - DeepThinkVLA Enhancing Reasoning Capability of Vision-Language-Action Models.pdf:application/pdf},
}

@misc{xu_wam-diff_2025,
	title = {{WAM}-{Diff}: {A} {Masked} {Diffusion} {VLA} {Framework} with {MoE} and {Online} {Reinforcement} {Learning} for {Autonomous} {Driving}},
	shorttitle = {{WAM}-{Diff}},
	url = {http://arxiv.org/abs/2512.11872},
	doi = {10.48550/arXiv.2512.11872},
	abstract = {End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Xu, Mingwang and Cui, Jiahao and Cai, Feipeng and Shang, Hanlin and Zhu, Zhihao and Luan, Shan and Xu, Yifang and Zhang, Neng and Li, Yaoyi and Cai, Jia and Zhu, Siyu},
	month = dec,
	year = {2025},
	note = {arXiv:2512.11872 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\KSYZV5LS\\Xu 等 - 2025 - WAM-Diff A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous.pdf:application/pdf},
}

@inproceedings{hancock_run-time_2025,
	title = {Run-time observation interventions make vision-language-action models more visually robust},
	url = {https://ieeexplore.ieee.org/abstract/document/11128017/},
	urldate = {2026-02-04},
	booktitle = {2025 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Hancock, Asher J. and Ren, Allen Z. and Majumdar, Anirudha},
	year = {2025},
	pages = {9499--9506},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\MTEDK5VD\\Hancock 等 - 2025 - Run-time observation interventions make vision-language-action models more visually robust.pdf:application/pdf},
}

@inproceedings{li_robonurse-vla_2025,
	title = {Robonurse-vla: {Robotic} scrub nurse system based on vision-language-action model},
	shorttitle = {Robonurse-vla},
	url = {https://ieeexplore.ieee.org/abstract/document/11246030/},
	urldate = {2026-02-04},
	booktitle = {2025 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Li, Shunlei and Wang, Jin and Dai, Rui and Ma, Wanyu and Ng, Wing Yin and Hu, Yingbai and Li, Zheng},
	year = {2025},
	pages = {3986--3993},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\84EMS2RM\\Li 等 - 2025 - Robonurse-vla Robotic scrub nurse system based on vision-language-action model.pdf:application/pdf},
}

@article{wang_vlatest_2025,
	title = {{VLATest}: {Testing} and {Evaluating} {Vision}-{Language}-{Action} {Models} for {Robotic} {Manipulation}},
	volume = {2},
	issn = {2994-970X},
	shorttitle = {{VLATest}},
	url = {https://dl.acm.org/doi/10.1145/3729343},
	doi = {10.1145/3729343},
	abstract = {The rapid advancement of generative AI and multi-modal foundation models has shown significant potential in advancing robotic manipulation. Vision-language-action (VLA) models, in particular, have emerged as a promising approach for visuomotor control by leveraging large-scale vision-language data and robot demonstrations. However, current VLA models are typically evaluated using a limited set of hand-crafted scenes, leaving their general performance and robustness in diverse scenarios largely unexplored. To address this gap, we present VLATest, a fuzzing framework designed to generate robotic manipulation scenes for testing VLA models. Based on VLATest, we conducted an empirical study to assess the performance of seven representative VLA models. Our study results revealed that current VLA models lack the robustness necessary for practical deployment. Additionally, we investigated the impact of various factors, including the number of confounding objects, lighting conditions, camera poses, unseen objects, and task instruction mutations, on the VLA model's performance. Our findings highlight the limitations of existing VLA models, emphasizing the need for further research to develop reliable and trustworthy VLA applications.},
	language = {en},
	number = {FSE},
	urldate = {2026-02-04},
	journal = {Proceedings of the ACM on Software Engineering},
	author = {Wang, Zhijie and Zhou, Zhehua and Song, Jiayang and Huang, Yuheng and Shu, Zhan and Ma, Lei},
	month = jun,
	year = {2025},
	pages = {1615--1638},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\5TP6W72A\\Wang 等 - 2025 - VLATest Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation.pdf:application/pdf},
}

@misc{wang_vla_2025,
	title = {{VLA} {Model} {Post}-{Training} via {Action}-{Chunked} {PPO} and {Self} {Behavior} {Cloning}},
	url = {http://arxiv.org/abs/2509.25718},
	doi = {10.48550/arXiv.2509.25718},
	abstract = {Reinforcement learning (RL) is a promising avenue for post-training vision-language-action (VLA) models, but practical deployment is hindered by sparse rewards and unstable training. This work mitigates these challenges by introducing an action chunk based on proximal policy optimization (PPO) with behavior cloning using self-collected demonstrations. Aggregating consecutive actions into chunks improves the temporal consistency of the policy and the density of informative feedback. In addition, an auxiliary behavior cloning loss is applied with a dynamically updated demonstration buffer that continually collects high-quality task trials during training. The relative weight between the action-chunked PPO objective and the self behavior clone auxiliary loss is adapted online to stabilize the post-training process. Experiments on the MetaWorld benchmark indicate improved performance over supervised fine-tuning, achieving a high success rate (0.93) and few steps to success (42.17). These results demonstrate the viability of RL for VLA post-training and help lay the groundwork for downstream VLA applications.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Wang, Si-Cheng and Xiang, Tian-Yu and Zhou, Xiao-Hu and Gui, Mei-Jiang and Xie, Xiao-Liang and Liu, Shi-Qi and Wang, Shuang-Yi and Jin, Ao-Qun and Hou, Zeng-Guang},
	month = sep,
	year = {2025},
	note = {arXiv:2509.25718 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\QPUEN6EZ\\Wang 等 - 2025 - VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning.pdf:application/pdf},
}

@misc{lin_evo-0_2025,
	title = {Evo-0: {Vision}-{Language}-{Action} {Model} with {Implicit} {Spatial} {Understanding}},
	shorttitle = {Evo-0},
	url = {http://arxiv.org/abs/2507.00416},
	doi = {10.48550/arXiv.2507.00416},
	abstract = {Vision-Language-Action (VLA) models have emerged as a promising framework for enabling generalist robots capable of perceiving, reasoning, and acting in the real world. These models usually build upon pretrained Vision-Language Models (VLMs), which excel at semantic understanding due to large-scale image and text pretraining. However, existing VLMs typically lack precise spatial understanding capabilities, as they are primarily tuned on 2D image-text pairs without 3D supervision. To address this limitation, recent approaches have incorporated explicit 3D inputs such as point clouds or depth maps, but this necessitates additional depth sensors or pre-trained depth estimation models, which may yield defective results. In contrast, our work introduces a plug-and-play module that implicitly incorporates 3D geometry features into VLA models by leveraging an off-the-shelf visual geometry foundation model. This integration provides the model with depth-aware visual representations, improving its ability to understand the geometric structure of the scene and the spatial relationships among objects from RGB images alone. We evaluate our method on a set of spatially challenging tasks in both simulation and the real world. Extensive evaluations show that our method significantly improves the performance of state-of-the-art VLA models across diverse scenarios.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Lin, Tao and Li, Gen and Zhong, Yilei and Zou, Yanwen and Du, Yuxin and Liu, Jiting and Gu, Encheng and Zhao, Bo},
	month = nov,
	year = {2025},
	note = {arXiv:2507.00416 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\LESMX9KW\\Lin 等 - 2025 - Evo-0 Vision-Language-Action Model with Implicit Spatial Understanding.pdf:application/pdf},
}

@article{hu_joint_2025,
	title = {Joint {Optimization} of {Fine}-grained {Representation} and {Workflow} {Orchestration} in {Metaverse} {Articulated} {Manipulation} {Auto}-generation by {VLA} {Method}},
	url = {https://ieeexplore.ieee.org/abstract/document/11207517/},
	urldate = {2026-02-04},
	journal = {IEEE Transactions on Services Computing},
	publisher = {IEEE},
	author = {Hu, Ruihan and He, Xiangdong and Huang, Feiyang and Zhao, Jiaxing and Cheng, Xinrui and Wang, Zhongjie},
	year = {2025},
	keywords = {Articulated manipulation auto-generation, Fine-grained Representation, Kinematics, Metaverse, Metaverse to Real-World, Optimization, Planning, Program processors, Robots, Service robots, Trajectory, Visual perception, Visualization, Workflow Optimization},
	file = {PDF:E\:\\Users\\AresZz\\Zotero\\storage\\EG6HC4BL\\Hu 等 - 2025 - Joint Optimization of Fine-grained Representation and Workflow Orchestration in Metaverse Articulate.pdf:application/pdf},
}

@misc{li_urbanvla_2025,
	title = {{UrbanVLA}: {A} {Vision}-{Language}-{Action} {Model} for {Urban} {Micromobility}},
	shorttitle = {{UrbanVLA}},
	url = {http://arxiv.org/abs/2510.23576},
	doi = {10.48550/arXiv.2510.23576},
	abstract = {Urban micromobility applications, such as delivery robots, demand reliable navigation across large-scale urban environments while following long-horizon route instructions. This task is particularly challenging due to the dynamic and unstructured nature of real-world city areas, yet most existing navigation methods remain tailored to short-scale and controllable scenarios. Effective urban micromobility requires two complementary levels of navigation skills: low-level capabilities such as point-goal reaching and obstacle avoidance, and high-level capabilities, such as route-visual alignment. To this end, we propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework designed for scalable urban navigation. Our method explicitly aligns noisy route waypoints with visual observations during execution, and subsequently plans trajectories to drive the robot. To enable UrbanVLA to master both levels of navigation, we employ a two-stage training pipeline. The process begins with Supervised Fine-Tuning (SFT) using simulated environments and trajectories parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on a mixture of simulation and real-world data, which enhances the model's safety and adaptability in real-world settings. Experiments demonstrate that UrbanVLA surpasses strong baselines by more than 55\% in the SocialNav task on MetaUrban. Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both scalability to large-scale urban environments and robustness against real-world uncertainties.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Anqi and Wang, Zhiyong and Zhang, Jiazhao and Li, Minghan and Qi, Yunpeng and Chen, Zhibo and Zhang, Zhizheng and Wang, He},
	month = oct,
	year = {2025},
	note = {arXiv:2510.23576 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\YTAL4ZPA\\Li 等 - 2025 - UrbanVLA A Vision-Language-Action Model for Urban Micromobility.pdf:application/pdf},
}

@misc{bendikas_focusing_2025,
	title = {Focusing on {What} {Matters}: {Object}-{Agent}-centric {Tokenization} for {Vision} {Language} {Action} models},
	shorttitle = {Focusing on {What} {Matters}},
	url = {http://arxiv.org/abs/2509.23655},
	doi = {10.48550/arXiv.2509.23655},
	abstract = {Vision-Language-Action (VLA) models offer a pivotal approach to learning robotic manipulation at scale by repurposing large pre-trained Vision-Language-Models (VLM) to output robotic actions. However, adapting VLMs for robotic domains comes with an unnecessarily high computational cost, which we attribute to the tokenization scheme of visual inputs. In this work, we aim to enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric Tokenization for VLAs. Building on the insights of object-centric representation learning, our method introduces an inductive bias towards scene objects and the agent's own visual information. As a result, we find that Oat-VLA can drastically reduce the number of visual tokens to just a few tokens without sacrificing performance. We reveal that Oat-VLA converges at least twice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in diverse real-world pick and place tasks.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Bendikas, Rokas and Dijkman, Daniel and Peschl, Markus and Haresh, Sanjay and Mazzaglia, Pietro},
	month = sep,
	year = {2025},
	note = {arXiv:2509.23655 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\X4AX2AAH\\Bendikas 等 - 2025 - Focusing on What Matters Object-Agent-centric Tokenization for Vision Language Action models.pdf:application/pdf},
}

@misc{liang_pixelvla_2025,
	title = {{PixelVLA}: {Advancing} {Pixel}-level {Understanding} in {Vision}-{Language}-{Action} {Model}},
	shorttitle = {{PixelVLA}},
	url = {http://arxiv.org/abs/2511.01571},
	doi = {10.48550/arXiv.2511.01571},
	abstract = {Vision-Language-Action models (VLAs) are emerging as powerful tools for learning generalizable visuomotor control policies. However, current VLAs are mostly trained on large-scale image-text-action data and remain limited in two key ways: (i) they struggle with pixel-level scene understanding, and (ii) they rely heavily on textual prompts, which reduces their flexibility in real-world settings. To address these challenges, we introduce PixelVLA, the first VLA model designed to support both pixel-level reasoning and multimodal prompting with text and visual inputs. Our approach is built on a new visuomotor instruction tuning framework that integrates a multiscale pixel-aware encoder with a visual prompting encoder. To train PixelVLA effectively, we further propose a two-stage automated annotation pipeline that generates Pixel-160K, a large-scale dataset with pixel-level annotations derived from existing robot data. Experiments on three standard VLA benchmarks and two VLA model variants show that PixelVLA improves manipulation success rates by 10.1\%-17.8\% over OpenVLA, while requiring only 1.5\% of its pretraining cost. These results demonstrate that PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments. The dataset and code will be released as open source.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Liang, Wenqi and Sun, Gan and He, Yao and Dong, Jiahua and Dai, Suyan and Laptev, Ivan and Khan, Salman and Cong, Yang},
	month = nov,
	year = {2025},
	note = {arXiv:2511.01571 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\5Z4N5BHZ\\Liang 等 - 2025 - PixelVLA Advancing Pixel-level Understanding in Vision-Language-Action Model.pdf:application/pdf},
}

@misc{budzianowski_edgevla_2025,
	title = {{EdgeVLA}: {Efficient} {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{EdgeVLA}},
	url = {http://arxiv.org/abs/2507.14049},
	doi = {10.48550/arXiv.2507.14049},
	abstract = {Vision-Language Models (VLMs) have emerged as a promising approach to address the data scarcity challenge in robotics, enabling the development of generalizable visuomotor control policies. While models like OpenVLA showcase the potential of this paradigm, deploying large-scale VLMs on resource-constrained mobile manipulation systems remains a significant hurdle. This paper introduces Edge VLA (EVLA), a novel approach designed to significantly enhance the inference speed of Vision-Language-Action (VLA) models. EVLA maintains the representational power of these models while enabling real-time performance on edge devices. We achieve this through two key innovations: 1) Eliminating the autoregressive requirement for end-effector position prediction, leading to a 7x speedup in inference, and 2) Leveraging the efficiency of Small Language Models (SLMs), demonstrating comparable training performance to larger models with significantly reduced computational demands. Our early results demonstrate that EVLA achieves comparable training characteristics to OpenVLA while offering substantial gains in inference speed and memory efficiency. We release our model checkpoints and training {\textbackslash}href\{https://github.com/kscalelabs/evla \}\{codebase\} to foster further research.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Budzianowski, Paweł and Maa, Wesley and Freed, Matthew and Mo, Jingxiang and Hsiao, Winston and Xie, Aaron and Młoduchowski, Tomasz and Tipnis, Viraj and Bolte, Benjamin},
	month = jul,
	year = {2025},
	note = {arXiv:2507.14049 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\U2NIU43Y\\Budzianowski 等 - 2025 - EdgeVLA Efficient Vision-Language-Action Models.pdf:application/pdf},
}

@inproceedings{wang_exploring_2025,
	title = {Exploring the adversarial vulnerabilities of vision-language-action models in robotics},
	url = {https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Exploring_the_Adversarial_Vulnerabilities_of_Vision-Language-Action_Models_in_Robotics_ICCV_2025_paper.html},
	urldate = {2026-02-04},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Wang, Taowen and Han, Cheng and Liang, James and Yang, Wenhao and Liu, Dongfang and Zhang, Luna Xinyu and Wang, Qifan and Luo, Jiebo and Tang, Ruixiang},
	year = {2025},
	pages = {6948--6958},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\2B8STIA2\\Wang 等 - 2025 - Exploring the adversarial vulnerabilities of vision-language-action models in robotics.pdf:application/pdf},
}

@misc{zhang_vtla_2025,
	title = {{VTLA}: {Vision}-{Tactile}-{Language}-{Action} {Model} with {Preference} {Learning} for {Insertion} {Manipulation}},
	shorttitle = {{VTLA}},
	url = {http://arxiv.org/abs/2505.09577},
	doi = {10.48550/arXiv.2505.09577},
	abstract = {While vision-language models have advanced significantly, their application in language-conditioned robotic manipulation is still underexplored, especially for contact-rich tasks that extend beyond visually dominant pick-and-place scenarios. To bridge this gap, we introduce Vision-Tactile-Language-Action model, a novel framework that enables robust policy generation in contact-intensive scenarios by effectively integrating visual and tactile inputs through cross-modal language grounding. A low-cost, multi-modal dataset has been constructed in a simulation environment, containing vision-tactile-action-instruction pairs specifically designed for the fingertip insertion task. Furthermore, we introduce Direct Preference Optimization (DPO) to offer regression-like supervision for the VTLA model, effectively bridging the gap between classification-based next token prediction loss and continuous robotic tasks. Experimental results show that the VTLA model outperforms traditional imitation learning methods (e.g., diffusion policies) and existing multi-modal baselines (TLA/VLA), achieving over 90\% success rates on unseen peg shapes. Finally, we conduct real-world peg-in-hole experiments to demonstrate the exceptional Sim2Real performance of the proposed VTLA model. For supplementary videos and results, please visit our project website: https://sites.google.com/view/vtla},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Chaofan and Hao, Peng and Cao, Xiaoge and Hao, Xiaoshuai and Cui, Shaowei and Wang, Shuo},
	month = may,
	year = {2025},
	note = {arXiv:2505.09577 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\ALH8I4ZU\\Zhang 等 - 2025 - VTLA Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation.pdf:application/pdf},
}

@article{wen_tinyvla_2025,
	title = {Tinyvla: {Towards} fast, data-efficient vision-language-action models for robotic manipulation},
	shorttitle = {Tinyvla},
	url = {https://ieeexplore.ieee.org/abstract/document/10900471/},
	urldate = {2026-02-04},
	journal = {IEEE Robotics and Automation Letters},
	publisher = {IEEE},
	author = {Wen, Junjie and Zhu, Yichen and Li, Jinming and Zhu, Minjie and Tang, Zhibin and Wu, Kun and Xu, Zhiyuan and Liu, Ning and Cheng, Ran and Shen, Chaomin},
	year = {2025},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\7UUN6NL3\\Wen 等 - 2025 - Tinyvla Towards fast, data-efficient vision-language-action models for robotic manipulation.pdf:application/pdf},
}

@misc{yuan_autodrive-r2_2025,
	title = {{AutoDrive}-{R}\${\textasciicircum}2\$: {Incentivizing} {Reasoning} and {Self}-{Reflection} {Capacity} for {VLA} {Model} in {Autonomous} {Driving}},
	shorttitle = {{AutoDrive}-{R}\${\textasciicircum}2\$},
	url = {http://arxiv.org/abs/2509.01944},
	doi = {10.48550/arXiv.2509.01944},
	abstract = {Vision-Language-Action (VLA) models in autonomous driving systems have recently demonstrated transformative potential by integrating multimodal perception with decision-making capabilities. However, the interpretability and coherence of the decision process and the plausibility of action sequences remain largely underexplored. To address these issues, we propose AutoDrive-R\${\textasciicircum}2\$, a novel VLA framework that enhances both reasoning and self-reflection capabilities of autonomous driving systems through chain-of-thought (CoT) processing and reinforcement learning (RL). Specifically, we first propose an innovative CoT dataset named nuScenesR\${\textasciicircum}2\$-6K for supervised fine-tuning, which effectively builds cognitive bridges between input information and output trajectories through a four-step logical chain with self-reflection for validation. Moreover, to maximize both reasoning and self-reflection during the RL stage, we further employ the Group Relative Policy Optimization (GRPO) algorithm within a physics-grounded reward framework that incorporates spatial alignment, vehicle dynamic, and temporal smoothness criteria to ensure reliable and realistic trajectory planning. Extensive evaluation results across both nuScenes and Waymo datasets demonstrates the state-of-the-art performance and robust generalization capacity of our proposed method.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Yuan, Zhenlong and Qian, Chengxuan and Tang, Jing and Chen, Rui and Song, Zijian and Sun, Lei and Chu, Xiangxiang and Cai, Yujun and Zhang, Dapeng and Li, Shuo},
	month = dec,
	year = {2025},
	note = {arXiv:2509.01944 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\B7HALQDR\\Yuan 等 - 2025 - AutoDrive-R\$^2\$ Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Dr.pdf:application/pdf},
}

@misc{berg_semantic_2025,
	title = {Semantic {World} {Models}},
	url = {http://arxiv.org/abs/2510.19818},
	doi = {10.48550/arXiv.2510.19818},
	abstract = {Planning with world models offers a powerful paradigm for robotic control. Conventional approaches train a model to predict future frames conditioned on current frames and actions, which can then be used for planning. However, the objective of predicting future pixels is often at odds with the actual planning objective; strong pixel reconstruction does not always correlate with good planning decisions. This paper posits that instead of reconstructing future frames as pixels, world models only need to predict task-relevant semantic information about the future. For such prediction the paper poses world modeling as a visual question answering problem about semantic information in future frames. This perspective allows world modeling to be approached with the same tools underlying vision language models. Thus vision language models can be trained as "semantic" world models through a supervised finetuning process on image-action-text data, enabling planning for decision-making while inheriting many of the generalization and robustness properties from the pretrained vision-language models. The paper demonstrates how such a semantic world model can be used for policy improvement on open-ended robotics tasks, leading to significant generalization improvements over typical paradigms of reconstruction-based action-conditional world modeling. Website available at https://weirdlabuw.github.io/swm.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Berg, Jacob and Zhu, Chuning and Bao, Yanda and Durugkar, Ishan and Gupta, Abhishek},
	month = oct,
	year = {2025},
	note = {arXiv:2510.19818 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\JDRH254Q\\Berg 等 - 2025 - Semantic World Models.pdf:application/pdf},
}

@misc{yang_vlaser_2026,
	title = {Vlaser: {Vision}-{Language}-{Action} {Model} with {Synergistic} {Embodied} {Reasoning}},
	shorttitle = {Vlaser},
	url = {http://arxiv.org/abs/2510.11027},
	doi = {10.48550/arXiv.2510.11027},
	abstract = {While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Yang, Ganlin and Zhang, Tianyi and Hao, Haoran and Wang, Weiyun and Liu, Yibin and Wang, Dehui and Chen, Guanzhou and Cai, Zijian and Chen, Junting and Su, Weijie and Zhou, Wengang and Qiao, Yu and Dai, Jifeng and Pang, Jiangmiao and Luo, Gen and Wang, Wenhai and Mu, Yao and Hou, Zhi},
	month = jan,
	year = {2026},
	note = {arXiv:2510.11027 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\7T3QECPU\\Yang 等 - 2026 - Vlaser Vision-Language-Action Model with Synergistic Embodied Reasoning.pdf:application/pdf},
}

@misc{xu_model-agnostic_2025,
	title = {Model-agnostic {Adversarial} {Attack} and {Defense} for {Vision}-{Language}-{Action} {Models}},
	url = {http://arxiv.org/abs/2510.13237},
	doi = {10.48550/arXiv.2510.13237},
	abstract = {Vision-Language-Action (VLA) models have achieved revolutionary progress in robot learning, enabling robots to execute complex physical robot tasks from natural language instructions. Despite this progress, their adversarial robustness remains underexplored. In this work, we propose both adversarial patch attack and corresponding defense strategies for VLA models. We first introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic adversarial attack that generates patches directly placeable within the camera's view. In comparison to prior methods, EDPA can be readily applied to different VLA models without requiring prior knowledge of the model architecture, or the controlled robotic manipulator. EDPA constructs these patches by (i) disrupting the semantic alignment between visual and textual latent representations, and (ii) maximizing the discrepancy of latent representations between adversarial and corresponding clean visual inputs. Through the optimization of these objectives, EDPA distorts the VLA's interpretation of visual information, causing the model to repeatedly generate incorrect actions and ultimately result in failure to complete the given robotic task. To counter this, we propose an adversarial fine-tuning scheme for the visual encoder, in which the encoder is optimized to produce similar latent representations for both clean and adversarially perturbed visual inputs. Extensive evaluations on the widely recognized LIBERO robotic simulation benchmark demonstrate that EDPA substantially increases the task failure rate of cutting-edge VLA models, while our proposed defense effectively mitigates this degradation. The codebase is accessible via the homepage at https://edpa-attack.github.io/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Xu, Haochuan and Koh, Yun Sing and Huang, Shuhuai and Zhou, Zirun and Wang, Di and Sakuma, Jun and Zhang, Jingfeng},
	month = oct,
	year = {2025},
	note = {arXiv:2510.13237 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\E684CUMA\\Xu 等 - 2025 - Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models.pdf:application/pdf},
}

@inproceedings{dey_revla_2025,
	title = {Revla: {Reverting} visual domain limitation of robotic foundation models},
	shorttitle = {Revla},
	url = {https://ieeexplore.ieee.org/abstract/document/11128823/},
	urldate = {2026-02-04},
	booktitle = {2025 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Dey, Sombit and Zaech, Jan-Nico and Nikolov, Nikolay and Van Gool, Luc and Paudel, Danda Pani},
	year = {2025},
	pages = {8679--8686},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\XJEDJH79\\Dey 等 - 2025 - Revla Reverting visual domain limitation of robotic foundation models.pdf:application/pdf},
}

@misc{wen_dexvla_2025,
	title = {{DexVLA}: {Vision}-{Language} {Model} with {Plug}-{In} {Diffusion} {Expert} for {General} {Robot} {Control}},
	shorttitle = {{DexVLA}},
	url = {http://arxiv.org/abs/2502.05855},
	doi = {10.48550/arXiv.2502.05855},
	abstract = {Enabling robots to perform diverse tasks across varied environments is a central challenge in robot learning. While vision-language-action (VLA) models have shown promise for generalizable robot skills, realizing their full potential requires addressing limitations in action representation and efficient training. Current VLA models often focus on scaling the vision-language model (VLM) component, while the action space representation remains a critical bottleneck. This paper introduces DexVLA, a novel framework designed to enhance the efficiency and generalization capabilities of VLAs for complex, long-horizon tasks across diverse robot embodiments. DexVLA features a novel diffusion-based action expert, scaled to one billion parameters, designed for cross-embodiment learning. A novel embodiment curriculum learning strategy facilitates efficient training: (1) pre-training the diffusion expert that is separable from the VLA on cross-embodiment data, (2) aligning the VLA model to specific embodiments, and (3) post-training for rapid adaptation to new tasks. We conduct comprehensive experiments across multiple embodiments, including single-arm, bimanual, and dexterous hand, demonstrating DexVLA's adaptability to challenging tasks without task-specific adaptation, its ability to learn dexterous skills on novel embodiments with limited data, and its capacity to complete complex, long-horizon tasks using only direct language prompting, such as laundry folding. In all settings, our method demonstrates superior performance compared to state-of-the-art models like Octo, OpenVLA, and Diffusion Policy.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Wen, Junjie and Zhu, Yichen and Li, Jinming and Tang, Zhibin and Shen, Chaomin and Feng, Feifei},
	month = aug,
	year = {2025},
	note = {arXiv:2502.05855 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\NT58REMC\\Wen 等 - 2025 - DexVLA Vision-Language Model with Plug-In Diffusion Expert for General Robot Control.pdf:application/pdf},
}

@misc{hirose_omnivla_2025,
	title = {{OmniVLA}: {An} {Omni}-{Modal} {Vision}-{Language}-{Action} {Model} for {Robot} {Navigation}},
	shorttitle = {{OmniVLA}},
	url = {http://arxiv.org/abs/2509.19480},
	doi = {10.48550/arXiv.2509.19480},
	abstract = {Humans can flexibly interpret and compose different goal specifications, such as language instructions, spatial coordinates, or visual references, when navigating to a destination. In contrast, most existing robotic navigation policies are trained on a single modality, limiting their adaptability to real-world scenarios where different forms of goal specification are natural and complementary. In this work, we present a training framework for robotic foundation models that enables omni-modal goal conditioning for vision-based navigation. Our approach leverages a high-capacity vision-language-action (VLA) backbone and trains with three primary goal modalities: 2D poses, egocentric images, and natural language, as well as their combinations, through a randomized modality fusion strategy. This design not only expands the pool of usable datasets but also encourages the policy to develop richer geometric, semantic, and visual representations. The resulting model, OmniVLA, achieves strong generalization to unseen environments, robustness to scarce modalities, and the ability to follow novel natural language instructions. We demonstrate that OmniVLA outperforms specialist baselines across modalities and offers a flexible foundation for fine-tuning to new modalities and tasks. We believe OmniVLA provides a step toward broadly generalizable and flexible navigation policies, and a scalable path for building omni-modal robotic foundation models. We present videos showcasing OmniVLA performance and will release its checkpoints and training code on our project page.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Hirose, Noriaki and Glossop, Catherine and Shah, Dhruv and Levine, Sergey},
	month = sep,
	year = {2025},
	note = {arXiv:2509.19480 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\ZDAXFUID\\Hirose 等 - 2025 - OmniVLA An Omni-Modal Vision-Language-Action Model for Robot Navigation.pdf:application/pdf},
}

@misc{fang_dualvla_2025,
	title = {{DualVLA}: {Building} a {Generalizable} {Embodied} {Agent} via {Partial} {Decoupling} of {Reasoning} and {Action}},
	shorttitle = {{DualVLA}},
	url = {http://arxiv.org/abs/2511.22134},
	doi = {10.48550/arXiv.2511.22134},
	abstract = {To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we refer to as action degeneration. To address this issue, we propose DualVLA, which enhances action performance through carefully designed post-training while still preserving reasoning capability. We first introduce a dual-layer data pruning method that removes redundant embodied reasoning, preventing it from adversely influencing action learning. To further strengthen action generation, we design a dual-teacher adaptive distillation strategy that assigns different supervision signals to different data domains while maintaining reasoning ability. To fill the evaluation gap for generalist VLAs, we also propose VLA Score, which decouples VLA capability into reasoning, intention, action, and alignment dimensions for a more fine-grained assessment. Experiments show that DualVLA achieves an average success rate of 61.0 in SimplerEnv and an average score of 65.4 across eight competitive multimodal benchmarks, demonstrating a stronger balance between precise action execution and multimodal understanding. Project Website: https://costaliya.github.io/DualVLA/.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Fang, Zhen and Liu, Zhuoyang and Liu, Jiaming and Chen, Hao and Zeng, Yu and Huang, Shiting and Chen, Zehui and Chen, Lin and Zhang, Shanghang and Zhao, Feng},
	month = nov,
	year = {2025},
	note = {arXiv:2511.22134 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\D9B6P8N5\\Fang 等 - 2025 - DualVLA Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action.pdf:application/pdf},
}

@inproceedings{wen_diffusionvla_2025,
	title = {{DiffusionVLA}: {Scaling} {Robot} {Foundation} {Models} via {Unified} {Diffusion} and {Autoregression}},
	shorttitle = {{DiffusionVLA}},
	url = {https://openreview.net/forum?id=VdwdU81Uzy},
	urldate = {2026-02-04},
	booktitle = {Forty-second {International} {Conference} on {Machine} {Learning}},
	author = {Wen, Junjie and Zhu, Yichen and Zhu, Minjie and Tang, Zhibin and Li, Jinming and Zhou, Zhongyi and Liu, Xiaoyu and Shen, Chaomin and Peng, Yaxin and Feng, Feifei},
	year = {2025},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\N84M39TE\\Wen 等 - 2025 - DiffusionVLA Scaling Robot Foundation Models via Unified Diffusion and Autoregression.pdf:application/pdf},
}

@article{liu_hybridvla_2025-1,
	title = {{HybridVLA}: {Collaborative} {Autoregression} and {Diffusion} in a {Unified} {Vision}-{Language}-{Action} {Model}},
	shorttitle = {{HybridVLA}},
	url = {https://openreview.net/forum?id=8VyjwyLuSl},
	urldate = {2026-02-04},
	author = {Liu, Jiaming and Chen, Hao and An, Pengju and Liu, Zhuoyang and Zhang, Renrui and Gu, Chenyang and Li, Xiaoqi and Guo, Ziyu and Chen, Sixiang and Liu, Mengzhen},
	year = {2025},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\F77F5F3U\\Liu 等 - 2025 - HybridVLA Collaborative Autoregression and Diffusion in a Unified Vision-Language-Action Model.pdf:application/pdf},
}

@misc{xue_leverb_2025,
	title = {{LeVERB}: {Humanoid} {Whole}-{Body} {Control} with {Latent} {Vision}-{Language} {Instruction}},
	shorttitle = {{LeVERB}},
	url = {http://arxiv.org/abs/2506.13751},
	doi = {10.48550/arXiv.2506.13751},
	abstract = {Vision-language-action (VLA) models have demonstrated strong semantic understanding and zero-shot generalization, yet most existing systems assume an accurate low-level controller with hand-crafted action "vocabulary" such as end-effector pose or root velocity. This assumption confines prior work to quasi-static tasks and precludes the agile, whole-body behaviors required by humanoid whole-body control (WBC) tasks. To capture this gap in the literature, we start by introducing the first sim-to-real-ready, vision-language, closed-loop benchmark for humanoid WBC, comprising over 150 tasks from 10 categories. We then propose LeVERB: Latent Vision-Language-Encoded Robot Behavior, a hierarchical latent instruction-following framework for humanoid vision-language WBC, the first of its kind. At the top level, a vision-language policy learns a latent action vocabulary from synthetically rendered kinematic demonstrations; at the low level, a reinforcement-learned WBC policy consumes these latent verbs to generate dynamics-level commands. In our benchmark, LeVERB can zero-shot attain a 80\% success rate on simple visual navigation tasks, and 58.5\% success rate overall, outperforming naive hierarchical whole-body VLA implementation by 7.8 times.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Xue, Haoru and Huang, Xiaoyu and Niu, Dantong and Liao, Qiayuan and Kragerud, Thomas and Gravdahl, Jan Tommy and Peng, Xue Bin and Shi, Guanya and Darrell, Trevor and Sreenath, Koushil and Sastry, Shankar},
	month = sep,
	year = {2025},
	note = {arXiv:2506.13751 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\NB6BK62P\\Xue 等 - 2025 - LeVERB Humanoid Whole-Body Control with Latent Vision-Language Instruction.pdf:application/pdf},
}

@misc{serpiva_racevla_2025,
	title = {{RaceVLA}: {VLA}-based {Racing} {Drone} {Navigation} with {Human}-like {Behaviour}},
	shorttitle = {{RaceVLA}},
	url = {http://arxiv.org/abs/2503.02572},
	doi = {10.48550/arXiv.2503.02572},
	abstract = {RaceVLA presents an innovative approach for autonomous racing drone navigation by leveraging Visual-Language-Action (VLA) to emulate human-like behavior. This research explores the integration of advanced algorithms that enable drones to adapt their navigation strategies based on real-time environmental feedback, mimicking the decision-making processes of human pilots. The model, fine-tuned on a collected racing drone dataset, demonstrates strong generalization despite the complexity of drone racing environments. RaceVLA outperforms OpenVLA in motion (75.0 vs 60.0) and semantic generalization (45.5 vs 36.3), benefiting from the dynamic camera and simplified motion tasks. However, visual (79.6 vs 87.0) and physical (50.0 vs 76.7) generalization were slightly reduced due to the challenges of maneuvering in dynamic environments with varying object sizes. RaceVLA also outperforms RT-2 across all axes - visual (79.6 vs 52.0), motion (75.0 vs 55.0), physical (50.0 vs 26.7), and semantic (45.5 vs 38.8), demonstrating its robustness for real-time adjustments in complex environments. Experiments revealed an average velocity of 1.04 m/s, with a maximum speed of 2.02 m/s, and consistent maneuverability, demonstrating RaceVLA's ability to handle high-speed scenarios effectively. These findings highlight the potential of RaceVLA for high-performance navigation in competitive racing contexts. The RaceVLA codebase, pretrained weights, and dataset are available at this http URL: https://racevla.github.io/},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Serpiva, Valerii and Lykov, Artem and Myshlyaev, Artyom and Khan, Muhammad Haris and Abdulkarim, Ali Alridha and Sautenkov, Oleg and Tsetserukou, Dzmitry},
	month = mar,
	year = {2025},
	note = {arXiv:2503.02572 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\RNTPDE86\\Serpiva 等 - 2025 - RaceVLA VLA-based Racing Drone Navigation with Human-like Behaviour.pdf:application/pdf},
}

@misc{zhao_more_2025,
	title = {{MoRE}: {Unlocking} {Scalability} in {Reinforcement} {Learning} for {Quadruped} {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{MoRE}},
	url = {http://arxiv.org/abs/2503.08007},
	doi = {10.48550/arXiv.2503.08007},
	abstract = {Developing versatile quadruped robots that can smoothly perform various actions and tasks in real-world environments remains a significant challenge. This paper introduces a novel vision-language-action (VLA) model, mixture of robotic experts (MoRE), for quadruped robots that aim to introduce reinforcement learning (RL) for fine-tuning large-scale VLA models with a large amount of mixed-quality data. MoRE integrates multiple low-rank adaptation modules as distinct experts within a dense multi-modal large language model (MLLM), forming a sparse-activated mixture-of-experts model. This design enables the model to effectively adapt to a wide array of downstream tasks. Moreover, we employ a reinforcement learning-based training objective to train our model as a Q-function after deeply exploring the structural properties of our tasks. Effective learning from automatically collected mixed-quality data enhances data efficiency and model performance. Extensive experiments demonstrate that MoRE outperforms all baselines across six different skills and exhibits superior generalization capabilities in out-of-distribution scenarios. We further validate our method in real-world scenarios, confirming the practicality of our approach and laying a solid foundation for future research on multi-task learning in quadruped robots.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhao, Han and Song, Wenxuan and Wang, Donglin and Tong, Xinyang and Ding, Pengxiang and Cheng, Xuelian and Ge, Zongyuan},
	month = mar,
	year = {2025},
	note = {arXiv:2503.08007 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\Z352UWYN\\Zhao 等 - 2025 - MoRE Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models.pdf:application/pdf},
}

@misc{grover_enhancing_2025,
	title = {Enhancing {Generalization} in {Vision}-{Language}-{Action} {Models} by {Preserving} {Pretrained} {Representations}},
	url = {http://arxiv.org/abs/2509.11417},
	doi = {10.48550/arXiv.2509.11417},
	abstract = {Vision-language-action (VLA) models finetuned from vision-language models (VLMs) hold the promise of leveraging rich pretrained representations to build generalist robots across diverse tasks and environments. However, direct fine-tuning on robot data often disrupts these representations and limits generalization. We present a framework that better preserves pretrained features while adapting them for robot manipulation. Our approach introduces three components: (i) a dual-encoder design with one frozen vision encoder to retain pretrained features and another trainable for task adaptation, (ii) a string-based action tokenizer that casts continuous actions into character sequences aligned with the model's pretraining domain, and (iii) a co-training strategy that combines robot demonstrations with vision-language datasets emphasizing spatial reasoning and affordances. Evaluations in simulation and on real robots show that our method improves robustness to visual perturbations, generalization to novel instructions and environments, and overall task success compared to baselines.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Grover, Shresth and Gopalkrishnan, Akshay and Ai, Bo and Christensen, Henrik I. and Su, Hao and Li, Xuanlin},
	month = sep,
	year = {2025},
	note = {arXiv:2509.11417 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\6DSBACZR\\Grover 等 - 2025 - Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations.pdf:application/pdf},
}

@misc{lin_vote_2025,
	title = {{VOTE}: {Vision}-{Language}-{Action} {Optimization} with {Trajectory} {Ensemble} {Voting}},
	shorttitle = {{VOTE}},
	url = {http://arxiv.org/abs/2507.05116},
	doi = {10.48550/arXiv.2507.05116},
	abstract = {Recent large-scale Vision Language Action (VLA) models have shown superior performance in robotic manipulation tasks guided by natural language. However, current VLA models suffer from two drawbacks: (i) generation of massive tokens leading to high inference latency and increased training cost, and (ii) insufficient utilization of generated actions resulting in potential performance loss. To address these issues, we develop a training framework to finetune VLA models for generating significantly fewer action tokens with high parallelism, effectively reducing inference latency and training cost. Furthermore, we introduce an inference optimization technique with a novel voting-based ensemble strategy to combine current and previous action predictions, improving the utilization of generated actions and overall performance. Our results demonstrate that we achieve superior performance compared with state-of-the-art VLA models, achieving significantly higher success rates and 39\${\textbackslash}times\$ faster inference than OpenVLA with 46 Hz throughput on edge platforms, demonstrating practical deployability. The code is available at https://github.com/LukeLIN-web/VOTE.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Lin, Juyi and Taherin, Amir and Akbari, Arash and Akbari, Arman and Lu, Lei and Chen, Guangyu and Padir, Taskin and Yang, Xiaomeng and Chen, Weiwei and Li, Yiqian and Lin, Xue and Kaeli, David and Zhao, Pu and Wang, Yanzhi},
	month = oct,
	year = {2025},
	note = {arXiv:2507.05116 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\IBKRVX98\\Lin 等 - 2025 - VOTE Vision-Language-Action Optimization with Trajectory Ensemble Voting.pdf:application/pdf},
}

@misc{li_hamster_2025,
	title = {{HAMSTER}: {Hierarchical} {Action} {Models} {For} {Open}-{World} {Robot} {Manipulation}},
	shorttitle = {{HAMSTER}},
	url = {http://arxiv.org/abs/2502.05485},
	doi = {10.48550/arXiv.2502.05485},
	abstract = {Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation. A promising remedy is to leverage cheaper, off-domain data such as action-free videos, hand-drawn sketches or simulation data. In this work, we posit that hierarchical vision-language-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions. In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description. The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy's burden on complex task-level reasoning. We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences on embodiments, dynamics, visual appearances and task semantics, etc. In the real-robot experiments, we observe an average of 20\% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50\% relative gain. Visual results, code, and dataset are provided at: https://hamster-robot.github.io/},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Yi and Deng, Yuquan and Zhang, Jesse and Jang, Joel and Memmel, Marius and Yu, Raymond and Garrett, Caelan Reed and Ramos, Fabio and Fox, Dieter and Li, Anqi and Gupta, Abhishek and Goyal, Ankit},
	month = may,
	year = {2025},
	note = {arXiv:2502.05485 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\8FLCW72I\\Li 等 - 2025 - HAMSTER Hierarchical Action Models For Open-World Robot Manipulation.pdf:application/pdf},
}

@misc{yan_when_2025,
	title = {When {Alignment} {Fails}: {Multimodal} {Adversarial} {Attacks} on {Vision}-{Language}-{Action} {Models}},
	shorttitle = {When {Alignment} {Fails}},
	url = {http://arxiv.org/abs/2511.16203},
	doi = {10.48550/arXiv.2511.16203},
	abstract = {Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Yan, Yuping and Xie, Yuhan and Zhang, Yixin and Lyu, Lingjuan and Wang, Handing and Jin, Yaochu},
	month = dec,
	year = {2025},
	note = {arXiv:2511.16203 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\738HXU7N\\Yan 等 - 2025 - When Alignment Fails Multimodal Adversarial Attacks on Vision-Language-Action Models.pdf:application/pdf},
}

@misc{zhang_iref-vla_2025,
	title = {{IRef}-{VLA}: {A} {Benchmark} for {Interactive} {Referential} {Grounding} with {Imperfect} {Language} in {3D} {Scenes}},
	shorttitle = {{IRef}-{VLA}},
	url = {http://arxiv.org/abs/2503.17406},
	doi = {10.48550/arXiv.2503.17406},
	abstract = {With the recent rise of large language models, vision-language models, and other general foundation models, there is growing potential for multimodal, multi-task robotics that can operate in diverse environments given natural language input. One such application is indoor navigation using natural language instructions. However, despite recent progress, this problem remains challenging due to the 3D spatial reasoning and semantic understanding required. Additionally, the language used may be imperfect or misaligned with the scene, further complicating the task. To address this challenge, we curate a benchmark dataset, IRef-VLA, for Interactive Referential Vision and Language-guided Action in 3D Scenes with imperfect references. IRef-VLA is the largest real-world dataset for the referential grounding task, consisting of over 11.5K scanned 3D rooms from existing datasets, 7.6M heuristically generated semantic relations, and 4.7M referential statements. Our dataset also contains semantic object and room annotations, scene graphs, navigable free space annotations, and is augmented with statements where the language has imperfections or ambiguities. We verify the generalizability of our dataset by evaluating with state-of-the-art models to obtain a performance baseline and also develop a graph-search baseline to demonstrate the performance bound and generation of alternatives using scene-graph knowledge. With this benchmark, we aim to provide a resource for 3D scene understanding that aids the development of robust, interactive navigation systems. The dataset and all source code is publicly released at https://github.com/HaochenZ11/IRef-VLA.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Haochen and Zantout, Nader and Kachana, Pujith and Zhang, Ji and Wang, Wenshan},
	month = mar,
	year = {2025},
	note = {arXiv:2503.17406 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\9M9SZE4I\\Zhang 等 - 2025 - IRef-VLA A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes.pdf:application/pdf},
}

@misc{ding_humanoid-vla_2025,
	title = {Humanoid-{VLA}: {Towards} {Universal} {Humanoid} {Control} with {Visual} {Integration}},
	shorttitle = {Humanoid-{VLA}},
	url = {http://arxiv.org/abs/2502.14795},
	doi = {10.48550/arXiv.2502.14795},
	abstract = {This paper addresses the limitations of current humanoid robot control frameworks, which primarily rely on reactive mechanisms and lack autonomous interaction capabilities due to data scarcity. We propose Humanoid-VLA, a novel framework that integrates language understanding, egocentric scene perception, and motion control, enabling universal humanoid control. Humanoid-VLA begins with language-motion pre-alignment using non-egocentric human motion datasets paired with textual descriptions, allowing the model to learn universal motion patterns and action semantics. We then incorporate egocentric visual context through a parameter efficient video-conditioned fine-tuning, enabling context-aware motion generation. Furthermore, we introduce a self-supervised data augmentation strategy that automatically generates pseudoannotations directly derived from motion data. This process converts raw motion sequences into informative question-answer pairs, facilitating the effective use of large-scale unlabeled video data. Built upon whole-body control architectures, extensive experiments show that Humanoid-VLA achieves object interaction and environment exploration tasks with enhanced contextual awareness, demonstrating a more human-like capacity for adaptive and intelligent engagement.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Ding, Pengxiang and Ma, Jianfei and Tong, Xinyang and Zou, Binghong and Luo, Xinxin and Fan, Yiguo and Wang, Ting and Lu, Hongchao and Mo, Panzhong and Liu, Jinxin and Wang, Yuefan and Zhou, Huaicheng and Feng, Wenshuo and Liu, Jiacheng and Huang, Siteng and Wang, Donglin},
	month = feb,
	year = {2025},
	note = {arXiv:2502.14795 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\EY7SETL6\\Ding 等 - 2025 - Humanoid-VLA Towards Universal Humanoid Control with Visual Integration.pdf:application/pdf},
}

@misc{cui_end--end_2025,
	title = {End-to-{End} {Dexterous} {Arm}-{Hand} {VLA} {Policies} via {Shared} {Autonomy}: {VR} {Teleoperation} {Augmented} by {Autonomous} {Hand} {VLA} {Policy} for {Efficient} {Data} {Collection}},
	shorttitle = {End-to-{End} {Dexterous} {Arm}-{Hand} {VLA} {Policies} via {Shared} {Autonomy}},
	url = {http://arxiv.org/abs/2511.00139},
	doi = {10.48550/arXiv.2511.00139},
	abstract = {Achieving human-like dexterous manipulation remains a major challenge for general-purpose robots. While Vision-Language-Action (VLA) models show potential in learning skills from demonstrations, their scalability is limited by scarce high-quality training data. Existing data collection methods face inherent constraints: manual teleoperation overloads human operators, while automated planning often produces unnatural motions. We propose a Shared Autonomy framework that divides control between macro and micro motions. A human operator guides the robot's arm pose through intuitive VR teleoperation, while an autonomous DexGrasp-VLA policy handles fine-grained hand control using real-time tactile and visual feedback. This division significantly reduces cognitive load and enables efficient collection of high-quality coordinated arm-hand demonstrations. Using this data, we train an end-to-end VLA policy enhanced with our novel Arm-Hand Feature Enhancement module, which captures both distinct and shared representations of macro and micro movements for more natural coordination. Our Corrective Teleoperation system enables continuous policy improvement through human-in-the-loop failure recovery. Experiments demonstrate that our framework generates high-quality data with minimal manpower and achieves a 90\% success rate across diverse objects, including unseen instances. Comprehensive evaluations validate the system's effectiveness in developing dexterous manipulation capabilities.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Cui, Yu and Zhang, Yujian and Tao, Lina and Li, Yang and Yi, Xinyu and Li, Zhibin},
	month = dec,
	year = {2025},
	note = {arXiv:2511.00139 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\KW5SQUEX\\Cui 等 - 2025 - End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy VR Teleoperation Augmented by Autono.pdf:application/pdf},
}

@misc{zhang_step_2025,
	title = {A {Step} {Toward} {World} {Models}: {A} {Survey} on {Robotic} {Manipulation}},
	shorttitle = {A {Step} {Toward} {World} {Models}},
	url = {http://arxiv.org/abs/2511.02097},
	doi = {10.48550/arXiv.2511.02097},
	abstract = {Autonomous agents are increasingly expected to operate in complex, dynamic, and uncertain environments, performing tasks such as manipulation, navigation, and decision-making. Achieving these capabilities requires agents to understand the underlying mechanisms and dynamics of the world, moving beyond reactive control or simple replication of observed states. This motivates the development of world models as internal representations that encode environmental states, capture dynamics, and support prediction, planning, and reasoning. Despite growing interest, the definition, scope, architectures, and essential capabilities of world models remain ambiguous. In this survey, we go beyond prescribing a fixed definition and limiting our scope to methods explicitly labeled as world models. Instead, we examine approaches that exhibit the core capabilities of world models through a review of methods in robotic manipulation. We analyze their roles across perception, prediction, and control, identify key challenges and solutions, and distill the core components, capabilities, and functions that a fully realized world model should possess. Building on this analysis, we aim to motivate further development toward generalizable and practical world models for robotics.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhang, Peng-Fei and Cheng, Ying and Sun, Xiaofan and Wang, Shijie and Li, Fengling and Zhu, Lei and Shen, Heng Tao},
	month = nov,
	year = {2025},
	note = {arXiv:2511.02097 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\76X5TDRQ\\Zhang 等 - 2025 - A Step Toward World Models A Survey on Robotic Manipulation.pdf:application/pdf},
}

@misc{bi_motus_2025,
	title = {Motus: {A} {Unified} {Latent} {Action} {World} {Model}},
	shorttitle = {Motus},
	url = {http://arxiv.org/abs/2512.13030},
	doi = {10.48550/arXiv.2512.13030},
	abstract = {While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15\% improvement over X-VLA and a +45\% improvement over Pi0.5) and real-world scenarios(improved by +11{\textasciitilde}48\%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Bi, Hongzhe and Tan, Hengkai and Xie, Shenghao and Wang, Zeyuan and Huang, Shuhe and Liu, Haitian and Zhao, Ruowen and Feng, Yao and Xiang, Chendong and Rong, Yinze and Zhao, Hongyan and Liu, Hanyu and Su, Zhizhong and Ma, Lei and Su, Hang and Zhu, Jun},
	month = dec,
	year = {2025},
	note = {arXiv:2512.13030 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\DURGCYKB\\Bi 等 - 2025 - Motus A Unified Latent Action World Model.pdf:application/pdf},
}

@misc{deng_graspvla_2025,
	title = {{GraspVLA}: a {Grasping} {Foundation} {Model} {Pre}-trained on {Billion}-scale {Synthetic} {Action} {Data}},
	shorttitle = {{GraspVLA}},
	url = {http://arxiv.org/abs/2505.03233},
	doi = {10.48550/arXiv.2505.03233},
	abstract = {Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post-training. However, existing models rely heavily on real-world data, which is costly and labor-intensive to collect. Synthetic data offers a cost-effective alternative, yet its potential remains largely underexplored. To bridge this gap, we explore the feasibility of training Vision-Language-Action models entirely with large-scale synthetic action data. We curate SynGrasp-1B, a billion-frame robotic grasping dataset generated in simulation with photorealistic rendering and extensive domain randomization. Building on this, we present GraspVLA, a VLA model pretrained on large-scale synthetic action data as a foundational model for grasping tasks. GraspVLA integrates autoregressive perception tasks and flow-matching-based action generation into a unified Chain-of-Thought process, enabling joint training on synthetic action data and Internet semantics data. This design helps mitigate sim-to-real gaps and facilitates the transfer of learned actions to a broader range of Internet-covered objects, achieving open-vocabulary generalization in grasping. Extensive evaluations across real-world and simulation benchmarks demonstrate GraspVLA's advanced zero-shot generalizability and few-shot adaptability to specific human preferences. We will release SynGrasp-1B dataset and pre-trained weights to benefit the community.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Deng, Shengliang and Yan, Mi and Wei, Songlin and Ma, Haixin and Yang, Yuxin and Chen, Jiayi and Zhang, Zhiqi and Yang, Taoyu and Zhang, Xuheng and Zhang, Wenhao and Cui, Heming and Zhang, Zhizheng and Wang, He},
	month = aug,
	year = {2025},
	note = {arXiv:2505.03233 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\GSRZGKE3\\Deng 等 - 2025 - GraspVLA a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data.pdf:application/pdf},
}

@misc{li_switchvla_2025,
	title = {{SwitchVLA}: {Execution}-{Aware} {Task} {Switching} for {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{SwitchVLA}},
	url = {http://arxiv.org/abs/2506.03574},
	doi = {10.48550/arXiv.2506.03574},
	abstract = {Robots deployed in dynamic environments must be able to not only follow diverse language instructions but flexibly adapt when user intent changes mid-execution. While recent Vision-Language-Action (VLA) models have advanced multi-task learning and instruction following, they typically assume static task intent, failing to respond when new instructions arrive during ongoing execution. This limitation hinders natural and robust interaction in dynamic settings, such as retail or household environments, where real-time intent changes are common. We propose SwitchVLA, a unified, execution-aware framework that enables smooth and reactive task switching without external planners or additional switch-specific data. We model task switching as a behavior modulation problem conditioned on execution state and instruction context. Expert demonstrations are segmented into temporally grounded contact phases, allowing the policy to infer task progress and adjust its behavior accordingly. A multi-behavior conditional policy is then trained to generate flexible action chunks under varying behavior modes through conditioned trajectory modeling. Experiments in both simulation and real-world robotic manipulation demonstrate that SwitchVLA enables robust instruction adherence, fluid task switching, and strong generalization-outperforming prior VLA baselines in both task success rate and interaction naturalness.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Meng and Zhao, Zhen and Che, Zhengping and Liao, Fei and Wu, Kun and Xu, Zhiyuan and Ren, Pei and Jin, Zhao and Liu, Ning and Tang, Jian},
	month = jun,
	year = {2025},
	note = {arXiv:2506.03574 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\HHN4XZAB\\Li 等 - 2025 - SwitchVLA Execution-Aware Task Switching for Vision-Language-Action Models.pdf:application/pdf},
}

@misc{lin_evo-1_2025,
	title = {Evo-1: {Lightweight} {Vision}-{Language}-{Action} {Model} with {Preserved} {Semantic} {Alignment}},
	shorttitle = {Evo-1},
	url = {http://arxiv.org/abs/2511.04555},
	doi = {10.48550/arXiv.2511.04555},
	abstract = {Vision-Language-Action (VLA) models have emerged as a powerful framework that unifies perception, language, and control, enabling robots to perform diverse tasks through multimodal understanding. However, current VLA models typically contain massive parameters and rely heavily on large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training paradigms often degrade the perceptual representations of the vision-language backbone, resulting in overfitting and poor generalization to downstream tasks. In this work, we present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot data. Evo-1 builds on a native multimodal Vision-Language model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further introduce a two-stage training paradigm that progressively aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the Meta-World and RoboTwin suite, surpassing the previous best models by 12.4\% and 6.9\%, respectively, and also attains a competitive result of 94.8\% on LIBERO. In real-world evaluations, Evo-1 attains a 78\% success rate with high inference frequency and low memory overhead, outperforming all baseline methods. We release code, data, and model weights to facilitate future research on lightweight and efficient VLA models.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Lin, Tao and Zhong, Yilei and Du, Yuxin and Zhang, Jingjing and Liu, Jiting and Chen, Yinxinyu and Gu, Encheng and Liu, Ziyan and Cai, Hongyi and Zou, Yanwen and Zou, Lixing and Zhou, Zhaoye and Li, Gen and Zhao, Bo},
	month = dec,
	year = {2025},
	note = {arXiv:2511.04555 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\7L48Z2RZ\\Lin 等 - 2025 - Evo-1 Lightweight Vision-Language-Action Model with Preserved Semantic Alignment.pdf:application/pdf},
}

@misc{ye_learning_2025,
	title = {Learning to {Feel} the {Future}: {DreamTacVLA} for {Contact}-{Rich} {Manipulation}},
	shorttitle = {Learning to {Feel} the {Future}},
	url = {http://arxiv.org/abs/2512.23864},
	doi = {10.48550/arXiv.2512.23864},
	abstract = {Vision-Language-Action (VLA) models have shown remarkable generalization by mapping web-scale knowledge to robotic control, yet they remain blind to physical contact. Consequently, they struggle with contact-rich manipulation tasks that require reasoning about force, texture, and slip. While some approaches incorporate low-dimensional tactile signals, they fail to capture the high-resolution dynamics essential for such interactions. To address this limitation, we introduce DreamTacVLA, a framework that grounds VLA models in contact physics by learning to feel the future. Our model adopts a hierarchical perception scheme in which high-resolution tactile images serve as micro-vision inputs coupled with wrist-camera local vision and third-person macro vision. To reconcile these multi-scale sensory streams, we first train a unified policy with a Hierarchical Spatial Alignment (HSA) loss that aligns tactile tokens with their spatial counterparts in the wrist and third-person views. To further deepen the model's understanding of fine-grained contact dynamics, we finetune the system with a tactile world model that predicts future tactile signals. To mitigate tactile data scarcity and the wear-prone nature of tactile sensors, we construct a hybrid large-scale dataset sourced from both high-fidelity digital twin and real-world experiments. By anticipating upcoming tactile states, DreamTacVLA acquires a rich model of contact physics and conditions its actions on both real observations and imagined consequences. Across contact-rich manipulation tasks, it outperforms state-of-the-art VLA baselines, achieving up to 95\% success, highlighting the importance of understanding physical contact for robust, touch-aware robotic agents.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Ye, Guo and Zhang, Zexi and Zhao, Xu and Wu, Shang and Lu, Haoran and Lu, Shihan and Liu, Han},
	month = dec,
	year = {2025},
	note = {arXiv:2512.23864 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\JFD5Q5A5\\Ye 等 - 2025 - Learning to Feel the Future DreamTacVLA for Contact-Rich Manipulation.pdf:application/pdf},
}

@misc{liu_ttf-vla_2025,
	title = {{TTF}-{VLA}: {Temporal} {Token} {Fusion} via {Pixel}-{Attention} {Integration} for {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{TTF}-{VLA}},
	url = {http://arxiv.org/abs/2508.19257},
	doi = {10.48550/arXiv.2508.19257},
	abstract = {Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4{\textbackslash}\% vs 68.4{\textbackslash}\% baseline), cross-environment validation on SimplerEnv (4.8{\textbackslash}\% relative improvement), and 8.7{\textbackslash}\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Liu, Chenghao and Zhang, Jiachen and Li, Chengxuan and Zhou, Zhimu and Wu, Shixin and Huang, Songfang and Duan, Huiling},
	month = nov,
	year = {2025},
	note = {arXiv:2508.19257 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\38AIJ9D9\\Liu 等 - 2025 - TTF-VLA Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models.pdf:application/pdf},
}

@misc{koo_retovla_2025,
	title = {{RetoVLA}: {Reusing} {Register} {Tokens} for {Spatial} {Reasoning} in {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{RetoVLA}},
	url = {http://arxiv.org/abs/2509.21243},
	doi = {10.48550/arXiv.2509.21243},
	abstract = {Recent Vision-Language-Action (VLA) models demonstrate remarkable generalization in robotics but are restricted by their substantial size and computational cost, limiting real-world deployment. However, conventional lightweighting methods often sacrifice critical capabilities, particularly spatial reasoning. This creates a trade-off between efficiency and performance. To address this challenge, our work reuses Register Tokens, which were introduced for artifact removal in Vision Transformers but subsequently discarded. We suppose that these tokens contain essential spatial information and propose RetoVLA, a novel architecture that reuses them directly by injecting them into the Action Expert. RetoVLA maintains a lightweight structure while leveraging this repurposed spatial context to enhance reasoning. We demonstrate RetoVLA's effectiveness through a series of comprehensive experiments. On our custom-built 7-DOF robot arm, the model achieves a 17.1\%p absolute improvement in success rates for complex manipulation tasks. Our results confirm that reusing Register Tokens directly enhances spatial reasoning, demonstrating that what was previously discarded as an artifact is in fact a valuable, unexplored resource for robotic intelligence. A video demonstration is available at: https://youtu.be/2CseBR-snZg},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Koo, Jiyeon and Cho, Taewan and Kang, Hyunjoon and Pyo, Eunseom and Oh, Tae Gyun and Kim, Taeryang and Choi, Andrew Jaeyong},
	month = sep,
	year = {2025},
	note = {arXiv:2509.21243 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\6AGVGVNC\\Koo 等 - 2025 - RetoVLA Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models.pdf:application/pdf},
}

@misc{xu_vla-cache_2025,
	title = {{VLA}-{Cache}: {Efficient} {Vision}-{Language}-{Action} {Manipulation} via {Adaptive} {Token} {Caching}},
	shorttitle = {{VLA}-{Cache}},
	url = {http://arxiv.org/abs/2502.02175},
	doi = {10.48550/arXiv.2502.02175},
	abstract = {Vision-Language-Action (VLA) models have demonstrated strong multi-modal reasoning capabilities, enabling direct action generation from visual perception and language instructions in an end-to-end manner. However, their substantial computational cost poses a challenge for real-time robotic control, where rapid decision-making is essential. This paper introduces VLA-Cache, a training-free inference acceleration method that reduces computational overhead by adaptively caching and reusing static visual tokens across frames. Exploiting the temporal continuity in robotic manipulation, VLA-Cache identifies minimally changed tokens between adjacent frames and reuses their cached key-value representations, thereby circumventing redundant computations. Additionally, to maintain action precision, VLA-Cache selectively re-computes task-relevant tokens that are environmentally sensitive, ensuring the fidelity of critical visual information. To further optimize efficiency, we introduce a layer adaptive token reusing strategy that dynamically adjusts the reuse ratio based on attention concentration across decoder layers, prioritizing critical tokens for recomputation. Extensive experiments on two simulation platforms (LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache achieves up to 1.7x speedup in CUDA latency and a 15\% increase in control frequency, with negligible loss on task success rate. The code and videos can be found at our project page: https://vla-cache.github.io.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Xu, Siyu and Wang, Yunke and Xia, Chenghao and Zhu, Dihao and Huang, Tao and Xu, Chang},
	month = oct,
	year = {2025},
	note = {arXiv:2502.02175 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\XHXMK3D5\\Xu 等 - 2025 - VLA-Cache Efficient Vision-Language-Action Manipulation via Adaptive Token Caching.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\85KK2PSB\\2502.html:text/html},
}

@misc{zhou_chatvla-2_2025,
	title = {{ChatVLA}-2: {Vision}-{Language}-{Action} {Model} with {Open}-{World} {Embodied} {Reasoning} from {Pretrained} {Knowledge}},
	shorttitle = {{ChatVLA}-2},
	url = {http://arxiv.org/abs/2505.21906},
	doi = {10.48550/arXiv.2505.21906},
	abstract = {Vision-language-action (VLA) models have emerged as the next generation of models in robotics. However, despite leveraging powerful pre-trained Vision-Language Models (VLMs), existing end-to-end VLA systems often lose key capabilities during fine-tuning as the model adapts to specific robotic tasks. We argue that a generalizable VLA model should retain and expand upon the VLM's core competencies: 1) Open-world embodied reasoning - the VLA should inherit the knowledge from VLM, i.e., recognize anything that the VLM can recognize, be capable of solving math problems, and possess visual-spatial intelligence, 2) Reasoning following - effectively translating the open-world reasoning into actionable steps for the robot. In this work, we introduce ChatVLA-2, a novel mixture-of-expert VLA model coupled with a specialized two-stage training pipeline designed to preserve the VLM's original strengths while enabling actionable reasoning. To validate our approach, we design a math-matching task wherein a robot interprets math problems written on a whiteboard and picks corresponding number cards from a table to solve equations. Remarkably, our method exhibits exceptional mathematical reasoning and OCR capabilities, despite these abilities not being explicitly trained within the VLA. Furthermore, we demonstrate that the VLA possesses strong spatial reasoning skills, enabling it to interpret novel directional instructions involving previously unseen objects. Overall, our method showcases reasoning and comprehension abilities that significantly surpass state-of-the-art imitation learning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a substantial advancement toward developing truly generalizable robotic foundation models endowed with robust reasoning capacities.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Zhou, Zhongyi and Zhu, Yichen and Wen, Junjie and Shen, Chaomin and Xu, Yi},
	month = may,
	year = {2025},
	note = {arXiv:2505.21906 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\JPBLNGWL\\Zhou 等 - 2025 - ChatVLA-2 Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\5XQB56UX\\2505.html:text/html},
}

@article{feng_embodied_2025,
	title = {Embodied {AI}: {From} {LLMs} to {World} {Models} [{Feature}]},
	volume = {25},
	shorttitle = {Embodied {AI}},
	url = {https://ieeexplore.ieee.org/abstract/document/11317901/},
	number = {4},
	urldate = {2026-02-06},
	journal = {IEEE Circuits and Systems Magazine},
	publisher = {IEEE},
	author = {Feng, Tongtong and Wang, Xin and Jiang, Yu-Gang and Zhu, Wenwu},
	year = {2025},
	pages = {14--37},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\ADBMIFKV\\Feng 等 - 2025 - Embodied AI From LLMs to World Models [Feature].pdf:application/pdf},
}

@misc{jiang_irl-vla_2025,
	title = {{IRL}-{VLA}: {Training} an {Vision}-{Language}-{Action} {Policy} via {Reward} {World} {Model}},
	shorttitle = {{IRL}-{VLA}},
	url = {http://arxiv.org/abs/2508.06571},
	doi = {10.48550/arXiv.2508.06571},
	abstract = {Vision-Language-Action (VLA) models have demonstrated potential in autonomous driving. However, two critical challenges hinder their development: (1) Existing VLA architectures are typically based on imitation learning in open-loop setup which tends to capture the recorded behaviors in the dataset, leading to suboptimal and constrained performance, (2) Close-loop training relies heavily on high-fidelity sensor simulation, where domain gaps and computational inefficiencies pose significant barriers. In this paper, we introduce IRL-VLA, a novel close-loop Reinforcement Learning via {\textbackslash}textbf\{I\}nverse {\textbackslash}textbf\{R\}einforcement {\textbackslash}textbf\{L\}earning reward world model with a self-built VLA approach. Our framework proceeds in a three-stage paradigm: In the first stage, we propose a VLA architecture and pretrain the VLA policy via imitation learning. In the second stage, we construct a lightweight reward world model via inverse reinforcement learning to enable efficient close-loop reward computation. To further enhance planning performance, finally, we design specialized reward world model guidence reinforcement learning via PPO(Proximal Policy Optimization) to effectively balance the safety incidents, comfortable driving, and traffic efficiency. Our approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that our framework will accelerate VLA research in close-loop autonomous driving.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Jiang, Anqing and Gao, Yu and Wang, Yiru and Sun, Zhigang and Wang, Shuo and Heng, Yuwen and Sun, Hao and Tang, Shichen and Zhu, Lijuan and Chai, Jinhao and Wang, Jijun and Gu, Zichong and Jiang, Hao and Sun, Li},
	month = aug,
	year = {2025},
	note = {arXiv:2508.06571 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\2GDMLWBN\\Jiang 等 - 2025 - IRL-VLA Training an Vision-Language-Action Policy via Reward World Model.pdf:application/pdf},
}

@misc{liao_genie_2025,
	title = {Genie {Envisioner}: {A} {Unified} {World} {Foundation} {Platform} for {Robotic} {Manipulation}},
	shorttitle = {Genie {Envisioner}},
	url = {http://arxiv.org/abs/2508.05635},
	doi = {10.48550/arXiv.2508.05635},
	abstract = {We introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Liao, Yue and Zhou, Pengfei and Huang, Siyuan and Yang, Donglin and Chen, Shengcong and Jiang, Yuxin and Hu, Yue and Cai, Jingbin and Liu, Si and Luo, Jianlan and Chen, Liliang and Yan, Shuicheng and Yao, Maoqing and Ren, Guanghui},
	month = nov,
	year = {2025},
	note = {arXiv:2508.05635 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\NY38NDSK\\Liao 等 - 2025 - Genie Envisioner A Unified World Foundation Platform for Robotic Manipulation.pdf:application/pdf},
}

@misc{lin_out_2024,
	title = {Out of {Many}, {One}: {Designing} and {Scaffolding} {Proteins} at the {Scale} of the {Structural} {Universe} with {Genie} 2},
	shorttitle = {Out of {Many}, {One}},
	url = {http://arxiv.org/abs/2405.15489},
	doi = {10.48550/arXiv.2405.15489},
	abstract = {Protein diffusion models have emerged as a promising approach for protein design. One such pioneering model is Genie, a method that asymmetrically represents protein structures during the forward and backward processes, using simple Gaussian noising for the former and expressive SE(3)-equivariant attention for the latter. In this work we introduce Genie 2, extending Genie to capture a larger and more diverse protein structure space through architectural innovations and massive data augmentation. Genie 2 adds motif scaffolding capabilities via a novel multi-motif framework that designs co-occurring motifs with unspecified inter-motif positions and orientations. This makes possible complex protein designs that engage multiple interaction partners and perform multiple functions. On both unconditional and conditional generation, Genie 2 achieves state-of-the-art performance, outperforming all known methods on key design metrics including designability, diversity, and novelty. Genie 2 also solves more motif scaffolding problems than other methods and does so with more unique and varied solutions. Taken together, these advances set a new standard for structure-based protein design. Genie 2 inference and training code, as well as model weights, are freely available at: https://github.com/aqlaboratory/genie2.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Lin, Yeqing and Lee, Minji and Zhang, Zhao and AlQuraishi, Mohammed},
	month = may,
	year = {2024},
	note = {arXiv:2405.15489 [q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\4MC9G72I\\Lin 等 - 2024 - Out of Many, One Designing and Scaffolding Proteins at the Scale of the Structural Universe with Ge.pdf:application/pdf},
}

@inproceedings{bruce_genie_2024,
	title = {Genie: {Generative} interactive environments},
	shorttitle = {Genie},
	url = {https://openreview.net/forum?id=bJbSbJskOS},
	urldate = {2026-02-06},
	booktitle = {Forty-first {International} {Conference} on {Machine} {Learning}},
	author = {Bruce, Jake and Dennis, Michael D. and Edwards, Ashley and Parker-Holder, Jack and Shi, Yuge and Hughes, Edward and Lai, Matthew and Mavalankar, Aditi and Steigerwald, Richie and Apps, Chris},
	year = {2024},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\IRCX9X54\\Bruce 等 - 2024 - Genie Generative interactive environments.pdf:application/pdf},
}

@article{wang_genie_2025,
	title = {Genie: {A} generalizable navigation system for in-the-wild environments},
	shorttitle = {Genie},
	url = {https://ieeexplore.ieee.org/abstract/document/11206420/},
	urldate = {2026-02-06},
	journal = {IEEE Robotics and Automation Letters},
	publisher = {IEEE},
	author = {Wang, Jiaming and Liu, Diwen and Chen, Jizhuo and Da, Jiaxuan and Qian, Nuowen and Tram, Minh Man and Soh, Harold},
	year = {2025},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\57Z2XB3N\\Wang 等 - 2025 - Genie A generalizable navigation system for in-the-wild environments.pdf:application/pdf},
}

@misc{kazemi_learning_2024,
	title = {Learning {Generative} {Interactive} {Environments} {By} {Trained} {Agent} {Exploration}},
	url = {http://arxiv.org/abs/2409.06445},
	doi = {10.48550/arXiv.2409.06445},
	abstract = {World models are increasingly pivotal in interpreting and simulating the rules and actions of complex environments. Genie, a recent model, excels at learning from visually diverse environments but relies on costly human-collected data. We observe that their alternative method of using random agents is too limited to explore the environment. We propose to improve the model by employing reinforcement learning based agents for data generation. This approach produces diverse datasets that enhance the model's ability to adapt and perform well across various scenarios and realistic actions within the environment. In this paper, we first release the model GenieRedux - an implementation based on Genie. Additionally, we introduce GenieRedux-G, a variant that uses the agent's readily available actions to factor out action prediction uncertainty during validation. Our evaluation, including a replication of the Coinrun case study, shows that GenieRedux-G achieves superior visual fidelity and controllability using the trained agent exploration. The proposed approach is reproducable, scalable and adaptable to new types of environments. Our codebase is available at https://github.com/insait-institute/GenieRedux .},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Kazemi, Naser and Savov, Nedko and Paudel, Danda and Gool, Luc Van},
	month = oct,
	year = {2024},
	note = {arXiv:2409.06445 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\X2A8NXYG\\Kazemi 等 - 2024 - Learning Generative Interactive Environments By Trained Agent Exploration.pdf:application/pdf},
}

@misc{yehudai_genie_2024,
	title = {Genie: {Achieving} {Human} {Parity} in {Content}-{Grounded} {Datasets} {Generation}},
	shorttitle = {Genie},
	url = {http://arxiv.org/abs/2401.14367},
	doi = {10.48550/arXiv.2401.14367},
	abstract = {The lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks. To address this gap, we propose Genie, a novel method for automatically generating high-quality content-grounded data. It consists of three stages: (a) Content Preparation, (b) Generation: creating task-specific examples from the content (e.g., question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure the quality and faithfulness of the generated data. We showcase this methodology by generating three large-scale synthetic data, making wishes, for Long-Form Question-Answering (LFQA), summarization, and information extraction. In a human evaluation, our generated data was found to be natural and of high quality. Furthermore, we compare models trained on our data with models trained on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for Summarization. We show that our models are on par with or outperforming models trained on human-generated data and consistently outperforming them in faithfulness. Finally, we applied our method to create LFQA data within the medical domain and compared a model trained on it with models trained on other domains.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Yehudai, Asaf and Carmeli, Boaz and Mass, Yosi and Arviv, Ofir and Mills, Nathaniel and Toledo, Assaf and Shnarch, Eyal and Choshen, Leshem},
	month = jan,
	year = {2024},
	note = {arXiv:2401.14367 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\NEZ9BNQY\\Yehudai 等 - 2024 - Genie Achieving Human Parity in Content-Grounded Datasets Generation.pdf:application/pdf},
}

@inproceedings{savov_exploration-driven_2025,
	title = {Exploration-{Driven} {Generative} {Interactive} {Environments}},
	url = {https://openaccess.thecvf.com/content/CVPR2025/html/Savov_Exploration-Driven_Generative_Interactive_Environments_CVPR_2025_paper.html},
	urldate = {2026-02-06},
	booktitle = {Proceedings of the {Computer} {Vision} and {Pattern} {Recognition} {Conference}},
	author = {Savov, Nedko and Kazemi, Naser and Mahdi, Mohammad and Paudel, Danda Pani and Wang, Xi and Van Gool, Luc},
	year = {2025},
	pages = {27597--27607},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\9ZS5QDIR\\Savov 等 - 2025 - Exploration-Driven Generative Interactive Environments.pdf:application/pdf},
}

@misc{yin_genie_2026,
	title = {Genie {Sim} 3.0 : {A} {High}-{Fidelity} {Comprehensive} {Simulation} {Platform} for {Humanoid} {Robot}},
	shorttitle = {Genie {Sim} 3.0},
	url = {http://arxiv.org/abs/2601.02078},
	doi = {10.48550/arXiv.2601.02078},
	abstract = {The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie\_sim.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Yin, Chenghao and Huang, Da and Yang, Di and Wang, Jichao and Zhao, Nanshu and Xu, Chen and Sun, Wenjun and Hou, Linjie and Li, Zhijun and Wu, Junhui and Liu, Zhaobo and Xiao, Zhen and Zhang, Sheng and Bao, Lei and Feng, Rui and Pang, Zhenquan and Li, Jiayu and Wang, Qian and Yao, Maoqing},
	month = jan,
	year = {2026},
	note = {arXiv:2601.02078 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\AMDWNBXJ\\Yin 等 - 2026 - Genie Sim 3.0  A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot.pdf:application/pdf},
}

@misc{upadhyay_worldbench_2026,
	title = {{WorldBench}: {Disambiguating} {Physics} for {Diagnostic} {Evaluation} of {World} {Models}},
	shorttitle = {{WorldBench}},
	url = {http://arxiv.org/abs/2601.21282},
	doi = {10.48550/arXiv.2601.21282},
	abstract = {Recent advances in generative foundational models, often termed "world models," have propelled interest in applying them to critical tasks like robotic planning and autonomous system training. For reliable deployment, these models must exhibit high physical fidelity, accurately simulating real-world dynamics. Existing physics-based video benchmarks, however, suffer from entanglement, where a single test simultaneously evaluates multiple physical laws and concepts, fundamentally limiting their diagnostic capability. We introduce WorldBench, a novel video-based benchmark specifically designed for concept-specific, disentangled evaluation, allowing us to rigorously isolate and assess understanding of a single physical concept or law at a time. To make WorldBench comprehensive, we design benchmarks at two different levels: 1) an evaluation of intuitive physical understanding with concepts such as object permanence or scale/perspective, and 2) an evaluation of low-level physical constants and material properties such as friction coefficients or fluid viscosity. When SOTA video-based world models are evaluated on WorldBench, we find specific patterns of failure in particular physics concepts, with all tested models lacking the physical consistency required to generate reliable real-world interactions. Through its concept-specific evaluation, WorldBench offers a more nuanced and scalable framework for rigorously evaluating the physical reasoning capabilities of video generation and world models, paving the way for more robust and generalizable world-model-driven learning.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Upadhyay, Rishi and Zhang, Howard and Solomon, Jim and Agrawal, Ayush and Boreddy, Pranay and Narayana, Shruti Satya and Ba, Yunhao and Wong, Alex and Melo, Celso M. de and Kadambi, Achuta},
	month = jan,
	year = {2026},
	note = {arXiv:2601.21282 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\UWT35YPC\\Upadhyay 等 - 2026 - WorldBench Disambiguating Physics for Diagnostic Evaluation of World Models.pdf:application/pdf},
}

@misc{ren_aligning_2026,
	title = {Aligning {Agentic} {World} {Models} via {Knowledgeable} {Experience} {Learning}},
	url = {http://arxiv.org/abs/2601.13247},
	doi = {10.48550/arXiv.2601.13247},
	abstract = {Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Ren, Baochang and Yao, Yunzhi and Sun, Rui and Qiao, Shuofei and Zhang, Ningyu and Chen, Huajun},
	month = jan,
	year = {2026},
	note = {arXiv:2601.13247 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Multimedia},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\PH25CUZL\\Ren 等 - 2026 - Aligning Agentic World Models via Knowledgeable Experience Learning.pdf:application/pdf},
}

@misc{zhou_digital_2026,
	title = {Digital {Twin} {AI}: {Opportunities} and {Challenges} from {Large} {Language} {Models} to {World} {Models}},
	shorttitle = {Digital {Twin} {AI}},
	url = {http://arxiv.org/abs/2601.01321},
	doi = {10.48550/arXiv.2601.01321},
	abstract = {Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Zhou, Rong and Chen, Dongping and Jia, Zihan and Su, Yao and Liu, Yixin and Lu, Yiwen and Shi, Dongwei and Huang, Yue and Xu, Tianyang and Pan, Yi and Li, Xinliang and Abate, Yohannes and Chen, Qingyu and Tu, Zhengzhong and Yang, Yu and Zhang, Yu and Wen, Qingsong and Mai, Gengchen and Fu, Sunyang and Li, Jiachen and Wang, Xuyu and Wang, Ziran and Huang, Jing and Liu, Tianming and Chen, Yong and Sun, Lichao and He, Lifang},
	month = jan,
	year = {2026},
	note = {arXiv:2601.01321 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\UU462F9I\\Zhou 等 - 2026 - Digital Twin AI Opportunities and Challenges from Large Language Models to World Models.pdf:application/pdf},
}

@misc{fan_wow_2026,
	title = {Wow, wo, val! {A} {Comprehensive} {Embodied} {World} {Model} {Evaluation} {Turing} {Test}},
	url = {http://arxiv.org/abs/2601.04137},
	doi = {10.48550/arXiv.2601.04137},
	abstract = {As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to serve as a universal prior for real-world embodied agents. To provide a standardized framework for answering these questions, we introduce the Embodied Turing Test benchmark: WoW-World-Eval (Wow,wo,val). Building upon 609 robot manipulation data, Wow-wo-val examines five core abilities, including perception, planning, prediction, generalization, and execution. We propose a comprehensive evaluation protocol with 22 metrics to assess the models' generation ability, which achieves a high Pearson Correlation between the overall score and human preference ({\textgreater}0.93) and establishes a reliable foundation for the Human Turing Test. On Wow-wo-val, models achieve only 17.27 on long-horizon planning and at best 68.02 on physical consistency, indicating limited spatiotemporal consistency and physical reasoning. For the Inverse Dynamic Model Turing Test, we first use an IDM to evaluate the video foundation models' execution accuracy in the real world. However, most models collapse to \${\textbackslash}approx\$ 0\% success, while WoW maintains a 40.74\% success rate. These findings point to a noticeable gap between the generated videos and the real world, highlighting the urgency and necessity of benchmarking World Model in Embodied AI.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Fan, Chun-Kai and Chi, Xiaowei and Ju, Xiaozhu and Li, Hao and Bao, Yong and Wang, Yu-Kai and Chen, Lizhang and Jiang, Zhiyuan and Ge, Kuangzhi and Li, Ying and Mi, Weishi and Wuwu, Qingpo and Jia, Peidong and Luo, Yulin and Zhang, Kevin and Qin, Zhiyuan and Dai, Yong and Han, Sirui and Guo, Yike and Zhang, Shanghang and Tang, Jian},
	month = jan,
	year = {2026},
	note = {arXiv:2601.04137 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\IBL4M5TI\\Fan 等 - 2026 - Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test.pdf:application/pdf},
}

@misc{peng_reworld_2026,
	title = {{ReWorld}: {Multi}-{Dimensional} {Reward} {Modeling} for {Embodied} {World} {Models}},
	shorttitle = {{ReWorld}},
	url = {http://arxiv.org/abs/2601.12428},
	doi = {10.48550/arXiv.2601.12428},
	abstract = {Recently, video-based world models that learn to simulate the dynamics have gained increasing attention in robot learning. However, current approaches primarily emphasize visual generative quality while overlooking physical fidelity, dynamic consistency, and task logic, especially for contact-rich manipulation tasks, which limits their applicability to downstream tasks. To this end, we introduce ReWorld, a framework aimed to employ reinforcement learning to align the video-based embodied world models with physical realism, task completion capability, embodiment plausibility and visual quality. Specifically, we first construct a large-scale ({\textasciitilde}235K) video preference dataset and employ it to train a hierarchical reward model designed to capture multi-dimensional reward consistent with human preferences. We further propose a practical alignment algorithm that post-trains flow-based world models using this reward through a computationally efficient PPO-style algorithm. Comprehensive experiments and theoretical analysis demonstrate that ReWorld significantly improves the physical fidelity, logical coherence, embodiment and visual quality of generated rollouts, outperforming previous methods.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Peng, Baorui and Zhang, Wenyao and Xu, Liang and Qi, Zekun and Zhang, Jiazhao and Liu, Hongsi and Zeng, Wenjun and Jin, Xin},
	month = jan,
	year = {2026},
	note = {arXiv:2601.12428 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\MZE9JQHG\\Peng 等 - 2026 - ReWorld Multi-Dimensional Reward Modeling for Embodied World Models.pdf:application/pdf},
}

@misc{cai_internvla-a1_2026,
	title = {{InternVLA}-{A1}: {Unifying} {Understanding}, {Generation} and {Action} for {Robotic} {Manipulation}},
	shorttitle = {{InternVLA}-{A1}},
	url = {http://arxiv.org/abs/2601.02456},
	doi = {10.48550/arXiv.2601.02456},
	abstract = {Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5{\textbackslash}\% improvement in daily tasks and a 40{\textbackslash}\%-73.3{\textbackslash}\% boost in dynamic settings, such as conveyor belt sorting.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Cai, Junhao and Cai, Zetao and Cao, Jiafei and Chen, Yilun and He, Zeyu and Jiang, Lei and Li, Hang and Li, Hengjie and Li, Yang and Liu, Yufei and Lu, Yanan and Lv, Qi and Ma, Haoxiang and Pang, Jiangmiao and Qiao, Yu and Qiu, Zherui and Shen, Yanqing and Shi, Xu and Tian, Yang and Wang, Bolun and Wang, Hanqing and Wang, Jiaheng and Wang, Tai and Wei, Xueyuan and Wu, Chao and Xie, Yiman and Xing, Boyang and Yang, Yuqiang and Yang, Yuyin and Yu, Qiaojun and Yuan, Feng and Zeng, Jia and Zhang, Jingjing and Zhang, Shenghan and Zhang, Shi and Zhaxi, Zhuoma and Zhou, Bowen and Zhou, Yuanzhen and Zhou, Yunsong and Zhu, Hongrui and Zhu, Yangkun and Zhu, Yuchen},
	month = jan,
	year = {2026},
	note = {arXiv:2601.02456 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\YYYEUQC6\\Cai 等 - 2026 - InternVLA-A1 Unifying Understanding, Generation and Action for Robotic Manipulation.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\BZCK7R45\\2601.html:text/html},
}

@misc{wu_visual_2026,
	title = {Visual {Generation} {Unlocks} {Human}-{Like} {Reasoning} through {Multimodal} {World} {Models}},
	url = {http://arxiv.org/abs/2601.19834},
	doi = {10.48550/arXiv.2601.19834},
	abstract = {Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Wu, Jialong and Zhang, Xiaoying and Yuan, Hongyi and Zhang, Xiangcheng and Huang, Tianhao and He, Changjing and Deng, Chaoyi and Zhang, Renrui and Wu, Youbin and Long, Mingsheng},
	month = jan,
	year = {2026},
	note = {arXiv:2601.19834 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\SIKZNF4B\\Wu 等 - 2026 - Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models.pdf:application/pdf},
}

@misc{shen_efficient_2026,
	title = {An {Efficient} and {Multi}-{Modal} {Navigation} {System} with {One}-{Step} {World} {Model}},
	url = {http://arxiv.org/abs/2601.12277},
	doi = {10.48550/arXiv.2601.12277},
	abstract = {Navigation is a fundamental capability for mobile robots. While the current trend is to use learning-based approaches to replace traditional geometry-based methods, existing end-to-end learning-based policies often struggle with 3D spatial reasoning and lack a comprehensive understanding of physical world dynamics. Integrating world models-which predict future observations conditioned on given actions-with iterative optimization planning offers a promising solution due to their capacity for imagination and flexibility. However, current navigation world models, typically built on pure transformer architectures, often rely on multi-step diffusion processes and autoregressive frame-by-frame generation. These mechanisms result in prohibitive computational latency, rendering real-time deployment impossible. To address this bottleneck, we propose a lightweight navigation world model that adopts a one-step generation paradigm and a 3D U-Net backbone equipped with efficient spatial-temporal attention. This design drastically reduces inference latency, enabling high-frequency control while achieving superior predictive performance. We also integrate this model into an optimization-based planning framework utilizing anchor-based initialization to handle multi-modal goal navigation tasks. Extensive closed-loop experiments in both simulation and real-world environments demonstrate our system's superior efficiency and robustness compared to state-of-the-art baselines.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Shen, Wangtian and Meng, Ziyang and Ma, Jinming and Zhou, Mingliang and Xiang, Diyun},
	month = jan,
	year = {2026},
	note = {arXiv:2601.12277 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\PXRVIBFC\\Shen 等 - 2026 - An Efficient and Multi-Modal Navigation System with One-Step World Model.pdf:application/pdf},
}

@misc{mei_video_2026,
	title = {Video {Generation} {Models} in {Robotics} -- {Applications}, {Research} {Challenges}, {Future} {Directions}},
	url = {http://arxiv.org/abs/2601.07823},
	doi = {10.48550/arXiv.2601.07823},
	abstract = {Video generation models have emerged as high-fidelity models of the physical world, capable of synthesizing high-quality videos capturing fine-grained interactions between agents and their environments conditioned on multi-modal user inputs. Their impressive capabilities address many of the long-standing challenges faced by physics-based simulators, driving broad adoption in many problem domains, e.g., robotics. For example, video models enable photorealistic, physically consistent deformable-body simulation without making prohibitive simplifying assumptions, which is a major bottleneck in physics-based simulation. Moreover, video models can serve as foundation world models that capture the dynamics of the world in a fine-grained and expressive way. They thus overcome the limited expressiveness of language-only abstractions in describing intricate physical interactions. In this survey, we provide a review of video models and their applications as embodied world models in robotics, encompassing cost-effective data generation and action prediction in imitation learning, dynamics and rewards modeling in reinforcement learning, visual planning, and policy evaluation. Further, we highlight important challenges hindering the trustworthy integration of video models in robotics, which include poor instruction following, hallucinations such as violations of physics, and unsafe content generation, in addition to fundamental limitations such as significant data curation, training, and inference costs. We present potential future directions to address these open research challenges to motivate research and ultimately facilitate broader applications, especially in safety-critical settings.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Mei, Zhiting and Yin, Tenny and Shorinwa, Ola and Badithela, Apurva and Zheng, Zhonghe and Bruno, Joseph and Bland, Madison and Zha, Lihan and Hancock, Asher and Fisac, Jaime Fernández and Dames, Philip and Majumdar, Anirudha},
	month = jan,
	year = {2026},
	note = {arXiv:2601.07823 [eess]},
	keywords = {Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\Z254FNPD\\Mei 等 - 2026 - Video Generation Models in Robotics -- Applications, Research Challenges, Future Directions.pdf:application/pdf},
}

@misc{wang_mechanistic_2026,
	title = {A {Mechanistic} {View} on {Video} {Generation} as {World} {Models}: {State} and {Dynamics}},
	shorttitle = {A {Mechanistic} {View} on {Video} {Generation} as {World} {Models}},
	url = {http://arxiv.org/abs/2601.17067},
	doi = {10.48550/arXiv.2601.17067},
	abstract = {Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary "stateless" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Wang, Luozhou and Chen, Zhifei and Du, Yihua and Yan, Dongyu and Ge, Wenhang and Shen, Guibao and Xu, Xinli and Wu, Leyi and Chen, Man and Xu, Tianshuo and Ren, Peiran and Tao, Xin and Wan, Pengfei and Chen, Ying-Cong},
	month = jan,
	year = {2026},
	note = {arXiv:2601.17067 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\RKYHGQGZ\\Wang 等 - 2026 - A Mechanistic View on Video Generation as World Models State and Dynamics.pdf:application/pdf},
}

@misc{lillemark_flow_2026,
	title = {Flow {Equivariant} {World} {Models}: {Memory} for {Partially} {Observed} {Dynamic} {Environments}},
	shorttitle = {Flow {Equivariant} {World} {Models}},
	url = {http://arxiv.org/abs/2601.01075},
	doi = {10.48550/arXiv.2601.01075},
	abstract = {Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Lillemark, Hansen Jin and Huang, Benhao and Zhan, Fangneng and Du, Yilun and Keller, Thomas Anderson},
	month = jan,
	year = {2026},
	note = {arXiv:2601.01075 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\BRU4LNLM\\Lillemark 等 - 2026 - Flow Equivariant World Models Memory for Partially Observed Dynamic Environments.pdf:application/pdf},
}

@misc{magne_nitrogen_2026,
	title = {{NitroGen}: {An} {Open} {Foundation} {Model} for {Generalist} {Gaming} {Agents}},
	shorttitle = {{NitroGen}},
	url = {http://arxiv.org/abs/2601.02427},
	doi = {10.48550/arXiv.2601.02427},
	abstract = {We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52\% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Magne, Loïc and Awadalla, Anas and Wang, Guanzhi and Xu, Yinzhen and Belofsky, Joshua and Hu, Fengyuan and Kim, Joohwan and Schmidt, Ludwig and Gkioxari, Georgia and Kautz, Jan and Yue, Yisong and Choi, Yejin and Zhu, Yuke and Fan, Linxi "Jim"},
	month = jan,
	year = {2026},
	note = {arXiv:2601.02427 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\FBB95CN7\\Magne 等 - 2026 - NitroGen An Open Foundation Model for Generalist Gaming Agents.pdf:application/pdf},
}

@inproceedings{shah_learning_2026,
	title = {Learning {Action}-{Conditioned} {World} {Models} for {Cataract} {Surgery} from {Unlabeled} {Videos}},
	url = {https://openreview.net/forum?id=aYQYOVm2AB},
	urldate = {2026-02-06},
	booktitle = {Medical {Imaging} with {Deep} {Learning}},
	author = {Shah, Nisarg A. and Xia, Mingze and Sikder, Shameema and Vedula, S. Swaroop and Patel, Vishal M.},
	year = {2026},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\F9EAVYBI\\Shah 等 - 2026 - Learning Action-Conditioned World Models for Cataract Surgery from Unlabeled Videos.pdf:application/pdf},
}

@article{guo_flowdreamer_2026,
	title = {Flowdreamer: {A} rgb-d world model with flow-based motion representations for robot manipulation},
	volume = {11},
	shorttitle = {Flowdreamer},
	url = {https://ieeexplore.ieee.org/abstract/document/11345941/},
	number = {3},
	urldate = {2026-02-06},
	journal = {IEEE Robotics and Automation Letters},
	publisher = {IEEE},
	author = {Guo, Jun and Ma, Xiaojian and Wang, Yikai and Yang, Min and Liu, Huaping and Li, Qing},
	year = {2026},
	pages = {2466--2473},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\XPIWXFIH\\Guo 等 - 2026 - Flowdreamer A rgb-d world model with flow-based motion representations for robot manipulation.pdf:application/pdf},
}

@misc{khazatsky_droid_2025,
	title = {{DROID}: {A} {Large}-{Scale} {In}-{The}-{Wild} {Robot} {Manipulation} {Dataset}},
	shorttitle = {{DROID}},
	url = {http://arxiv.org/abs/2403.12945},
	doi = {10.48550/arXiv.2403.12945},
	abstract = {The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity. In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability. We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Khazatsky, Alexander and Pertsch, Karl and Nair, Suraj and Balakrishna, Ashwin and Dasari, Sudeep and Karamcheti, Siddharth and Nasiriany, Soroush and Srirama, Mohan Kumar and Chen, Lawrence Yunliang and Ellis, Kirsty and Fagan, Peter David and Hejna, Joey and Itkina, Masha and Lepert, Marion and Ma, Yecheng Jason and Miller, Patrick Tree and Wu, Jimmy and Belkhale, Suneel and Dass, Shivin and Ha, Huy and Jain, Arhan and Lee, Abraham and Lee, Youngwoon and Memmel, Marius and Park, Sungjae and Radosavovic, Ilija and Wang, Kaiyuan and Zhan, Albert and Black, Kevin and Chi, Cheng and Hatch, Kyle Beltran and Lin, Shan and Lu, Jingpei and Mercat, Jean and Rehman, Abdul and Sanketi, Pannag R. and Sharma, Archit and Simpson, Cody and Vuong, Quan and Walke, Homer Rich and Wulfe, Blake and Xiao, Ted and Yang, Jonathan Heewon and Yavary, Arefeh and Zhao, Tony Z. and Agia, Christopher and Baijal, Rohan and Castro, Mateo Guaman and Chen, Daphne and Chen, Qiuyu and Chung, Trinity and Drake, Jaimyn and Foster, Ethan Paul and Gao, Jensen and Guizilini, Vitor and Herrera, David Antonio and Heo, Minho and Hsu, Kyle and Hu, Jiaheng and Irshad, Muhammad Zubair and Jackson, Donovon and Le, Charlotte and Li, Yunshuang and Lin, Kevin and Lin, Roy and Ma, Zehan and Maddukuri, Abhiram and Mirchandani, Suvir and Morton, Daniel and Nguyen, Tony and O'Neill, Abigail and Scalise, Rosario and Seale, Derick and Son, Victor and Tian, Stephen and Tran, Emi and Wang, Andrew E. and Wu, Yilin and Xie, Annie and Yang, Jingyun and Yin, Patrick and Zhang, Yunchu and Bastani, Osbert and Berseth, Glen and Bohg, Jeannette and Goldberg, Ken and Gupta, Abhinav and Gupta, Abhishek and Jayaraman, Dinesh and Lim, Joseph J. and Malik, Jitendra and Martín-Martín, Roberto and Ramamoorthy, Subramanian and Sadigh, Dorsa and Song, Shuran and Wu, Jiajun and Yip, Michael C. and Zhu, Yuke and Kollar, Thomas and Levine, Sergey and Finn, Chelsea},
	month = apr,
	year = {2025},
	note = {arXiv:2403.12945 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\3AR5YMAE\\Khazatsky 等 - 2025 - DROID A Large-Scale In-The-Wild Robot Manipulation Dataset.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\T84VF8PF\\2403.html:text/html},
}

@misc{brohan_rt-1_2023,
	title = {{RT}-1: {Robotics} {Transformer} for {Real}-{World} {Control} at {Scale}},
	shorttitle = {{RT}-1},
	url = {http://arxiv.org/abs/2212.06817},
	doi = {10.48550/arXiv.2212.06817},
	abstract = {By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jackson, Tomas and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Malla, Utsav and Manjunath, Deeksha and Mordatch, Igor and Nachum, Ofir and Parada, Carolina and Peralta, Jodilyn and Perez, Emily and Pertsch, Karl and Quiambao, Jornell and Rao, Kanishka and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sayed, Kevin and Singh, Jaspiar and Sontakke, Sumedh and Stone, Austin and Tan, Clayton and Tran, Huong and Vanhoucke, Vincent and Vega, Steve and Vuong, Quan and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
	month = aug,
	year = {2023},
	note = {arXiv:2212.06817 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\F6AMMTBZ\\Brohan 等 - 2023 - RT-1 Robotics Transformer for Real-World Control at Scale.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\K6B6ZYJR\\2212.html:text/html},
}

@misc{salzer_bringing_2024,
	title = {Bringing the {RT}-1-{X} {Foundation} {Model} to a {SCARA} robot},
	url = {http://arxiv.org/abs/2409.03299},
	doi = {10.48550/arXiv.2409.03299},
	abstract = {Traditional robotic systems require specific training data for each task, environment, and robot form. While recent advancements in machine learning have enabled models to generalize across new tasks and environments, the challenge of adapting these models to entirely new settings remains largely unexplored. This study addresses this by investigating the generalization capabilities of the RT-1-X robotic foundation model to a type of robot unseen during its training: a SCARA robot from UMI-RTX. Initial experiments reveal that RT-1-X does not generalize zero-shot to the unseen type of robot. However, fine-tuning of the RT-1-X model by demonstration allows the robot to learn a pickup task which was part of the foundation model (but learned for another type of robot). When the robot is presented with an object that is included in the foundation model but not in the fine-tuning dataset, it demonstrates that only the skill, but not the object-specific knowledge, has been transferred.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Salzer, Jonathan and Visser, Arnoud},
	month = sep,
	year = {2024},
	note = {arXiv:2409.03299 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\T4IWTWYJ\\Salzer和Visser - 2024 - Bringing the RT-1-X Foundation Model to a SCARA robot.pdf:application/pdf},
}

@misc{gu_rt-trajectory_2023,
	title = {{RT}-{Trajectory}: {Robotic} {Task} {Generalization} via {Hindsight} {Trajectory} {Sketches}},
	shorttitle = {{RT}-{Trajectory}},
	url = {http://arxiv.org/abs/2311.01977},
	doi = {10.48550/arXiv.2311.01977},
	abstract = {Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to allow the learned policy to interpret the trajectory sketch in the context of situational visual observations. In addition, we show how trajectory sketches can provide a useful interface to communicate with robotic policies: they can be specified through simple human inputs like drawings or videos, or through automated methods such as modern image-generating or waypoint-generating methods. We evaluate RT-Trajectory at scale on a variety of real-world robotic tasks, and find that RT-Trajectory is able to perform a wider range of tasks compared to language-conditioned and goal-conditioned policies, when provided the same training data.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Gu, Jiayuan and Kirmani, Sean and Wohlhart, Paul and Lu, Yao and Arenas, Montserrat Gonzalez and Rao, Kanishka and Yu, Wenhao and Fu, Chuyuan and Gopalakrishnan, Keerthana and Xu, Zhuo and Sundaresan, Priya and Xu, Peng and Su, Hao and Hausman, Karol and Finn, Chelsea and Vuong, Quan and Xiao, Ted},
	month = nov,
	year = {2023},
	note = {arXiv:2311.01977 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\KEKKSU3A\\Gu 等 - 2023 - RT-Trajectory Robotic Task Generalization via Hindsight Trajectory Sketches.pdf:application/pdf},
}

@inproceedings{oneill_open_2024,
	title = {Open x-embodiment: {Robotic} learning datasets and rt-x models: {Open} x-embodiment collaboration 0},
	shorttitle = {Open x-embodiment},
	url = {https://ieeexplore.ieee.org/abstract/document/10611477/},
	urldate = {2026-02-06},
	booktitle = {2024 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {O’Neill, Abby and Rehman, Abdul and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and Jain, Ajinkya},
	year = {2024},
	pages = {6892--6903},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\J4KJLPL6\\O’Neill 等 - 2024 - Open x-embodiment Robotic learning datasets and rt-x models Open x-embodiment collaboration 0.pdf:application/pdf},
}

@misc{yoshikawa_achieving_2024,
	title = {Achieving {Faster} and {More} {Accurate} {Operation} of {Deep} {Predictive} {Learning}},
	url = {http://arxiv.org/abs/2408.10231},
	doi = {10.48550/arXiv.2408.10231},
	abstract = {Achieving both high speed and precision in robot operations is a significant challenge for social implementation. While factory robots excel at predefined tasks, they struggle with environment-specific actions like cleaning and cooking. Deep learning research aims to address this by enabling robots to autonomously execute behaviors through end-to-end learning with sensor data. RT-1 and ACT are notable examples that have expanded robots' capabilities. However, issues with model inference speed and hand position accuracy persist. High-quality training data and fast, stable inference mechanisms are essential to overcome these challenges. This paper proposes a motion generation model for high-speed, high-precision tasks, exemplified by the sports stacking task. By teaching motions slowly and inferring at high speeds, the model achieved a 94\% success rate in stacking cups with a real robot.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Yoshikawa, Masaki and Ito, Hiroshi and Ogata, Tetsuya},
	month = aug,
	year = {2024},
	note = {arXiv:2408.10231 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\YLKTGPZB\\Yoshikawa 等 - 2024 - Achieving Faster and More Accurate Operation of Deep Predictive Learning.pdf:application/pdf},
}

@inproceedings{leal_sara-rt_2024,
	title = {Sara-rt: {Scaling} up robotics transformers with self-adaptive robust attention},
	shorttitle = {Sara-rt},
	url = {https://ieeexplore.ieee.org/abstract/document/10611597/},
	urldate = {2026-02-06},
	booktitle = {2024 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Leal, Isabel and Choromanski, Krzysztof and Jain, Deepali and Dubey, Avinava and Varley, Jake and Ryoo, Michael and Lu, Yao and Liu, Frederick and Sindhwani, Vikas and Vuong, Quan},
	year = {2024},
	pages = {6920--6927},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\9FE6VDAZ\\Leal 等 - 2024 - Sara-rt Scaling up robotics transformers with self-adaptive robust attention.pdf:application/pdf},
}

@inproceedings{walke_bridgedata_2023,
	title = {Bridgedata v2: {A} dataset for robot learning at scale},
	shorttitle = {Bridgedata v2},
	url = {https://proceedings.mlr.press/v229/walke23a.html},
	urldate = {2026-02-06},
	booktitle = {Conference on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Walke, Homer Rich and Black, Kevin and Zhao, Tony Z. and Vuong, Quan and Zheng, Chongyi and Hansen-Estruch, Philippe and He, Andre Wang and Myers, Vivek and Kim, Moo Jin and Du, Max},
	year = {2023},
	pages = {1723--1736},
	file = {Available Version (via Google Scholar):E\:\\Users\\AresZz\\Zotero\\storage\\9MDMRANU\\Walke 等 - 2023 - Bridgedata v2 A dataset for robot learning at scale.pdf:application/pdf},
}

@misc{team_octo_2024,
	title = {Octo: {An} {Open}-{Source} {Generalist} {Robot} {Policy}},
	shorttitle = {Octo},
	url = {http://arxiv.org/abs/2405.12213},
	doi = {10.48550/arXiv.2405.12213},
	abstract = {Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Team, Octo Model and Ghosh, Dibya and Walke, Homer and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Kreiman, Tobias and Xu, Charles and Luo, Jianlan and Tan, You Liang and Chen, Lawrence Yunliang and Sanketi, Pannag and Vuong, Quan and Xiao, Ted and Sadigh, Dorsa and Finn, Chelsea and Levine, Sergey},
	month = may,
	year = {2024},
	note = {arXiv:2405.12213 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\KK6ZJIJ3\\Team 等 - 2024 - Octo An Open-Source Generalist Robot Policy.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\E2339YXF\\2405.html:text/html},
}

@inproceedings{zhou_chatvla_2025,
	address = {Suzhou, China},
	title = {{ChatVLA}: {Unified} {Multimodal} {Understanding} and {Robot} {Control} with {Vision}-{Language}-{Action} {Model}},
	isbn = {979-8-89176-332-6},
	shorttitle = {{ChatVLA}},
	url = {https://aclanthology.org/2025.emnlp-main.273/},
	doi = {10.18653/v1/2025.emnlp-main.273},
	abstract = {Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world. Why can't large language models replicate this holistic understanding? Through a systematic analysis of existing training paradigms in vision-language-action models (VLA), we identify two key challenges: spurious forgetting, where robot training overwrites crucial visual-text alignments, and task interference, where competing control and understanding tasks degrade performance when trained jointly. To overcome these limitations, we propose ChatVLA, a novel framework featuring Phased Alignment Training, which incrementally integrates multimodal data after initial control mastery, and a Mixture-of-Experts architecture to minimize task interference. ChatVLA demonstrates competitive performance on visual question-answering datasets and significantly surpasses state-of-the-art vision-language-action (VLA) methods on multimodal understanding benchmarks. Notably, it achieves a six times higher performance on MMMU and scores 47.2\% on MMStar with a more parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates superior performance on 25 real-world robot manipulation tasks compared to existing VLA methods like OpenVLA. Our findings highlight the potential of our unified framework for achieving both robust multimodal understanding and effective robot control.},
	urldate = {2026-02-06},
	booktitle = {Proceedings of the 2025 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhou, Zhongyi and Zhu, Yichen and Zhu, Minjie and Wen, Junjie and Liu, Ning and Xu, Zhiyuan and Meng, Weibin and Peng, Yaxin and Shen, Chaomin and Feng, Feifei and Xu, Yi},
	editor = {Christodoulopoulos, Christos and Chakraborty, Tanmoy and Rose, Carolyn and Peng, Violet},
	month = nov,
	year = {2025},
	pages = {5377--5395},
	file = {PDF:E\:\\Users\\AresZz\\Zotero\\storage\\DIVU36XE\\Zhou 等 - 2025 - ChatVLA Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model.pdf:application/pdf},
}

@misc{huang_grounded_2023,
	title = {Grounded {Decoding}: {Guiding} {Text} {Generation} with {Grounded} {Models} for {Embodied} {Agents}},
	shorttitle = {Grounded {Decoding}},
	url = {http://arxiv.org/abs/2303.00855},
	doi = {10.48550/arXiv.2303.00855},
	abstract = {Recent progress in large language models (LLMs) has demonstrated the ability to learn and leverage Internet-scale knowledge through pre-training with autoregressive models. Unfortunately, applying such models to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require. On the other hand, language-conditioned robotic policies that learn from interaction data can provide the necessary grounding that allows the agent to be correctly situated in the real world, but such policies are limited by the lack of high-level semantic understanding due to the limited breadth of the interaction data available for training them. Thus, if we want to make use of the semantic knowledge in a language model while still situating it in an embodied setting, we must construct an action sequence that is both likely according to the language model and also realizable according to grounded models of the environment. We frame this as a problem similar to probabilistic filtering: decode a sequence that both has high probability under the language model and high probability under a set of grounded model objectives. We demonstrate how such grounded models can be obtained across three simulation and real-world domains, and that the proposed decoding strategy is able to solve complex, long-horizon embodiment tasks in a robotic setting by leveraging the knowledge of both models. The project's website can be found at grounded-decoding.github.io.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Huang, Wenlong and Xia, Fei and Shah, Dhruv and Driess, Danny and Zeng, Andy and Lu, Yao and Florence, Pete and Mordatch, Igor and Levine, Sergey and Hausman, Karol and Ichter, Brian},
	month = dec,
	year = {2023},
	note = {arXiv:2303.00855 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\NRRFRYYX\\Huang 等 - 2023 - Grounded Decoding Guiding Text Generation with Grounded Models for Embodied Agents.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\W6W4LN6T\\2303.html:text/html},
}

@misc{song_llm-planner_2023,
	title = {{LLM}-{Planner}: {Few}-{Shot} {Grounded} {Planning} for {Embodied} {Agents} with {Large} {Language} {Models}},
	shorttitle = {{LLM}-{Planner}},
	url = {http://arxiv.org/abs/2212.04088},
	doi = {10.48550/arXiv.2212.04088},
	abstract = {This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5\% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. Website: https://dki-lab.github.io/LLM-Planner},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Song, Chan Hee and Wu, Jiaman and Washington, Clayton and Sadler, Brian M. and Chao, Wei-Lun and Su, Yu},
	month = mar,
	year = {2023},
	note = {arXiv:2212.04088 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\KTRAUVC6\\Song 等 - 2023 - LLM-Planner Few-Shot Grounded Planning for Embodied Agents with Large Language Models.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\XR39MSHZ\\2212.html:text/html},
}

@misc{fan_minedojo_2022,
	title = {{MineDojo}: {Building} {Open}-{Ended} {Embodied} {Agents} with {Internet}-{Scale} {Knowledge}},
	shorttitle = {{MineDojo}},
	url = {http://arxiv.org/abs/2206.08853},
	doi = {10.48550/arXiv.2206.08853},
	abstract = {Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MineDojo, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MineDojo's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of open-ended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Fan, Linxi and Wang, Guanzhi and Jiang, Yunfan and Mandlekar, Ajay and Yang, Yuncong and Zhu, Haoyi and Tang, Andrew and Huang, De-An and Zhu, Yuke and Anandkumar, Anima},
	month = nov,
	year = {2022},
	note = {arXiv:2206.08853 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\8L6E86QK\\Fan 等 - 2022 - MineDojo Building Open-Ended Embodied Agents with Internet-Scale Knowledge.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\JLG4FA8G\\2206.html:text/html},
}

@misc{collaboration_open_2025,
	title = {Open {X}-{Embodiment}: {Robotic} {Learning} {Datasets} and {RT}-{X} {Models}},
	shorttitle = {Open {X}-{Embodiment}},
	url = {http://arxiv.org/abs/2310.08864},
	doi = {10.48550/arXiv.2310.08864},
	abstract = {Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website https://robotics-transformer-x.github.io.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Collaboration, Open X.-Embodiment and O'Neill, Abby and Rehman, Abdul and Gupta, Abhinav and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and Jain, Ajinkya and Tung, Albert and Bewley, Alex and Herzog, Alex and Irpan, Alex and Khazatsky, Alexander and Rai, Anant and Gupta, Anchit and Wang, Andrew and Kolobov, Andrey and Singh, Anikait and Garg, Animesh and Kembhavi, Aniruddha and Xie, Annie and Brohan, Anthony and Raffin, Antonin and Sharma, Archit and Yavary, Arefeh and Jain, Arhan and Balakrishna, Ashwin and Wahid, Ayzaan and Burgess-Limerick, Ben and Kim, Beomjoon and Schölkopf, Bernhard and Wulfe, Blake and Ichter, Brian and Lu, Cewu and Xu, Charles and Le, Charlotte and Finn, Chelsea and Wang, Chen and Xu, Chenfeng and Chi, Cheng and Huang, Chenguang and Chan, Christine and Agia, Christopher and Pan, Chuer and Fu, Chuyuan and Devin, Coline and Xu, Danfei and Morton, Daniel and Driess, Danny and Chen, Daphne and Pathak, Deepak and Shah, Dhruv and Büchler, Dieter and Jayaraman, Dinesh and Kalashnikov, Dmitry and Sadigh, Dorsa and Johns, Edward and Foster, Ethan and Liu, Fangchen and Ceola, Federico and Xia, Fei and Zhao, Feiyu and Frujeri, Felipe Vieira and Stulp, Freek and Zhou, Gaoyue and Sukhatme, Gaurav S. and Salhotra, Gautam and Yan, Ge and Feng, Gilbert and Schiavi, Giulio and Berseth, Glen and Kahn, Gregory and Yang, Guangwen and Wang, Guanzhi and Su, Hao and Fang, Hao-Shu and Shi, Haochen and Bao, Henghui and Amor, Heni Ben and Christensen, Henrik I. and Furuta, Hiroki and Bharadhwaj, Homanga and Walke, Homer and Fang, Hongjie and Ha, Huy and Mordatch, Igor and Radosavovic, Ilija and Leal, Isabel and Liang, Jacky and Abou-Chakra, Jad and Kim, Jaehyung and Drake, Jaimyn and Peters, Jan and Schneider, Jan and Hsu, Jasmine and Vakil, Jay and Bohg, Jeannette and Bingham, Jeffrey and Wu, Jeffrey and Gao, Jensen and Hu, Jiaheng and Wu, Jiajun and Wu, Jialin and Sun, Jiankai and Luo, Jianlan and Gu, Jiayuan and Tan, Jie and Oh, Jihoon and Wu, Jimmy and Lu, Jingpei and Yang, Jingyun and Malik, Jitendra and Silvério, João and Hejna, Joey and Booher, Jonathan and Tompson, Jonathan and Yang, Jonathan and Salvador, Jordi and Lim, Joseph J. and Han, Junhyek and Wang, Kaiyuan and Rao, Kanishka and Pertsch, Karl and Hausman, Karol and Go, Keegan and Gopalakrishnan, Keerthana and Goldberg, Ken and Byrne, Kendra and Oslund, Kenneth and Kawaharazuka, Kento and Black, Kevin and Lin, Kevin and Zhang, Kevin and Ehsani, Kiana and Lekkala, Kiran and Ellis, Kirsty and Rana, Krishan and Srinivasan, Krishnan and Fang, Kuan and Singh, Kunal Pratap and Zeng, Kuo-Hao and Hatch, Kyle and Hsu, Kyle and Itti, Laurent and Chen, Lawrence Yunliang and Pinto, Lerrel and Fei-Fei, Li and Tan, Liam and Fan, Linxi "Jim" and Ott, Lionel and Lee, Lisa and Weihs, Luca and Chen, Magnum and Lepert, Marion and Memmel, Marius and Tomizuka, Masayoshi and Itkina, Masha and Castro, Mateo Guaman and Spero, Max and Du, Maximilian and Ahn, Michael and Yip, Michael C. and Zhang, Mingtong and Ding, Mingyu and Heo, Minho and Srirama, Mohan Kumar and Sharma, Mohit and Kim, Moo Jin and Irshad, Muhammad Zubair and Kanazawa, Naoaki and Hansen, Nicklas and Heess, Nicolas and Joshi, Nikhil J. and Suenderhauf, Niko and Liu, Ning and Palo, Norman Di and Shafiullah, Nur Muhammad Mahi and Mees, Oier and Kroemer, Oliver and Bastani, Osbert and Sanketi, Pannag R. and Miller, Patrick "Tree" and Yin, Patrick and Wohlhart, Paul and Xu, Peng and Fagan, Peter David and Mitrano, Peter and Sermanet, Pierre and Abbeel, Pieter and Sundaresan, Priya and Chen, Qiuyu and Vuong, Quan and Rafailov, Rafael and Tian, Ran and Doshi, Ria and Martín-Martín, Roberto and Baijal, Rohan and Scalise, Rosario and Hendrix, Rose and Lin, Roy and Qian, Runjia and Zhang, Ruohan and Mendonca, Russell and Shah, Rutav and Hoque, Ryan and Julian, Ryan and Bustamante, Samuel and Kirmani, Sean and Levine, Sergey and Lin, Shan and Moore, Sherry and Bahl, Shikhar and Dass, Shivin and Sonawani, Shubham and Tulsiani, Shubham and Song, Shuran and Xu, Sichun and Haldar, Siddhant and Karamcheti, Siddharth and Adebola, Simeon and Guist, Simon and Nasiriany, Soroush and Schaal, Stefan and Welker, Stefan and Tian, Stephen and Ramamoorthy, Subramanian and Dasari, Sudeep and Belkhale, Suneel and Park, Sungjae and Nair, Suraj and Mirchandani, Suvir and Osa, Takayuki and Gupta, Tanmay and Harada, Tatsuya and Matsushima, Tatsuya and Xiao, Ted and Kollar, Thomas and Yu, Tianhe and Ding, Tianli and Davchev, Todor and Zhao, Tony Z. and Armstrong, Travis and Darrell, Trevor and Chung, Trinity and Jain, Vidhi and Kumar, Vikash and Vanhoucke, Vincent and Guizilini, Vitor and Zhan, Wei and Zhou, Wenxuan and Burgard, Wolfram and Chen, Xi and Chen, Xiangyu and Wang, Xiaolong and Zhu, Xinghao and Geng, Xinyang and Liu, Xiyuan and Liangwei, Xu and Li, Xuanlin and Pang, Yansong and Lu, Yao and Ma, Yecheng Jason and Kim, Yejin and Chebotar, Yevgen and Zhou, Yifan and Zhu, Yifeng and Wu, Yilin and Xu, Ying and Wang, Yixuan and Bisk, Yonatan and Dou, Yongqiang and Cho, Yoonyoung and Lee, Youngwoon and Cui, Yuchen and Cao, Yue and Wu, Yueh-Hua and Tang, Yujin and Zhu, Yuke and Zhang, Yunchu and Jiang, Yunfan and Li, Yunshuang and Li, Yunzhu and Iwasawa, Yusuke and Matsuo, Yutaka and Ma, Zehan and Xu, Zhuo and Cui, Zichen Jeff and Zhang, Zichen and Fu, Zipeng and Lin, Zipeng},
	month = may,
	year = {2025},
	note = {arXiv:2310.08864 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\TJCBFW7X\\Collaboration 等 - 2025 - Open X-Embodiment Robotic Learning Datasets and RT-X Models.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\MNZRURVA\\2310.html:text/html},
}

@misc{sarch_open-ended_2023,
	title = {Open-{Ended} {Instructable} {Embodied} {Agents} with {Memory}-{Augmented} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.15127},
	doi = {10.48550/arXiv.2310.15127},
	abstract = {Pre-trained and frozen large language models (LLMs) can effectively map simple scene rearrangement instructions to programs over a robot's visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user's idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short. In this paper, we introduce HELPER, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction, or VLM description, and used as in-context prompt examples for LLM querying. The memory is expanded during deployment to include pairs of user's language and action plans, to assist future inferences and personalize them to the user's language and routines. HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for TfD. Our models, code, and video results can be found in our project's website: https://helper-agent-llm.github.io.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Sarch, Gabriel and Wu, Yue and Tarr, Michael J. and Fragkiadaki, Katerina},
	month = nov,
	year = {2023},
	note = {arXiv:2310.15127 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\28RLVQI2\\Sarch 等 - 2023 - Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\R7AZ4BCF\\2310.html:text/html},
}

@inproceedings{song_pd-vla_2025,
	title = {{PD}-{VLA}: {Accelerating} {Vision}-{Language}-{Action} {Model} {Integrated} with {Action} {Chunking} via {Parallel} {Decoding}},
	issn = {2153-0866},
	shorttitle = {{PD}-{VLA}},
	url = {https://ieeexplore.ieee.org/document/11247519/},
	doi = {10.1109/IROS60139.2025.11247519},
	abstract = {Vision-Language-Action (VLA) models demonstrate remarkable potential for generalizable robotic manipulation. The performance of VLA models can be improved by integrating with action chunking, a critical technique for effective control. However, action chunking linearly scales up action dimensions in VLA models with increased chunking sizes. This reduces the inference efficiency. Therefore, accelerating VLA integrated with action chunking is an urgent need. To tackle this problem, we propose PD-VLA, the first parallel decoding framework for VLA models integrated with action chunking. Our framework reformulates autoregressive decoding as a nonlinear system solved by parallel fixed-point iterations. This approach preserves model performance with mathematical guarantees while significantly improving decoding speed. In addition, it enables training-free acceleration without architectural changes, as well as seamless synergy with existing acceleration techniques. Extensive simulations validate that our PD-VLA maintains competitive success rates while achieving 2.52× execution frequency on manipulators (with 7 degrees of freedom) compared with the fundamental VLA model. Furthermore, we experimentally identify the most effective settings for acceleration. Finally, real-world experiments validate its high applicability across different tasks.},
	urldate = {2026-02-06},
	booktitle = {2025 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Song, Wenxuan and Chen, Jiayi and Ding, Pengxiang and Zhao, Han and Zhao, Wei and Zhong, Zhide and Ge, Zongyuan and Li, Zhijun and Wang, Donglin and Wang, Lujia and Ma, Jun and Li, Haoang},
	month = oct,
	year = {2025},
	note = {ISSN: 2153-0866},
	keywords = {Analytical models, Convergence, Decoding, Inference algorithms, Intelligent robots, Iterative decoding, Manipulators, Mathematical models, Nonlinear systems},
	pages = {13162--13169},
	file = {PDF:E\:\\Users\\AresZz\\Zotero\\storage\\M8QDIVYU\\Song 等 - 2025 - PD-VLA Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decod.pdf:application/pdf},
}

@misc{li_towards_2024,
	title = {Towards {Generalist} {Robot} {Policies}: {What} {Matters} in {Building} {Vision}-{Language}-{Action} {Models}},
	shorttitle = {Towards {Generalist} {Robot} {Policies}},
	url = {http://arxiv.org/abs/2412.14058},
	doi = {10.48550/arXiv.2412.14058},
	abstract = {Foundation Vision Language Models (VLMs) exhibit strong capabilities in multi-modal representation learning, comprehension, and reasoning. By injecting action components into the VLMs, Vision-Language-Action Models (VLAs) can be naturally formed and also show promising performance. Existing work has demonstrated the effectiveness and generalization of VLAs in multiple scenarios and tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since existing VLAs differ in their backbones, action-prediction formulations, data distributions, and training recipes. This leads to a missing piece for a systematic understanding of the design choices of VLAs. In this work, we disclose the key factors that significantly influence the performance of VLA and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures, and when to add cross-embodiment data. The obtained results convince us firmly to explain why we need VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures, and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets, and toolkits, along with detailed training and evaluation recipes at: robovlms.github.io.},
	urldate = {2026-02-06},
	publisher = {arXiv},
	author = {Li, Xinghang and Li, Peiyan and Liu, Minghuan and Wang, Dong and Liu, Jirong and Kang, Bingyi and Ma, Xiao and Kong, Tao and Zhang, Hanbo and Liu, Huaping},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14058 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\X4DR4STP\\Li 等 - 2024 - Towards Generalist Robot Policies What Matters in Building Vision-Language-Action Models.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\B5EX3K9L\\2412.html:text/html},
}

@misc{driess_palm-e_2023,
	title = {{PaLM}-{E}: {An} {Embodied} {Multimodal} {Language} {Model}},
	shorttitle = {{PaLM}-{E}},
	url = {http://arxiv.org/abs/2303.03378},
	doi = {10.48550/arXiv.2303.03378},
	abstract = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
	month = mar,
	year = {2023},
	note = {arXiv:2303.03378 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\VMRYI8RQ\\Driess 等 - 2023 - PaLM-E An Embodied Multimodal Language Model.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\2L9S7RCI\\2303.html:text/html},
}

@misc{reed_generalist_2022,
	title = {A {Generalist} {Agent}},
	url = {http://arxiv.org/abs/2205.06175},
	doi = {10.48550/arXiv.2205.06175},
	abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and Freitas, Nando de},
	month = nov,
	year = {2022},
	note = {arXiv:2205.06175 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\LL6K36ZA\\Reed 等 - 2022 - A Generalist Agent.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\DI2V6DP8\\2205.html:text/html},
}

@misc{bousmalis_robocat_2023,
	title = {{RoboCat}: {A} {Self}-{Improving} {Generalist} {Agent} for {Robotic} {Manipulation}},
	shorttitle = {{RoboCat}},
	url = {http://arxiv.org/abs/2306.11706},
	doi = {10.48550/arXiv.2306.11706},
	abstract = {The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a multi-embodiment, multi-task generalist agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100-1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Bousmalis, Konstantinos and Vezzani, Giulia and Rao, Dushyant and Devin, Coline and Lee, Alex X. and Bauza, Maria and Davchev, Todor and Zhou, Yuxiang and Gupta, Agrim and Raju, Akhil and Laurens, Antoine and Fantacci, Claudio and Dalibard, Valentin and Zambelli, Martina and Martins, Murilo and Pevceviciute, Rugile and Blokzijl, Michiel and Denil, Misha and Batchelor, Nathan and Lampe, Thomas and Parisotto, Emilio and Żołna, Konrad and Reed, Scott and Colmenarejo, Sergio Gómez and Scholz, Jon and Abdolmaleki, Abbas and Groth, Oliver and Regli, Jean-Baptiste and Sushkov, Oleg and Rothörl, Tom and Chen, José Enrique and Aytar, Yusuf and Barker, Dave and Ortiz, Joy and Riedmiller, Martin and Springenberg, Jost Tobias and Hadsell, Raia and Nori, Francesco and Heess, Nicolas},
	month = dec,
	year = {2023},
	note = {arXiv:2306.11706 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\FVDG7GFK\\Bousmalis 等 - 2023 - RoboCat A Self-Improving Generalist Agent for Robotic Manipulation.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\QVQSNMQC\\2306.html:text/html},
}

@misc{jiang_vima_2023,
	title = {{VIMA}: {General} {Robot} {Manipulation} with {Multimodal} {Prompts}},
	shorttitle = {{VIMA}},
	url = {http://arxiv.org/abs/2210.03094},
	doi = {10.48550/arXiv.2210.03094},
	abstract = {Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieves strong model scalability and data efficiency. It outperforms alternative designs in the hardest zero-shot generalization setting by up to \$2.9{\textbackslash}times\$ task success rate given the same training data. With \$10{\textbackslash}times\$ less training data, VIMA still performs \$2.7{\textbackslash}times\$ better than the best competing variant. Code and video demos are available at https://vimalabs.github.io/},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Jiang, Yunfan and Gupta, Agrim and Zhang, Zichen and Wang, Guanzhi and Dou, Yongqiang and Chen, Yanjun and Fei-Fei, Li and Anandkumar, Anima and Zhu, Yuke and Fan, Linxi},
	month = may,
	year = {2023},
	note = {arXiv:2210.03094 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\IPIWQ4XI\\Jiang 等 - 2023 - VIMA General Robot Manipulation with Multimodal Prompts.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\UAI9UL7U\\2210.html:text/html},
}

@misc{huang_voxposer_2023,
	title = {{VoxPoser}: {Composable} {3D} {Value} {Maps} for {Robotic} {Manipulation} with {Language} {Models}},
	shorttitle = {{VoxPoser}},
	url = {http://arxiv.org/abs/2307.05973},
	doi = {10.48550/arXiv.2307.05973},
	abstract = {Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https://voxposer.github.io},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and Fei-Fei, Li},
	month = nov,
	year = {2023},
	note = {arXiv:2307.05973 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\UJSKCUKQ\\Huang 等 - 2023 - VoxPoser Composable 3D Value Maps for Robotic Manipulation with Language Models.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\2YTVUX99\\2307.html:text/html},
}

@misc{ahn_as_2022,
	title = {Do {As} {I} {Can}, {Not} {As} {I} {Say}: {Grounding} {Language} in {Robotic} {Affordances}},
	shorttitle = {Do {As} {I} {Can}, {Not} {As} {I} {Say}},
	url = {http://arxiv.org/abs/2204.01691},
	doi = {10.48550/arXiv.2204.01691},
	abstract = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's "hands and eyes," while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/.},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan and Zeng, Andy},
	month = aug,
	year = {2022},
	note = {arXiv:2204.01691 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\Y84GDHD6\\Ahn 等 - 2022 - Do As I Can, Not As I Say Grounding Language in Robotic Affordances.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\R8YL42CP\\2204.html:text/html},
}

@misc{liang_code_2023,
	title = {Code as {Policies}: {Language} {Model} {Programs} for {Embodied} {Control}},
	shorttitle = {Code as {Policies}},
	url = {http://arxiv.org/abs/2209.07753},
	doi = {10.48550/arXiv.2209.07753},
	abstract = {Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions ("faster") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
	month = may,
	year = {2023},
	note = {arXiv:2209.07753 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\QIW8RQIC\\Liang 等 - 2023 - Code as Policies Language Model Programs for Embodied Control.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\NWIGW7XY\\2209.html:text/html},
}

@misc{huang_inner_2022,
	title = {Inner {Monologue}: {Embodied} {Reasoning} through {Planning} with {Language} {Models}},
	shorttitle = {Inner {Monologue}},
	url = {http://arxiv.org/abs/2207.05608},
	doi = {10.48550/arXiv.2207.05608},
	abstract = {Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and Sermanet, Pierre and Brown, Noah and Jackson, Tomas and Luu, Linda and Levine, Sergey and Hausman, Karol and Ichter, Brian},
	month = jul,
	year = {2022},
	note = {arXiv:2207.05608 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\I9DMHVMU\\Huang 等 - 2022 - Inner Monologue Embodied Reasoning through Planning with Language Models.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\H5CTDXD6\\2207.html:text/html},
}

@misc{li_vision-language_2024,
	title = {Vision-{Language} {Foundation} {Models} as {Effective} {Robot} {Imitators}},
	url = {http://arxiv.org/abs/2311.01378},
	doi = {10.48550/arXiv.2311.01378},
	abstract = {Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Li, Xinghang and Liu, Minghuan and Zhang, Hanbo and Yu, Cunjun and Xu, Jie and Wu, Hongtao and Cheang, Chilam and Jing, Ya and Zhang, Weinan and Liu, Huaping and Li, Hang and Kong, Tao},
	month = feb,
	year = {2024},
	note = {arXiv:2311.01378 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\AXWJQK25\\Li 等 - 2024 - Vision-Language Foundation Models as Effective Robot Imitators.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\8W39WHED\\2311.html:text/html},
}

@misc{nvidia_gr00t_2025,
	title = {{GR00T} {N1}: {An} {Open} {Foundation} {Model} for {Generalist} {Humanoid} {Robots}},
	shorttitle = {{GR00T} {N1}},
	url = {http://arxiv.org/abs/2503.14734},
	doi = {10.48550/arXiv.2503.14734},
	abstract = {General-purpose robots need a versatile body and an intelligent mind. Recent advancements in humanoid robots have shown great promise as a hardware platform for building generalist autonomy in the human world. A robot foundation model, trained on massive and diverse data sources, is essential for enabling the robots to reason about novel situations, robustly handle real-world variability, and rapidly learn new tasks. To this end, we introduce GR00T N1, an open foundation model for humanoid robots. GR00T N1 is a Vision-Language-Action (VLA) model with a dual-system architecture. The vision-language module (System 2) interprets the environment through vision and language instructions. The subsequent diffusion transformer module (System 1) generates fluid motor actions in real time. Both modules are tightly coupled and jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture of real-robot trajectories, human videos, and synthetically generated datasets. We show that our generalist robot model GR00T N1 outperforms the state-of-the-art imitation learning baselines on standard simulation benchmarks across multiple robot embodiments. Furthermore, we deploy our model on the Fourier GR-1 humanoid robot for language-conditioned bimanual manipulation tasks, achieving strong performance with high data efficiency.},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {NVIDIA and Bjorck, Johan and Castañeda, Fernando and Cherniadev, Nikita and Da, Xingye and Ding, Runyu and Fan, Linxi "Jim" and Fang, Yu and Fox, Dieter and Hu, Fengyuan and Huang, Spencer and Jang, Joel and Jiang, Zhenyu and Kautz, Jan and Kundalia, Kaushil and Lao, Lawrence and Li, Zhiqi and Lin, Zongyu and Lin, Kevin and Liu, Guilin and Llontop, Edith and Magne, Loic and Mandlekar, Ajay and Narayan, Avnish and Nasiriany, Soroush and Reed, Scott and Tan, You Liang and Wang, Guanzhi and Wang, Zu and Wang, Jing and Wang, Qi and Xiang, Jiannan and Xie, Yuqi and Xu, Yinzhen and Xu, Zhenjia and Ye, Seonghyeon and Yu, Zhiding and Zhang, Ao and Zhang, Hao and Zhao, Yizhou and Zheng, Ruijie and Zhu, Yuke},
	month = mar,
	year = {2025},
	note = {arXiv:2503.14734 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\CU2AZ464\\NVIDIA 等 - 2025 - GR00T N1 An Open Foundation Model for Generalist Humanoid Robots.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\Y6J6VVUN\\2503.html:text/html},
}

@misc{zeng_learning_2024,
	title = {Learning {Manipulation} by {Predicting} {Interaction}},
	url = {http://arxiv.org/abs/2406.00439},
	doi = {10.48550/arXiv.2406.00439},
	abstract = {Representation learning approaches for robotic manipulation have boomed in recent years. Due to the scarcity of in-domain robot data, prevailing methodologies tend to leverage large-scale human video datasets to extract generalizable features for visuomotor policy learning. Despite the progress achieved, prior endeavors disregard the interactive dynamics that capture behavior patterns and physical interaction during the manipulation process, resulting in an inadequate understanding of the relationship between objects and the environment. To this end, we propose a general pre-training pipeline that learns Manipulation by Predicting the Interaction (MPI) and enhances the visual representation.Given a pair of keyframes representing the initial and final states, along with language instructions, our algorithm predicts the transition frame and detects the interaction object, respectively. These two learning objectives achieve superior comprehension towards "how-to-interact" and "where-to-interact". We conduct a comprehensive evaluation of several challenging robotic tasks.The experimental results demonstrate that MPI exhibits remarkable improvement by 10\% to 64\% compared with previous state-of-the-art in real-world robot platforms as well as simulation environments. Code and checkpoints are publicly shared at https://github.com/OpenDriveLab/MPI.},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Zeng, Jia and Bu, Qingwen and Wang, Bangjun and Xia, Wenke and Chen, Li and Dong, Hao and Song, Haoming and Wang, Dong and Hu, Di and Luo, Ping and Cui, Heming and Zhao, Bin and Li, Xuelong and Qiao, Yu and Li, Hongyang},
	month = jun,
	year = {2024},
	note = {arXiv:2406.00439 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\H2JBGD3R\\Zeng 等 - 2024 - Learning Manipulation by Predicting Interaction.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\DFYIUBHJ\\2406.html:text/html},
}

@misc{zhen_3d-vla_2024,
	title = {{3D}-{VLA}: {A} {3D} {Vision}-{Language}-{Action} {Generative} {World} {Model}},
	shorttitle = {{3D}-{VLA}},
	url = {http://arxiv.org/abs/2403.09631},
	doi = {10.48550/arXiv.2403.09631},
	abstract = {Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds. To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications.},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Zhen, Haoyu and Qiu, Xiaowen and Chen, Peihao and Yang, Jincheng and Yan, Xin and Du, Yilun and Hong, Yining and Gan, Chuang},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09631 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\4K5QZCP7\\Zhen 等 - 2024 - 3D-VLA A 3D Vision-Language-Action Generative World Model.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\T4XXQ8PM\\2403.html:text/html},
}

@misc{huang_copa_2024,
	title = {{CoPa}: {General} {Robotic} {Manipulation} through {Spatial} {Constraints} of {Parts} with {Foundation} {Models}},
	shorttitle = {{CoPa}},
	url = {http://arxiv.org/abs/2403.08248},
	doi = {10.48550/arXiv.2403.08248},
	abstract = {Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object's grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: https://copa-2024.github.io/},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Huang, Haoxu and Lin, Fanqi and Hu, Yingdong and Wang, Shengjie and Gao, Yang},
	month = mar,
	year = {2024},
	note = {arXiv:2403.08248 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\TP3U8XFM\\Huang 等 - 2024 - CoPa General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\N6IFNFD4\\2403.html:text/html},
}

@misc{chebotar_q-transformer_2023,
	title = {Q-{Transformer}: {Scalable} {Offline} {Reinforcement} {Learning} via {Autoregressive} {Q}-{Functions}},
	shorttitle = {Q-{Transformer}},
	url = {http://arxiv.org/abs/2309.10150},
	doi = {10.48550/arXiv.2309.10150},
	abstract = {In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite. The project's website and videos can be found at https://qtransformer.github.io},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Chebotar, Yevgen and Vuong, Quan and Irpan, Alex and Hausman, Karol and Xia, Fei and Lu, Yao and Kumar, Aviral and Yu, Tianhe and Herzog, Alexander and Pertsch, Karl and Gopalakrishnan, Keerthana and Ibarz, Julian and Nachum, Ofir and Sontakke, Sumedh and Salazar, Grecia and Tran, Huong T. and Peralta, Jodilyn and Tan, Clayton and Manjunath, Deeksha and Singht, Jaspiar and Zitkovich, Brianna and Jackson, Tomas and Rao, Kanishka and Finn, Chelsea and Levine, Sergey},
	month = oct,
	year = {2023},
	note = {arXiv:2309.10150 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\HGE5IU9W\\Chebotar 等 - 2023 - Q-Transformer Scalable Offline Reinforcement Learning via Autoregressive Q-Functions.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\KH3N4FQL\\2309.html:text/html},
}

@misc{zhao_learning_2023,
	title = {Learning {Fine}-{Grained} {Bimanual} {Manipulation} with {Low}-{Cost} {Hardware}},
	url = {http://arxiv.org/abs/2304.13705},
	doi = {10.48550/arXiv.2304.13705},
	abstract = {Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers (ACT), which learns a generative model over action sequences. ACT allows the robot to learn 6 difficult tasks in the real world, such as opening a translucent condiment cup and slotting a battery with 80-90\% success, with only 10 minutes worth of demonstrations. Project website: https://tonyzhaozh.github.io/aloha/},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Zhao, Tony Z. and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
	month = apr,
	year = {2023},
	note = {arXiv:2304.13705 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\JYN34XYF\\Zhao 等 - 2023 - Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\N65QHYCW\\2304.html:text/html},
}

@misc{chi_diffusion_2024,
	title = {Diffusion {Policy}: {Visuomotor} {Policy} {Learning} via {Action} {Diffusion}},
	shorttitle = {Diffusion {Policy}},
	url = {http://arxiv.org/abs/2303.04137},
	doi = {10.48550/arXiv.2303.04137},
	abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Chi, Cheng and Xu, Zhenjia and Feng, Siyuan and Cousineau, Eric and Du, Yilun and Burchfiel, Benjamin and Tedrake, Russ and Song, Shuran},
	month = mar,
	year = {2024},
	note = {arXiv:2303.04137 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\L9X37DDM\\Chi 等 - 2024 - Diffusion Policy Visuomotor Policy Learning via Action Diffusion.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\LAVGRI4U\\2303.html:text/html},
}

@misc{lee_behavior_2024,
	title = {Behavior {Generation} with {Latent} {Actions}},
	url = {http://arxiv.org/abs/2403.03181},
	doi = {10.48550/arXiv.2403.03181},
	abstract = {Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision making. Unlike language or image generation, decision making requires modeling actions - continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction. A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes. However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions. In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations. VQ-BeT augments BeT by tokenizing continuous actions with a hierarchical vector quantization module. Across seven environments including simulated manipulation, autonomous driving, and robotics, VQ-BeT improves on state-of-the-art models such as BeT and Diffusion Policies. Importantly, we demonstrate VQ-BeT's improved ability to capture behavior modes while accelerating inference speed 5x over Diffusion Policies. Videos and code can be found https://sjlee.cc/vq-bet},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Lee, Seungjae and Wang, Yibin and Etukuru, Haritheja and Kim, H. Jin and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},
	month = jun,
	year = {2024},
	note = {arXiv:2403.03181 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\4EZQH2A7\\Lee 等 - 2024 - Behavior Generation with Latent Actions.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\C8CJ546R\\2403.html:text/html},
}

@misc{wu_unleashing_2023,
	title = {Unleashing {Large}-{Scale} {Video} {Generative} {Pre}-training for {Visual} {Robot} {Manipulation}},
	url = {http://arxiv.org/abs/2312.13139},
	doi = {10.48550/arXiv.2312.13139},
	abstract = {Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a straightforward GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9\% to 94.9\%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3\% to 85.4\%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Wu, Hongtao and Jing, Ya and Cheang, Chilam and Chen, Guangzeng and Xu, Jiafeng and Li, Xinghang and Liu, Minghuan and Li, Hang and Kong, Tao},
	month = dec,
	year = {2023},
	note = {arXiv:2312.13139 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\NAYFR5C7\\Wu 等 - 2023 - Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\7Z4NKN9N\\2312.html:text/html},
}

@misc{cheang_gr-2_2024,
	title = {{GR}-2: {A} {Generative} {Video}-{Language}-{Action} {Model} with {Web}-{Scale} {Knowledge} for {Robot} {Manipulation}},
	shorttitle = {{GR}-2},
	url = {http://arxiv.org/abs/2410.06158},
	doi = {10.48550/arXiv.2410.06158},
	abstract = {We present GR-2, a state-of-the-art generalist robot agent for versatile and generalizable robot manipulation. GR-2 is first pre-trained on a vast number of Internet videos to capture the dynamics of the world. This large-scale pre-training, involving 38 million video clips and over 50 billion tokens, equips GR-2 with the ability to generalize across a wide range of robotic tasks and environments during subsequent policy learning. Following this, GR-2 is fine-tuned for both video generation and action prediction using robot trajectories. It exhibits impressive multi-task learning capabilities, achieving an average success rate of 97.7\% across more than 100 tasks. Moreover, GR-2 demonstrates exceptional generalization to new, previously unseen scenarios, including novel backgrounds, environments, objects, and tasks. Notably, GR-2 scales effectively with model size, underscoring its potential for continued growth and application. Project page: {\textbackslash}url\{https://gr2-manipulation.github.io\}.},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Cheang, Chi-Lam and Chen, Guangzeng and Jing, Ya and Kong, Tao and Li, Hang and Li, Yifeng and Liu, Yuxiao and Wu, Hongtao and Xu, Jiafeng and Yang, Yichu and Zhang, Hanbo and Zhu, Minzhao},
	month = oct,
	year = {2024},
	note = {arXiv:2410.06158 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\SLXTDM3S\\Cheang 等 - 2024 - GR-2 A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\KXY39U62\\2410.html:text/html},
}

@misc{ahmaditeshnizi_optimus_2024,
	title = {{OptiMUS}: {Scalable} {Optimization} {Modeling} with ({MI}){LP} {Solvers} and {Large} {Language} {Models}},
	shorttitle = {{OptiMUS}},
	url = {http://arxiv.org/abs/2402.10172},
	doi = {10.48550/arXiv.2402.10172},
	abstract = {Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions. OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations. OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts. Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy datasets by more than \$20{\textbackslash}\%\$ and on hard datasets (including a new dataset, NLP4LP, released with this paper that features long and complex problems) by more than \$30{\textbackslash}\%\$.},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {AhmadiTeshnizi, Ali and Gao, Wenzhi and Udell, Madeleine},
	month = feb,
	year = {2024},
	note = {arXiv:2402.10172 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\7HF2VL3T\\AhmadiTeshnizi 等 - 2024 - OptiMUS Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\FWDUZNZL\\2402.html:text/html},
}

@misc{nvidia_cosmos_2025,
	title = {Cosmos {World} {Foundation} {Model} {Platform} for {Physical} {AI}},
	url = {http://arxiv.org/abs/2501.03575},
	doi = {10.48550/arXiv.2501.03575},
	abstract = {Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make Cosmos open-source and our models open-weight with permissive licenses available via https://github.com/nvidia-cosmos/cosmos-predict1.},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {NVIDIA and Agarwal, Niket and Ali, Arslan and Bala, Maciej and Balaji, Yogesh and Barker, Erik and Cai, Tiffany and Chattopadhyay, Prithvijit and Chen, Yongxin and Cui, Yin and Ding, Yifan and Dworakowski, Daniel and Fan, Jiaojiao and Fenzi, Michele and Ferroni, Francesco and Fidler, Sanja and Fox, Dieter and Ge, Songwei and Ge, Yunhao and Gu, Jinwei and Gururani, Siddharth and He, Ethan and Huang, Jiahui and Huffman, Jacob and Jannaty, Pooya and Jin, Jingyi and Kim, Seung Wook and Klár, Gergely and Lam, Grace and Lan, Shiyi and Leal-Taixe, Laura and Li, Anqi and Li, Zhaoshuo and Lin, Chen-Hsuan and Lin, Tsung-Yi and Ling, Huan and Liu, Ming-Yu and Liu, Xian and Luo, Alice and Ma, Qianli and Mao, Hanzi and Mo, Kaichun and Mousavian, Arsalan and Nah, Seungjun and Niverty, Sriharsha and Page, David and Paschalidou, Despoina and Patel, Zeeshan and Pavao, Lindsey and Ramezanali, Morteza and Reda, Fitsum and Ren, Xiaowei and Sabavat, Vasanth Rao Naik and Schmerling, Ed and Shi, Stella and Stefaniak, Bartosz and Tang, Shitao and Tchapmi, Lyne and Tredak, Przemek and Tseng, Wei-Cheng and Varghese, Jibin and Wang, Hao and Wang, Haoxiang and Wang, Heng and Wang, Ting-Chun and Wei, Fangyin and Wei, Xinyue and Wu, Jay Zhangjie and Xu, Jiashu and Yang, Wei and Yen-Chen, Lin and Zeng, Xiaohui and Zeng, Yu and Zhang, Jing and Zhang, Qinsheng and Zhang, Yuxuan and Zhao, Qingqing and Zolkowski, Artur},
	month = jul,
	year = {2025},
	note = {arXiv:2501.03575 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\BXZKUSCQ\\NVIDIA 等 - 2025 - Cosmos World Foundation Model Platform for Physical AI.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\KNM463DM\\2501.html:text/html},
}

@misc{kim_openvla_2024,
	title = {{OpenVLA}: {An} {Open}-{Source} {Vision}-{Language}-{Action} {Model}},
	shorttitle = {{OpenVLA}},
	url = {http://arxiv.org/abs/2406.09246},
	doi = {10.48550/arXiv.2406.09246},
	abstract = {Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5\% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4\%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.},
	urldate = {2026-02-12},
	publisher = {arXiv},
	author = {Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and Vuong, Quan and Kollar, Thomas and Burchfiel, Benjamin and Tedrake, Russ and Sadigh, Dorsa and Levine, Sergey and Liang, Percy and Finn, Chelsea},
	month = sep,
	year = {2024},
	note = {arXiv:2406.09246 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\7CVNIG96\\Kim 等 - 2024 - OpenVLA An Open-Source Vision-Language-Action Model.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\7P4EQ2EW\\2406.html:text/html},
}

@misc{chen_goal-vla_2025,
	title = {Goal-{VLA}: {Image}-{Generative} {VLMs} as {Object}-{Centric} {World} {Models} {Empowering} {Zero}-shot {Robot} {Manipulation}},
	shorttitle = {Goal-{VLA}},
	url = {http://arxiv.org/abs/2506.23919},
	doi = {10.48550/arXiv.2506.23919},
	abstract = {Generalization remains a fundamental challenge in robotic manipulation. To tackle this challenge, recent Vision-Language-Action (VLA) models build policies on top of Vision-Language Models (VLMs), seeking to transfer their open-world semantic knowledge. However, their zero-shot capability lags significantly behind the base VLMs, as the instruction-vision-action data is too limited to cover diverse scenarios, tasks, and robot embodiments. In this work, we present Goal-VLA, a zero-shot framework that leverages Image-Generative VLMs as world models to generate desired goal states, from which the target object pose is derived to enable generalizable manipulation. The key insight is that object state representation is the golden interface, naturally separating a manipulation system into high-level and low-level policies. This representation abstracts away explicit action annotations, allowing the use of highly generalizable VLMs while simultaneously providing spatial cues for training-free low-level control. To further improve robustness, we introduce a Reflection-through-Synthesis process that iteratively validates and refines the generated goal image before execution. Both simulated and real-world experiments demonstrate that our {\textbackslash}name achieves strong performance and inspiring generalizability in manipulation tasks. Supplementary materials are available at https://nus-lins-lab.github.io/goalvlaweb/.},
	urldate = {2026-02-28},
	publisher = {arXiv},
	author = {Chen, Haonan and Guo, Jingxiang and Wang, Bangjun and Zhang, Tianrui and Huang, Xuchuan and Zheng, Boren and Hou, Yiwen and Tie, Chenrui and Deng, Jiajun and Shao, Lin},
	month = sep,
	year = {2025},
	note = {arXiv:2506.23919 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\NR7ULWFQ\\Chen 等 - 2025 - Goal-VLA Image-Generative VLMs as Object-Centric World Models Empowering Zero-shot Robot Manipulati.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\ZGE4KYXA\\2506.html:text/html},
}

@misc{zhang_reinbot_2025,
	title = {{ReinboT}: {Amplifying} {Robot} {Visual}-{Language} {Manipulation} with {Reinforcement} {Learning}},
	shorttitle = {{ReinboT}},
	url = {http://arxiv.org/abs/2505.07395},
	doi = {10.48550/arXiv.2505.07395},
	abstract = {Vision-Language-Action (VLA) models have shown great potential in general robotic decision-making tasks via imitation learning. However, the variable quality of training data often constrains the performance of these models. On the other hand, offline Reinforcement Learning (RL) excels at learning robust policy models from mixed-quality data. In this paper, we introduce Reinforced robot GPT (ReinboT), a novel end-to-end VLA model that integrates the RL principle of maximizing cumulative reward. ReinboT achieves a deeper understanding of the data quality distribution by predicting dense returns that capture the nuances of manipulation tasks. The dense return prediction capability enables the robot to generate more robust decision-making actions, oriented towards maximizing future benefits. Extensive experiments show that ReinboT achieves state-of-the-art performance on the CALVIN mixed-quality dataset and exhibits superior few-shot learning and out-of-distribution generalization capabilities in real-world tasks.},
	urldate = {2026-02-28},
	publisher = {arXiv},
	author = {Zhang, Hongyin and Zhuang, Zifeng and Zhao, Han and Ding, Pengxiang and Lu, Hongchao and Wang, Donglin},
	month = may,
	year = {2025},
	note = {arXiv:2505.07395 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:E\:\\Users\\AresZz\\Zotero\\storage\\AKJ2M3SZ\\Zhang 等 - 2025 - ReinboT Amplifying Robot Visual-Language Manipulation with Reinforcement Learning.pdf:application/pdf;Snapshot:E\:\\Users\\AresZz\\Zotero\\storage\\QPJK87B3\\2505.html:text/html},
}

@article{jian_pi-vla_2026,
	title = {{PI}-{VLA}: {Adaptive} {Symmetry}-{Aware} {Decision}-{Making} for {Long}-{Horizon} {Vision}–{Language}–{Action} {Manipulation}},
	volume = {18},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-8994},
	shorttitle = {{PI}-{VLA}},
	url = {https://www.mdpi.com/2073-8994/18/3/394},
	doi = {10.3390/sym18030394},
	abstract = {Vision–language–action (VLA) models often suffer from limited robustness in long-horizon manipulation tasks—where robots must execute extended sequences of actions over multiple time steps to achieve complex goals—due to their inability to explicitly exploit structural symmetries and to react adaptively when such symmetries are violated by environmental uncertainty. To address this limitation, this paper proposes PI-VLA, a symmetry-aware predictive and interactive VLA framework for robust robotic manipulation. PI-VLA is built upon three key symmetry-driven principles. First, a Cognitive–Motor Synergy (CMS) module jointly generates discrete and continuous action chunks together with predictive world-model features in a single forward pass, enforcing cross-modal action consistency as an implicit symmetry constraint across heterogeneous action representations. Second, a unified training objective integrates imitation learning, reinforcement learning, and state prediction, encouraging invariance to task-relevant transformations while enabling adaptive symmetry breaking when long-horizon deviations emerge. Third, an Active Uncertainty-Resolving Decider (AURD) explicitly monitors action consensus discrepancies and state prediction errors as symmetry-breaking signals, dynamically adjusting the execution horizon through closed-loop replanning. Extensive experiments on long-horizon benchmarks demonstrate that PI-VLA achieves state-of-the-art performance, attaining a 73.2\% average success rate on the LIBERO benchmark (with particularly strong gains on the Long-Horizon suite) and an 88.3\% success rate in real-world manipulation tasks under visual distractions and unseen conditions. Ablation studies confirm that symmetry-aware action consensus and uncertainty-triggered replanning are critical to robust execution. These results establish PI-VLA as a principled framework that leverages symmetry preservation and controlled symmetry breaking to enable reliable and interactive robotic manipulation.},
	language = {en},
	number = {3},
	urldate = {2026-02-28},
	journal = {Symmetry},
	publisher = {Multidisciplinary Digital Publishing Institute},
	author = {Jian, Yina and Tian, Di and Chen, Xuan-Jing and Wei, Zhen-Yuan and Liang, Chen-Wei and Wang, Mu-Jiang-Shan},
	month = mar,
	year = {2026},
	keywords = {action consistency, long-horizon decision making, predictive world models, robot learning, robotic manipulation, robust control, symmetry breaking, symmetry-aware learning, uncertainty-aware planning, vision–language–action},
	pages = {394},
	file = {Full Text PDF:E\:\\Users\\AresZz\\Zotero\\storage\\49DMGN3T\\Jian 等 - 2026 - PI-VLA Adaptive Symmetry-Aware Decision-Making for Long-Horizon Vision–Language–Action Manipulation.pdf:application/pdf},
}
