# 数据即引擎：2025-2026年具身智能与世界模型数据集构建方法的范式演进

## 仿真驱动的数据集构建：从可控环境到无限世界的演进

在2025至2026年的研究周期内，基于仿真的数据集构建方法依然是具身智能领域的重要基石。然而，其内涵与外延均发生了显著演变，研究重心正从创建静态、有限的受控环境，转向构建大规模、多样化且具备物理真实感的虚拟世界。这一演进主要体现在三大技术方向：无限世界生成、现实世界的数字化孪生，以及对物理精确性的极致追求。这些进展共同推动了仿真数据从单纯的实验工具向更具自主性和复杂性的数据生产平台发展。

首先，无限世界生成是该时期最引人注目的发展方向之一，其核心目标是克服传统仿真环境依赖预设3D模型、难以覆盖长尾场景的局限。以程序化生成技术为基础，新一代工具旨在创造理论上无边界的交互式3D空间 [[46]]。`Infinigen-Sim`是一个典型代表，它专注于生成多样化的关节物体，允许用户创建复杂的对象生成器，从而极大地丰富了场景内容的多样性 [[21]]。另一项工作`WorldGen`则展示了从文本提示直接生成大型、可交互3D世界的能力，这标志着生成式人工智能在世界构建层面的深度应用 [[48]]。此外，`ProcWORLD`和`ProcWorld`基准的提出，也反映了社区通过程序化生成来专门评测模型在部分可观测环境下进行长期规划和空间推理能力的趋势 [[84]]。在效率方面，`WorldGrow`框架通过分块、迭代的“粗到精”策略，能够高效合成超过1800平方米的大尺度连续3D场景，为大规模场景合成提供了重要的技术路径 [[58]]。这类方法的数据规模通常是理论上的无限，实际应用中可根据需求生成任意大小的场景和轨迹，为训练开放域机器人提供了前所未有的数据可能性。

其次，现实世界的数字化孪生（Digital Twins）成为连接真实与虚拟世界的桥梁。此流派的核心思想是利用先进的3D重建技术将物理世界扫描并转化为高保真的数字副本，然后在此基础上进行数据增强和合成。`RoboGSim`是这一理念的成功实践典范 [[90]]。该系统首先使用3D Gaussian Splatting (3DGS) 技术从多视角视频重建真实场景和物体，随后将其导入NVIDIA的Isaac Sim仿真环境，并通过场景合成等模块生成包含新物体、新视角和新任务的演示数据。其成果极为显著：生成1000条合成轨迹仅需4小时，而手动收集同等数量的真实数据则需要40小时；更重要的是，在零样本测试中，由合成数据训练的视觉-语言-动作（VLA）模型表现优于纯真实数据训练的模型，显示出更强的泛化能力 [[90]]。这种“Real-to-Sim-to-Real”的闭环流程，不仅提升了数据的真实感，也为解决机器人技能迁移问题提供了一条高效的路径。与此类似，`ReconDreamer`的工作强调了在线整合世界模型知识以恢复和重建驾驶场景，这是一种动态、增量式的数字孪生构建方法，进一步拓展了该方向的应用边界 [[49]]。

最后，对物理精确性的追求贯穿于各类仿真平台的开发中。为了使生成的数据能够准确反映现实世界的动态行为，研究人员致力于提升仿真器的物理引擎精度。`Nimbus`仿真平台引入了一个模块化的四层架构，通过解耦执行模型中的轨迹规划、渲染等模块，旨在实现更灵活和高效的合成数据生成 [[75]]。`SimWorld-Robotics`则是一个面向代理的开放仿真平台，特别强调在城市级大场景下模拟逼真的物理和社会动态，并支持语言驱动的交互，为评估机器人在复杂环境中的综合能力提供了新的基准 [[83,86]]。此外，`DynScene`的研究聚焦于生成动态的机器人操作场景，这要求仿真环境不仅能处理刚体动力学，还需能模拟接触、抓取等更为复杂的交互过程，体现了对仿真细节的更高要求 [[45]]。这些努力共同推动了仿真数据的质量，使其在弥合“Sim2Real鸿沟”方面发挥着越来越重要的作用。

| 代表性工作        | 构造方法                       | 核心特点                                               | 数据规模/生成能力                     |
| :---------------- | :----------------------------- | :----------------------------------------------------- | :------------------------------------ |
| Infinigen-Sim     | 程序化生成关节物体             | 用户可创建多样化、现实的关节物体生成器                 | 规模可扩展，具体数值未提供            |
| WorldGen          | 文本到3D世界生成               | 从文本提示自动生成大型、可交互的3D世界                 | 规模可扩展，具体数值未提供            |
| WorldGrow         | 分块、迭代的“粗到精”3D场景合成 | 高效合成超过1800m²的大尺度连续3D场景                   | 已成功合成超过1800m²的场景 [[58]]     |
| RoboGSim          | 数字孪生+仿真                  | 从真实世界重建数字孪生，再合成新数据                   | 生成1000条轨迹耗时4小时 [[90]]        |
| SimWorld-Robotics | 物理精确仿真                   | 开放式仿真平台，模拟城市级大场景下的逼真物理和社会动态 | 用于构建新型机器人能力评估基准 [[56]] |

## 真实世界驱动的数据集构建：从稀缺采集到智慧利用的转变

尽管仿真数据在规模和可控性上占据优势，但真实世界数据因其无可比拟的真实性，始终是训练鲁棒具身智能模型不可或缺的组成部分。在2025至2026年间，该领域的研究重点发生了深刻的转变：从单纯追求大规模采集，转向更加注重数据的广度、深度和利用效率。这一转变体现在三个方面：构建涵盖广泛任务的大规模多源数据集、探索非结构化的真实世界数据源，以及针对特定子任务构建高质量的精细化数据集。

构建大规模、多任务的真实世界数据集已成为主流趋势。研究人员普遍认识到，单一来源或单一任务的数据不足以支撑通用型机器人的发展，因此，整合来自不同机器人平台、不同环境和不同任务的异构数据成为必然选择。`OpenVLA`项目便是一个例证，它旨在建立一个开源的视觉-语言-行动模型，其训练数据本身就包含了多种来源 [[19]]。同样，`GigaBrain-0`模型的训练也采用了混合数据策略，其庞大的语料库不仅整合了多个公开的机器人数据集，还包含了大量内部采集的真实世界数据，例如来自AgiBot G1机器人长达983小时的通用数据和来自Agilex Cobot Magic机器人199小时的工业数据 [[93]]。这种跨领域的数据融合策略，使得模型能够学习到更具普适性的世界知识和操作技能。除了通用数据集的构建，针对特定难题的超大规模真实数据集也开始涌现。例如，`GraspClutter6D`数据集专注于高度杂乱场景下的抓取任务，包含了1000个精心布置的密集场景，为解决机器人在真实环境中最常见的抓取难题提供了宝贵的、大规模的真实世界数据资源 [[39]]。

与此同时，研究者们开始探索超越传统实验室和远程操控范畴的全新真实世界数据源，其中最具潜力的是互联网上海量的非结构化视频数据。Skild AI提出了一种颠覆性的“观察学习”方法，认为互联网上的大量人类活动视频（如YouTube教程、头戴相机记录的视频）是物理AI领域的“Common Crawl时刻” [[88]]。该方法的核心在于建立一个人类手部意图与机器人执行器之间的映射关系，从而让机器人能够直接从观看人类活动的视频中学习技能。这种方法有望绕过传统数据采集中成本高昂、效率低下的人工操作环节，实现数据获取的指数级增长。虽然这一想法仍处于早期探索阶段，但它预示着数据获取范式的一次重大变革。`Infinite-World`的研究也在一定程度上间接支持了这一思路，它指出了自然视频流（如网络视频）的一个关键缺陷——缺乏密集的视角回溯，这使得学习全局几何结构变得困难 [[44]]。这一发现反过来促使研究者思考如何设计模型来更好地处理这类非结构化数据，例如通过姿态无关的记忆压缩机制来克服数据的线性特性。

最后，针对特定子任务构建高质量、细粒度标注的数据集，是提升模型在特定能力上表现的有效途径。这些数据集通常规模不及前述的百万级数据集，但其标注的深度和任务的相关性使其具有极高的研究价值。`NavMorph`数据集通过在连续的3D模拟环境（Habitat-Matterport）中让智能体进行“自我博弈”，生成了85万条指令-轨迹对 [[94]]。这些数据不仅规模庞大，而且随着智能体世界模型的不断演化而“自我进化”，体现了数据与模型协同增长的独特模式。`InterAct`数据集则专注于人机交互（HOI），它是一个大规模、多任务的生成数据集，涵盖了从文本到交互、从动作到交互等多种生成任务，并进行了丰富的标注，为研究交互行为的生成提供了坚实的基础 [[61]]。此外，`MoMa-Kitchen`数据集专注于移动操纵中的可达性导航，它在AI2-THOR的厨房模拟环境中提供了超过11.7万个专家示范序列，每个序列都附有详细的 affordance-annotated 交互注释，为训练能够在复杂室内环境中执行长程任务的导航和操作模型提供了宝贵资源 [[94]]。这些精细化数据集的出现，标志着数据构建正从“广撒网”式的大规模采集，转向“深挖洞”式的针对性能力培养。

## 合成与真实的混合策略：弥合“Sim2Real鸿沟”的前沿实践

在具身智能领域，合成数据与真实数据各有其优劣：合成数据具备无限的可扩展性、精确的标注能力和低成本的优势，但往往面临“Sim2Real鸿沟”的挑战，即模型在仿真环境中表现良好，但在真实世界中性能下降 [[92]]；真实数据则保证了物理真实性，但其采集成本高昂、过程繁琐且存在隐私等问题。因此，2025至2026年的研究热点之一便是探索有效的混合策略，旨在结合二者之长，最大限度地缩小这一鸿沟。当前的混合策略主要可以归纳为两大类：以数据增强和域适应为目标的传统Sim2Real方法，以及以数字孪生为核心的Real2Sim2Real闭环流程。

传统的Sim2Real方法主要通过在仿真环境中生成大量数据，然后利用各种技术手段（如域随机化、域自适应）来提升模型对真实世界变化的鲁棒性。域随机化是一种常见的策略，它通过对仿真环境中的光照、纹理、材质、背景等参数进行系统性地随机化，迫使模型学习那些对这些变化不敏感的本质特征 [[91]]。研究人员持续优化域随机化的范围和方法，例如增加新的随机化项目或扩大现有项目的参数范围，以期获得更好的泛化效果 [[92]]。域自适应则是另一种重要技术，它试图在生成合成数据后，通过对抗性训练或其他学习方法，将合成数据的特征分布调整得与真实世界数据的分布对齐 [[91]]。近年来，神经图像合成方法也被越来越多地应用于数据增强，而非从零开始创建全新的数据集。例如，MagicDrive、DriveDreamer和DriveGEN等方法利用生成模型来扩充现有的真实驾驶数据集，特别是用于生成罕见或危险的边缘案例，从而提升自动驾驶模型的安全性 [[91]]。

然而，一种更为先进和高效的混合策略正在兴起，即“现实世界到仿真再到现实世界”的闭环流程。`RoboGSim`是该策略的杰出代表，它完美地诠释了这一范式 [[90]]。首先，该系统利用3D Gaussian Splatting（3DGS）技术从真实的多视角RGB视频中重建出场景和物体的高保真数字孪生。然后，这个数字孪生被导入到强大的物理仿真器Isaac Sim中。在仿真环境中，研究人员可以自由地合成新的演示数据，例如改变相机视角、添加新的物体、设计新的任务轨迹等。最后，用这些合成数据训练出的策略可以直接部署到物理机器人上。`RoboGSim`的实验结果令人信服：训练一个环套任务的模型，使用1000条RoboGSim生成的合成数据达到90%的抓取成功率，而使用1000条人工收集的真实数据也达到了90%的成功率 [[90]]。更有趣的是，当面对新场景时，RoboGSim训练的模型成功率达到90%，远高于真实数据训练模型的60%。这表明，经过精心设计的数字孪生和仿真环境，其生成的数据不仅有效，甚至在某些情况下比原始真实数据更具泛化优势。

除了上述两种主流策略，还涌现出一些独特的混合范式。例如，`DreamerV3`算法展示了一种完全不同的“在线数据生成”模式 [[47]]。它并不依赖任何预先存在的外部数据集，而是让智能体在模拟环境中自主探索，通过与环境的交互实时生成经验数据（轨迹）。模型自身就是数据的生产者，这种动态生成的数据天然地包含了智能体在探索过程中遇到的各种情况，形成了一种由模型驱动的、持续进化的混合数据生态。此外，一些项目强调了数据质量评估在整个混合策略中的核心地位。例如，`DataQA`管道专为机器人数据集设计，用于在数据进入训练流程前进行严格的品质控制和评估 [[25]]。`CroCoDL`数据集则通过整合来自机器人和混合现实设备（如手机、头显）的传感器记录，探索跨设备协作定位的可能性 [[34]]。这些工作共同指向一个共识：成功的混合策略不仅仅是简单地混合数据，更是要建立一套完整的、以数据质量为导向的闭环体系，确保输入给模型的每一份数据都能最大化其价值。

## 多模态融合与精细化标注：提升数据内在价值的趋势

随着具身智能系统日益复杂，单一模态的数据已不足以支撑其理解和交互世界的能力。2025至2026年的研究趋势清晰地表明，未来的数据集必须是多模态的，并且需要具备精细的时空标注。这一趋势源于一个基本认知：人类的智能是建立在融合视觉、听觉、运动、触觉等多种感官信息的基础上的，一个真正通用的具身智能体也必须具备类似的能力。因此，现代世界模型的数据集构建正朝着更全面、更接近人类感知模式的方向发展。

多模态融合已成为构建新一代数据集的“标配”。除了传统的高维RGB图像序列，深度（Depth）、惯性测量单元（IMU）数据、六自由度（6-DoF）相机位姿、全身动作捕捉（Motion Capture）数据，甚至是第一人称视角的脑部活动（fMRI）信号，都被越来越多地纳入数据采集和建模的考量范围。`GigaBrain-0`模型在预训练阶段就明确融合了RGB-D输入，以增强其空间推理能力。它采用SigLIP作为视觉编码器，并巧妙地通过零初始化的卷积核来扩展其处理深度通道的能力，同时在训练中随机丢弃深度通道，以确保模型在推理时也能处理纯RGB输入 [[93]]。`PEVA`模型的训练则完全基于一个名为Nymeria的大规模同步数据集，该数据集包含了从真实世界不同场景录制的同步RGB视频流和全身动作捕捉数据，为模型学习从人体动作预测第一人称视角视频提供了基础 [[87]]。更前沿的研究甚至开始触及心智与感知的关系，`NSD-Imagery`数据集尝试将人类的内心意象（mental images）与对应的fMRI大脑活动信号配对，为探索意识和想象的计算模型开辟了新的数据维度 [[60]]。这些工作共同推动了数据集从二维像素集合向包含丰富物理和情境信息的多维时空数据体的转变。

与多模态并行发展的，是对数据进行精细化时空标注的需求。仅仅拥有高质量的原始数据是不够的，若没有精确的标注，数据的内在价值将大打折扣。精细化标注不仅包括传统的物体检测框、分割掩码，更延伸到了动作、交互、因果关系等多个层面。`InterAct`数据集在这方面树立了标杆，它不仅是一个大规模的人机交互（HOI）生成数据集，更重要的是其包含了对交互事件的丰富标注，支持从文本生成交互、从动作生成交互等多种复杂任务 [[61]]。`MoMa-Kitchen`数据集则专注于可达性（affordance-grounded）的导航和操作，其超过11.7万个专家示范序列都配有详细的物体交互注释，如“打开冰箱”、“拿起杯子”等，这些注释对于训练能够理解物体功能和执行长程任务的模型至关重要 [[94]]。此外，一些研究开始关注对数据内在结构的标注。例如，`NavMorph`数据集通过让智能体在模拟环境中进行“自我博弈”来生成数据，其生成的轨迹自然地与智能体内部不断演化的世界模型表示相一致，这种“模型驱动”的数据标注方式为训练能够进行自主探索和地图构建的导航模型提供了独特的优势 [[94]]。这些精细化标注的努力，使得数据集不仅是被动的训练材料，更成为了蕴含丰富先验知识的知识库，能够引导模型学习更深层次的语义和逻辑关系。

| 数据集/模型  | 融合模态                                                | 标注类型                                                     | 关键技术/贡献                                                |
| :----------- | :------------------------------------------------------ | :----------------------------------------------------------- | :----------------------------------------------------------- |
| GigaBrain-0  | RGB-D                                                   | 无特殊标注，但显式融合深度信息                               | 扩展视觉编码器以处理深度通道，增强空间推理 [[93]]            |
| PEVA         | RGB, 全身动作捕捉                                       | 动作轨迹（Kinematic Pose Trajectories）                      | 基于同步的RGB和全身动作捕捉数据，预测第一人称视角视频 [[87]] |
| NSD-Imagery  | fMRI, 内心意象                                          | fMRI活动信号与内心意象配对                                   | 连接人类心智意象与大脑活动，为探索想象的计算模型提供数据 [[60]] |
| InterAct     | RGB, 3D坐标, 语义标签                                   | 丰富的交互事件标注                                           | 定义了六个关键的HOI生成任务，支持从文本到交互等多种生成 [[61]] |
| MoMa-Kitchen | egocentric RGB-D                                        | affordance-annotated object interactions, action logs, spatial reasoning annotations | 专注于mobile manipulation中的可达性导航，提供超过117k expert-demonstrated sequences [[94]] |
| NavMorph     | RGB, language instructions, ego-motion, occupancy grids | evolving world state graphs                                  | 通过self-play in continuous environments生成850k instruction-trajectory pairs，数据与模型协同进化 [[94]] |

## 范式革新：世界模型作为数据引擎的兴起

在2025至2026年的研究浪潮中，一个根本性的范式革新正在发生：世界模型的角色正从一个被动的“世界预测器”转变为一个主动的“数据生成引擎”。这一转变的核心思想是，既然世界模型已经能够以很高的保真度模拟物理世界、物体交互和动态过程，那么就可以利用它来按需、高效地创造大规模、多样化且高质量的训练数据，以解决真实世界数据采集成本高昂、稀缺或难以覆盖长尾场景的根本性瓶颈。这一理念的集中体现是`GigaWorld`框架及其催生的`GigaBrain-0`模型，它标志着数据集构建方法进入了一个智能化、自动化的新阶段。

`GigaWorld`框架是这一新范式的典型代表，它通过一系列互补的合成数据生成管道，实现了对训练数据的规模化、多样化和高效化生产 [[73,93]]。该框架整合了多种高级数据生成策略，包括：
1.  **Real2Real Transfer**：将已有的真实世界轨迹在仿真环境中重新渲染，以改变其视觉背景和上下文，从而创造出新的训练样本。
2.  **View Transfer**：利用单目深度估计等技术，从一个已有的数据集中生成新的、从未见过的相机视角，极大地扩展了数据的视点多样性。
3.  **Sim2Real Transfer**：在物理仿真器中创建全新的场景和任务，然后通过先进的渲染管线生成具有照片级真实感的图像和视频数据。
4.  **Human Video Transfer**：这是最具创新性的策略之一，它旨在将从互联网上收集的、未经标注的、第一人称视角的人类活动视频，转换成可供机器人模仿的、结构化的动作执行轨迹。
5.  **Inverse Dynamics Modeling**：通过分析生成的视频，反向推断出能够产生该视觉变化的动作序列，从而为模型提供监督信号。

通过这套组合拳，`GigaWorld`成功地为`GigaBrain-0`这一视觉-语言-动作（VLA）基础模型提供了极其丰富的训练数据，该模型的训练语料库整合了公共数据集、大量内部真实世界数据，以及由世界模型自身生成的海量合成数据 [[93]]。这项工作的意义在于，它首次提供了强有力的证据，证明仅使用合成数据进行预训练，其效果可以媲美甚至超越依赖大量真实机器人数据的模型 [[74]]。这彻底改变了社区对合成数据价值的认知，从将其视为辅助工具，提升到其本身即可成为训练通用智能体的主要驱动力。

除了`GigaWorld`这样由模型主导的离线数据生成范式，数据生成过程的自动化和智能化也在其他方向上取得进展。`DreamerV3`算法展示了一种独特的“在线数据生成”模式，它不依赖任何外部数据集，而是让智能体在模拟环境中自主探索，通过与环境的交互实时生成经验数据（轨迹）进行学习 [[47]]。这种模式可以看作是一种动态的、由模型自身需求驱动的混合数据策略，数据是在解决问题的过程中自然产生的。另一个例子是`NavMorph`数据集，它通过在连续3D模拟环境中让智能体进行“自我博弈”，生成了85万条指令-轨迹对 [[94]]。这些数据不仅量大，而且随着智能体世界模型的不断演化而“自我进化”，形成了数据与模型相互塑造、共同演进的良性循环。这种“数据与模型协同进化”的理念，预示着未来的数据集构建将不再是孤立的、一次性完成的任务，而是一个与模型训练紧密耦合的、持续进行的有机过程。世界模型作为数据引擎的兴起，不仅极大地提升了数据生产的效率和灵活性，也从根本上改变了我们对数据、模型和智能之间关系的理解。

## 总结与展望：数据集构建的未来趋势与挑战

回顾2025至2026年间具身智能与世界模型领域在数据集构建方面的研究进展，可以清晰地看到一条从被动采集到主动生成、从单一模态到多维融合、从规模导向到质量与能力导向的演进脉络。这一时期的标志性成就，不仅体现在各大顶会和期刊上涌现出的众多高质量数据集，更体现在整个社区对数据本质和作用的认知深化。总结而言，几个核心趋势对未来的发展方向具有深远影响。

首先，“数据即引擎”的范式已然确立。以`GigaWorld`为代表的实践证明，世界模型本身有能力成为高效、多样且高质量的合成数据生产者 [[73,79]]。这一转变意味着数据瓶颈有望得到缓解，研究的重点可以从“如何获取更多数据”转向“如何设计更有效的数据生成策略和模型来消费这些数据”。其次，数据与模型的协同进化成为一种新兴范式。无论是`NavMorph`数据集通过智能体自我博弈实现数据的“自我进化” [[94]]，还是`GigaWorld`框架利用世界模型生成数据来训练下一代世界模型，都揭示了数据与模型不再是线性的输入与产出关系，而是相互促进、共同演化的共生关系。再次，对数据内在价值的挖掘成为共识。研究不再满足于简单的规模堆砌，而是更加关注数据的结构、模态和标注的精细度。对非结构化真实数据源（如互联网视频）的探索 [[88]]，以及对物理推理、因果关系等高级能力的基准测试 [[89]]，都在引导数据集的设计从服务于感知任务，转向服务于更深层次的认知和推理任务。

尽管取得了显著进展，但该领域依然面临着诸多挑战。第一，合成数据的长期有效性与泛化能力仍有待验证。虽然短期内合成数据表现出色，但其能否在开放、动态且充满未知的真实世界中保持鲁棒性，仍是悬而未决的问题。过度拟合于合成数据的模型是否会失去对真实世界细微差别的感知能力，是一个需要警惕的风险。第二，全新的、低成本数据源的可靠性问题亟待解决。以Skild AI提出的利用互联网视频为例，如何从海量、混乱、带有偏见的视频中提取干净、可靠的物理知识，以及如何解决不同视频源之间的巨大风格差异，是巨大的技术和工程挑战 [[88]]。第三，数据隐私与伦理问题日益凸显。随着越来越多来自公共视频或个人传感器的真实世界数据被用于训练，如何确保数据采集和使用的合规性，保护个人隐私和安全，将成为无法回避的社会议题。目前的研究对此关注甚少。

展望未来，数据集构建方法可能会沿着以下几个方向继续发展。一是向更复杂的多模态融合迈进，例如将触觉、力反馈、声音等更多维度的信息融入统一的建模范式中，以构建更接近人类全感官体验的模型。二是探索更具创造性的数据生成方式，例如结合符号推理和神经表征，生成蕴含丰富因果关系和常识知识的结构化世界模型。三是推动数据构建的民主化和标准化，通过开源工具和共享基准，降低高质量数据集的构建门槛，加速整个社区的创新步伐。最终，数据集构建的终极目标，将是创造出能够支撑通用人工智能体成长的、永不枯竭的、富有意义的“数字生命”生态系统。